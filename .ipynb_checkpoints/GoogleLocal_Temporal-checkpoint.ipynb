{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd \n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_user, n_item, k=1):\n",
    "        super(MF, self).__init__()\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        \n",
    "        # gammas (users and items)\n",
    "        self.user = nn.Embedding(n_user, k)\n",
    "        self.item = nn.Embedding(n_item, k)\n",
    "        \n",
    "        # alpha and betas (users and items)\n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        \n",
    "        # self.theta = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, train_x):\n",
    "        item_id = train_x[:, 0]\n",
    "        user_id = train_x[:, 1]\n",
    "        # freq = train_x[:, 2].float()\n",
    "        vector_user = self.user(user_id)\n",
    "        vector_item = self.item(item_id)\n",
    "        \n",
    "        # Pull out biases\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        \n",
    "        # biases = (self.bias + self.theta*freq + bias_user + bias_item)\n",
    "        biases = (self.bias + bias_user + bias_item)\n",
    "        \n",
    "        ui_interaction = torch.sum(vector_user * vector_item, dim=1)\n",
    "        \n",
    "        # Add bias prediction to the interaction prediction\n",
    "        prediction = ui_interaction + biases\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "        return F.mse_loss(prediction, target.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10601852 entries, 0 to 10601851\n",
      "Data columns (total 8 columns):\n",
      "gPlusPlaceId      int64\n",
      "gPlusUserId       int64\n",
      "rating            int64\n",
      "unixReviewTime    int64\n",
      "num_reviews       int64\n",
      "year              int64\n",
      "month             int64\n",
      "day               int64\n",
      "dtypes: int64(8)\n",
      "memory usage: 647.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gPlusPlaceId</th>\n",
       "      <th>gPlusUserId</th>\n",
       "      <th>rating</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804813</td>\n",
       "      <td>2021440</td>\n",
       "      <td>4</td>\n",
       "      <td>662601600</td>\n",
       "      <td>9</td>\n",
       "      <td>1990</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1918972</td>\n",
       "      <td>389663</td>\n",
       "      <td>4</td>\n",
       "      <td>662601600</td>\n",
       "      <td>20</td>\n",
       "      <td>1990</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>449452</td>\n",
       "      <td>2709545</td>\n",
       "      <td>4</td>\n",
       "      <td>662601600</td>\n",
       "      <td>15</td>\n",
       "      <td>1990</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>942354</td>\n",
       "      <td>4936600</td>\n",
       "      <td>4</td>\n",
       "      <td>662601600</td>\n",
       "      <td>293</td>\n",
       "      <td>1990</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3063673</td>\n",
       "      <td>828378</td>\n",
       "      <td>5</td>\n",
       "      <td>662601600</td>\n",
       "      <td>11</td>\n",
       "      <td>1990</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gPlusPlaceId  gPlusUserId  rating  unixReviewTime  num_reviews  year  \\\n",
       "0        804813      2021440       4       662601600            9  1990   \n",
       "1       1918972       389663       4       662601600           20  1990   \n",
       "2        449452      2709545       4       662601600           15  1990   \n",
       "3        942354      4936600       4       662601600          293  1990   \n",
       "4       3063673       828378       5       662601600           11  1990   \n",
       "\n",
       "   month  day  \n",
       "0     12   31  \n",
       "1     12   31  \n",
       "2     12   31  \n",
       "3     12   31  \n",
       "4     12   31  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../datasets/google_local/reviews_timesorted.csv\")\n",
    "display(data.info())\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5054567 3116785\n"
     ]
    }
   ],
   "source": [
    "original_data = pd.read_csv(\"../datasets/google_local/reviews_freq.csv\")\n",
    "\n",
    "n_user = len(original_data['gPlusUserId'].unique())\n",
    "n_place = len(original_data['gPlusPlaceId'].unique())\n",
    "\n",
    "print(n_user,n_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10601852 7421297 1590278 1590277\n"
     ]
    }
   ],
   "source": [
    "N = data.index.size\n",
    "\n",
    "# Note that the reviews are sorted by time\n",
    "train_split = int(N * 0.70)   \n",
    "valid_split =  int(N * 0.85)\n",
    "\n",
    "train_x = data.loc[:train_split, ['gPlusPlaceId','gPlusUserId','num_reviews', \\\n",
    "                                  'unixReviewTime','year','month','day']]\n",
    "train_y = data.loc[:train_split, 'rating':'rating']\n",
    "valid_x = data.loc[train_split+1:valid_split, ['gPlusPlaceId','gPlusUserId','num_reviews', \\\n",
    "                                  'unixReviewTime','year','month','day']]\n",
    "valid_y = data.loc[train_split+1:valid_split, 'rating':'rating']\n",
    "test_x = data.loc[valid_split+1:, ['gPlusPlaceId','gPlusUserId','num_reviews', \\\n",
    "                                  'unixReviewTime','year','month','day']]\n",
    "test_y = data.loc[valid_split+1:, 'rating':'rating']\n",
    "\n",
    "print(N, train_x.index.size, valid_x.index.size,test_x.index.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gPlusPlaceId</th>\n",
       "      <th>gPlusUserId</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804813</td>\n",
       "      <td>2021440</td>\n",
       "      <td>9</td>\n",
       "      <td>662601600</td>\n",
       "      <td>1990</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1918972</td>\n",
       "      <td>389663</td>\n",
       "      <td>20</td>\n",
       "      <td>662601600</td>\n",
       "      <td>1990</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>449452</td>\n",
       "      <td>2709545</td>\n",
       "      <td>15</td>\n",
       "      <td>662601600</td>\n",
       "      <td>1990</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>942354</td>\n",
       "      <td>4936600</td>\n",
       "      <td>293</td>\n",
       "      <td>662601600</td>\n",
       "      <td>1990</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3063673</td>\n",
       "      <td>828378</td>\n",
       "      <td>11</td>\n",
       "      <td>662601600</td>\n",
       "      <td>1990</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gPlusPlaceId  gPlusUserId  num_reviews  unixReviewTime  year  month  day\n",
       "0        804813      2021440            9       662601600  1990     12   31\n",
       "1       1918972       389663           20       662601600  1990     12   31\n",
       "2        449452      2709545           15       662601600  1990     12   31\n",
       "3        942354      4936600          293       662601600  1990     12   31\n",
       "4       3063673       828378           11       662601600  1990     12   31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gPlusPlaceId</th>\n",
       "      <th>gPlusUserId</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7421297</th>\n",
       "      <td>2725776</td>\n",
       "      <td>4737472</td>\n",
       "      <td>26</td>\n",
       "      <td>1377798285</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7421298</th>\n",
       "      <td>2604227</td>\n",
       "      <td>43504</td>\n",
       "      <td>50</td>\n",
       "      <td>1377798286</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7421299</th>\n",
       "      <td>1298806</td>\n",
       "      <td>1549363</td>\n",
       "      <td>29</td>\n",
       "      <td>1377798286</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7421300</th>\n",
       "      <td>730982</td>\n",
       "      <td>3720790</td>\n",
       "      <td>6</td>\n",
       "      <td>1377798289</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7421301</th>\n",
       "      <td>2919327</td>\n",
       "      <td>3774840</td>\n",
       "      <td>2</td>\n",
       "      <td>1377798290</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         gPlusPlaceId  gPlusUserId  num_reviews  unixReviewTime  year  month  \\\n",
       "7421297       2725776      4737472           26      1377798285  2013      8   \n",
       "7421298       2604227        43504           50      1377798286  2013      8   \n",
       "7421299       1298806      1549363           29      1377798286  2013      8   \n",
       "7421300        730982      3720790            6      1377798289  2013      8   \n",
       "7421301       2919327      3774840            2      1377798290  2013      8   \n",
       "\n",
       "         day  \n",
       "7421297   29  \n",
       "7421298   29  \n",
       "7421299   29  \n",
       "7421300   29  \n",
       "7421301   29  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gPlusPlaceId</th>\n",
       "      <th>gPlusUserId</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9011575</th>\n",
       "      <td>1959671</td>\n",
       "      <td>2938557</td>\n",
       "      <td>1</td>\n",
       "      <td>1387118864</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011576</th>\n",
       "      <td>2039338</td>\n",
       "      <td>4638537</td>\n",
       "      <td>12</td>\n",
       "      <td>1387118869</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011577</th>\n",
       "      <td>727670</td>\n",
       "      <td>840634</td>\n",
       "      <td>43</td>\n",
       "      <td>1387118873</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011578</th>\n",
       "      <td>678877</td>\n",
       "      <td>3725103</td>\n",
       "      <td>6</td>\n",
       "      <td>1387118876</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011579</th>\n",
       "      <td>2776187</td>\n",
       "      <td>2754988</td>\n",
       "      <td>2</td>\n",
       "      <td>1387118877</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         gPlusPlaceId  gPlusUserId  num_reviews  unixReviewTime  year  month  \\\n",
       "9011575       1959671      2938557            1      1387118864  2013     12   \n",
       "9011576       2039338      4638537           12      1387118869  2013     12   \n",
       "9011577        727670       840634           43      1387118873  2013     12   \n",
       "9011578        678877      3725103            6      1387118876  2013     12   \n",
       "9011579       2776187      2754988            2      1387118877  2013     12   \n",
       "\n",
       "         day  \n",
       "9011575   15  \n",
       "9011576   15  \n",
       "9011577   15  \n",
       "9011578   15  \n",
       "9011579   15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_x[:5])\n",
    "display(valid_x[:5])\n",
    "display(test_x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lr = 1e-2\n",
    "lamb = 1e-6\n",
    "k=1\n",
    "batch_size = 1024\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "print (cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10601852 2120371 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 14.38\n",
      "Epoch[0] Validation Loss: 14.182 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 2.03\n",
      "Epoch[0] Validation Loss: 1.926 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.89\n",
      "Epoch[0] Validation Loss: 1.799 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 1.21\n",
      "Epoch[1] Validation Loss: 1.795 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 1.62\n",
      "Epoch[1] Validation Loss: 1.759 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 1.23\n",
      "Epoch[1] Validation Loss: 1.716 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 1.14\n",
      "Epoch[2] Validation Loss: 1.715 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 1.09\n",
      "Epoch[2] Validation Loss: 1.706 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[2000] Training Loss: 0.52\n",
      "Epoch[2] Validation Loss: 1.680 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.68\n",
      "Epoch[3] Validation Loss: 1.679 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.67\n",
      "Epoch[3] Validation Loss: 1.684 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.81\n",
      "Epoch[3] Validation Loss: 1.668 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.45\n",
      "Epoch[4] Validation Loss: 1.670 \n",
      "Epoch[4] Iteration[1000] Training Loss: 0.38\n",
      "Epoch[4] Validation Loss: 1.676 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.73\n",
      "Epoch[4] Validation Loss: 1.666 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[0] Training Loss: 0.31\n",
      "Epoch[5] Validation Loss: 1.668 \n",
      "Epoch[5] Iteration[1000] Training Loss: 0.24\n",
      "Epoch[5] Validation Loss: 1.677 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.63\n",
      "Epoch[5] Validation Loss: 1.667 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.18\n",
      "Epoch[6] Validation Loss: 1.667 \n",
      "Epoch[6] Iteration[1000] Training Loss: 0.33\n",
      "Epoch[6] Validation Loss: 1.671 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.52\n",
      "Epoch[6] Validation Loss: 1.663 \n",
      "Save best theta...\n",
      "Epoch[7] Iteration[0] Training Loss: 0.20\n",
      "Epoch[7] Validation Loss: 1.663 \n",
      "Save best theta...\n",
      "Epoch[7] Iteration[1000] Training Loss: 0.33\n",
      "Epoch[7] Validation Loss: 1.667 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.46\n",
      "Epoch[7] Validation Loss: 1.663 \n",
      "Save best theta...\n",
      "Epoch[8] Iteration[0] Training Loss: 0.27\n",
      "Epoch[8] Validation Loss: 1.664 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.30\n",
      "Epoch[8] Validation Loss: 1.667 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.43\n",
      "Epoch[8] Validation Loss: 1.658 \n",
      "Save best theta...\n",
      "Epoch[9] Iteration[0] Training Loss: 0.22\n",
      "Epoch[9] Validation Loss: 1.662 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.26\n",
      "Epoch[9] Validation Loss: 1.665 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.51\n",
      "Epoch[9] Validation Loss: 1.660 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.20\n",
      "Epoch[10] Validation Loss: 1.659 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.36\n",
      "Epoch[10] Validation Loss: 1.662 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.30\n",
      "Epoch[10] Validation Loss: 1.659 \n",
      "10601852 2120371 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 14.17\n",
      "Epoch[0] Validation Loss: 13.993 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.49\n",
      "Epoch[0] Validation Loss: 1.856 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.10\n",
      "Epoch[0] Validation Loss: 1.735 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 1.33\n",
      "Epoch[1] Validation Loss: 1.730 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 1.14\n",
      "Epoch[1] Validation Loss: 1.691 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 1.45\n",
      "Epoch[1] Validation Loss: 1.654 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 1.21\n",
      "Epoch[2] Validation Loss: 1.651 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.60\n",
      "Epoch[2] Validation Loss: 1.640 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[2000] Training Loss: 0.57\n",
      "Epoch[2] Validation Loss: 1.620 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.31\n",
      "Epoch[3] Validation Loss: 1.620 \n",
      "Epoch[3] Iteration[1000] Training Loss: 0.35\n",
      "Epoch[3] Validation Loss: 1.620 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.76\n",
      "Epoch[3] Validation Loss: 1.609 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.29\n",
      "Epoch[4] Validation Loss: 1.605 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 0.31\n",
      "Epoch[4] Validation Loss: 1.614 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.34\n",
      "Epoch[4] Validation Loss: 1.603 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[0] Training Loss: 0.38\n",
      "Epoch[5] Validation Loss: 1.602 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[1000] Training Loss: 0.32\n",
      "Epoch[5] Validation Loss: 1.606 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.26\n",
      "Epoch[5] Validation Loss: 1.600 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[0] Training Loss: 0.15\n",
      "Epoch[6] Validation Loss: 1.600 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[1000] Training Loss: 0.28\n",
      "Epoch[6] Validation Loss: 1.606 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.29\n",
      "Epoch[6] Validation Loss: 1.598 \n",
      "Save best theta...\n",
      "Epoch[7] Iteration[0] Training Loss: 0.14\n",
      "Epoch[7] Validation Loss: 1.598 \n",
      "Save best theta...\n",
      "Epoch[7] Iteration[1000] Training Loss: 0.23\n",
      "Epoch[7] Validation Loss: 1.603 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.34\n",
      "Epoch[7] Validation Loss: 1.599 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.13\n",
      "Epoch[8] Validation Loss: 1.598 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.25\n",
      "Epoch[8] Validation Loss: 1.600 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.43\n",
      "Epoch[8] Validation Loss: 1.597 \n",
      "Save best theta...\n",
      "Epoch[9] Iteration[0] Training Loss: 0.10\n",
      "Epoch[9] Validation Loss: 1.595 \n",
      "Save best theta...\n",
      "Epoch[9] Iteration[1000] Training Loss: 0.21\n",
      "Epoch[9] Validation Loss: 1.600 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.32\n",
      "Epoch[9] Validation Loss: 1.596 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.10\n",
      "Epoch[10] Validation Loss: 1.597 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.20\n",
      "Epoch[10] Validation Loss: 1.599 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.43\n",
      "Epoch[10] Validation Loss: 1.596 \n",
      "10601852 2120372 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 12.82\n",
      "Epoch[0] Validation Loss: 13.631 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.29\n",
      "Epoch[0] Validation Loss: 1.674 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.27\n",
      "Epoch[0] Validation Loss: 1.569 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 1.71\n",
      "Epoch[1] Validation Loss: 1.565 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 0.87\n",
      "Epoch[1] Validation Loss: 1.526 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.71\n",
      "Epoch[1] Validation Loss: 1.492 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.53\n",
      "Epoch[2] Validation Loss: 1.490 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.67\n",
      "Epoch[2] Validation Loss: 1.476 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[2000] Training Loss: 0.57\n",
      "Epoch[2] Validation Loss: 1.460 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.35\n",
      "Epoch[3] Validation Loss: 1.460 \n",
      "Epoch[3] Iteration[1000] Training Loss: 0.43\n",
      "Epoch[3] Validation Loss: 1.458 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[2000] Training Loss: 0.46\n",
      "Epoch[3] Validation Loss: 1.449 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.22\n",
      "Epoch[4] Validation Loss: 1.447 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 0.34\n",
      "Epoch[4] Validation Loss: 1.449 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.35\n",
      "Epoch[4] Validation Loss: 1.443 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[0] Training Loss: 0.22\n",
      "Epoch[5] Validation Loss: 1.443 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[1000] Training Loss: 0.27\n",
      "Epoch[5] Validation Loss: 1.445 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.46\n",
      "Epoch[5] Validation Loss: 1.443 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.29\n",
      "Epoch[6] Validation Loss: 1.441 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[1000] Training Loss: 0.20\n",
      "Epoch[6] Validation Loss: 1.443 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.36\n",
      "Epoch[6] Validation Loss: 1.440 \n",
      "Save best theta...\n",
      "Epoch[7] Iteration[0] Training Loss: 0.13\n",
      "Epoch[7] Validation Loss: 1.440 \n",
      "Save best theta...\n",
      "Epoch[7] Iteration[1000] Training Loss: 0.26\n",
      "Epoch[7] Validation Loss: 1.442 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.21\n",
      "Epoch[7] Validation Loss: 1.440 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.12\n",
      "Epoch[8] Validation Loss: 1.440 \n",
      "Save best theta...\n",
      "Epoch[8] Iteration[1000] Training Loss: 0.20\n",
      "Epoch[8] Validation Loss: 1.442 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[8] Iteration[2000] Training Loss: 0.40\n",
      "Epoch[8] Validation Loss: 1.440 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.11\n",
      "Epoch[9] Validation Loss: 1.440 \n",
      "Save best theta...\n",
      "Epoch[9] Iteration[1000] Training Loss: 0.30\n",
      "Epoch[9] Validation Loss: 1.442 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.37\n",
      "Epoch[9] Validation Loss: 1.440 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.14\n",
      "Epoch[10] Validation Loss: 1.440 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.19\n",
      "Epoch[10] Validation Loss: 1.441 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.30\n",
      "Epoch[10] Validation Loss: 1.440 \n",
      "10601852 2120372 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 13.24\n",
      "Epoch[0] Validation Loss: 13.225 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.14\n",
      "Epoch[0] Validation Loss: 1.247 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.21\n",
      "Epoch[0] Validation Loss: 1.146 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.89\n",
      "Epoch[1] Validation Loss: 1.142 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 0.75\n",
      "Epoch[1] Validation Loss: 1.104 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.91\n",
      "Epoch[1] Validation Loss: 1.071 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.55\n",
      "Epoch[2] Validation Loss: 1.070 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.57\n",
      "Epoch[2] Validation Loss: 1.056 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[2000] Training Loss: 0.51\n",
      "Epoch[2] Validation Loss: 1.043 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.65\n",
      "Epoch[3] Validation Loss: 1.041 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.72\n",
      "Epoch[3] Validation Loss: 1.040 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[2000] Training Loss: 0.48\n",
      "Epoch[3] Validation Loss: 1.032 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.23\n",
      "Epoch[4] Validation Loss: 1.033 \n",
      "Epoch[4] Iteration[1000] Training Loss: 0.68\n",
      "Epoch[4] Validation Loss: 1.034 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.43\n",
      "Epoch[4] Validation Loss: 1.029 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[0] Training Loss: 0.20\n",
      "Epoch[5] Validation Loss: 1.031 \n",
      "Epoch[5] Iteration[1000] Training Loss: 0.25\n",
      "Epoch[5] Validation Loss: 1.032 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.37\n",
      "Epoch[5] Validation Loss: 1.030 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.19\n",
      "Epoch[6] Validation Loss: 1.029 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[1000] Training Loss: 0.23\n",
      "Epoch[6] Validation Loss: 1.033 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.24\n",
      "Epoch[6] Validation Loss: 1.029 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.17\n",
      "Epoch[7] Validation Loss: 1.028 \n",
      "Save best theta...\n",
      "Epoch[7] Iteration[1000] Training Loss: 0.21\n",
      "Epoch[7] Validation Loss: 1.031 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.43\n",
      "Epoch[7] Validation Loss: 1.030 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.11\n",
      "Epoch[8] Validation Loss: 1.029 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.28\n",
      "Epoch[8] Validation Loss: 1.033 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.47\n",
      "Epoch[8] Validation Loss: 1.030 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.14\n",
      "Epoch[9] Validation Loss: 1.029 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.28\n",
      "Epoch[9] Validation Loss: 1.032 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.26\n",
      "Epoch[9] Validation Loss: 1.031 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.13\n",
      "Epoch[10] Validation Loss: 1.030 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.15\n",
      "Epoch[10] Validation Loss: 1.032 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.22\n",
      "Epoch[10] Validation Loss: 1.031 \n",
      "10601852 2120371 1060186 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 13.99\n",
      "Epoch[0] Validation Loss: 13.303 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.38\n",
      "Epoch[0] Validation Loss: 1.253 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.25\n",
      "Epoch[0] Validation Loss: 1.169 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.87\n",
      "Epoch[1] Validation Loss: 1.166 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 1.18\n",
      "Epoch[1] Validation Loss: 1.124 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.79\n",
      "Epoch[1] Validation Loss: 1.092 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.59\n",
      "Epoch[2] Validation Loss: 1.090 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.48\n",
      "Epoch[2] Validation Loss: 1.077 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[2000] Training Loss: 0.67\n",
      "Epoch[2] Validation Loss: 1.064 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.38\n",
      "Epoch[3] Validation Loss: 1.062 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.42\n",
      "Epoch[3] Validation Loss: 1.059 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[2000] Training Loss: 0.47\n",
      "Epoch[3] Validation Loss: 1.054 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.47\n",
      "Epoch[4] Validation Loss: 1.053 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 0.39\n",
      "Epoch[4] Validation Loss: 1.055 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.36\n",
      "Epoch[4] Validation Loss: 1.051 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[0] Training Loss: 0.25\n",
      "Epoch[5] Validation Loss: 1.051 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[1000] Training Loss: 0.26\n",
      "Epoch[5] Validation Loss: 1.053 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.36\n",
      "Epoch[5] Validation Loss: 1.052 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.17\n",
      "Epoch[6] Validation Loss: 1.050 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[1000] Training Loss: 0.21\n",
      "Epoch[6] Validation Loss: 1.052 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.38\n",
      "Epoch[6] Validation Loss: 1.054 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.17\n",
      "Epoch[7] Validation Loss: 1.053 \n",
      "Epoch[7] Iteration[1000] Training Loss: 0.29\n",
      "Epoch[7] Validation Loss: 1.054 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.32\n",
      "Epoch[7] Validation Loss: 1.055 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.16\n",
      "Epoch[8] Validation Loss: 1.052 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.31\n",
      "Epoch[8] Validation Loss: 1.056 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.34\n",
      "Epoch[8] Validation Loss: 1.054 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.14\n",
      "Epoch[9] Validation Loss: 1.056 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.20\n",
      "Epoch[9] Validation Loss: 1.054 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.40\n",
      "Epoch[9] Validation Loss: 1.054 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.08\n",
      "Epoch[10] Validation Loss: 1.058 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.23\n",
      "Epoch[10] Validation Loss: 1.055 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.25\n",
      "Epoch[10] Validation Loss: 1.056 \n",
      "10601852 2120371 1060185 1060186\n",
      "Epoch[0] Iteration[0] Training Loss: 14.07\n",
      "Epoch[0] Validation Loss: 13.159 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.49\n",
      "Epoch[0] Validation Loss: 1.252 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.09\n",
      "Epoch[0] Validation Loss: 1.170 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.87\n",
      "Epoch[1] Validation Loss: 1.166 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 0.72\n",
      "Epoch[1] Validation Loss: 1.126 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.79\n",
      "Epoch[1] Validation Loss: 1.096 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.85\n",
      "Epoch[2] Validation Loss: 1.094 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.56\n",
      "Epoch[2] Validation Loss: 1.082 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[2000] Training Loss: 1.46\n",
      "Epoch[2] Validation Loss: 1.071 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.45\n",
      "Epoch[3] Validation Loss: 1.069 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.46\n",
      "Epoch[3] Validation Loss: 1.069 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[2000] Training Loss: 0.85\n",
      "Epoch[3] Validation Loss: 1.065 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.28\n",
      "Epoch[4] Validation Loss: 1.065 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 0.70\n",
      "Epoch[4] Validation Loss: 1.069 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.42\n",
      "Epoch[4] Validation Loss: 1.067 \n",
      "Epoch[5] Iteration[0] Training Loss: 0.22\n",
      "Epoch[5] Validation Loss: 1.066 \n",
      "Epoch[5] Iteration[1000] Training Loss: 0.34\n",
      "Epoch[5] Validation Loss: 1.069 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.35\n",
      "Epoch[5] Validation Loss: 1.070 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.23\n",
      "Epoch[6] Validation Loss: 1.070 \n",
      "Epoch[6] Iteration[1000] Training Loss: 0.31\n",
      "Epoch[6] Validation Loss: 1.072 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.33\n",
      "Epoch[6] Validation Loss: 1.072 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7] Iteration[0] Training Loss: 0.14\n",
      "Epoch[7] Validation Loss: 1.073 \n",
      "Epoch[7] Iteration[1000] Training Loss: 0.24\n",
      "Epoch[7] Validation Loss: 1.074 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.40\n",
      "Epoch[7] Validation Loss: 1.075 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.16\n",
      "Epoch[8] Validation Loss: 1.075 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.14\n",
      "Epoch[8] Validation Loss: 1.077 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.46\n",
      "Epoch[8] Validation Loss: 1.078 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.10\n",
      "Epoch[9] Validation Loss: 1.079 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.26\n",
      "Epoch[9] Validation Loss: 1.079 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.25\n",
      "Epoch[9] Validation Loss: 1.081 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.10\n",
      "Epoch[10] Validation Loss: 1.077 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.15\n",
      "Epoch[10] Validation Loss: 1.078 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.33\n",
      "Epoch[10] Validation Loss: 1.084 \n",
      "10601852 2120371 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 12.06\n",
      "Epoch[0] Validation Loss: 14.315 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 2.33\n",
      "Epoch[0] Validation Loss: 1.438 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.28\n",
      "Epoch[0] Validation Loss: 1.249 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 1.27\n",
      "Epoch[1] Validation Loss: 1.244 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 1.72\n",
      "Epoch[1] Validation Loss: 1.229 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 1.17\n",
      "Epoch[1] Validation Loss: 1.208 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.73\n",
      "Epoch[2] Validation Loss: 1.201 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 1.22\n",
      "Epoch[2] Validation Loss: 1.192 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[2000] Training Loss: 0.91\n",
      "Epoch[2] Validation Loss: 1.176 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 1.21\n",
      "Epoch[3] Validation Loss: 1.188 \n",
      "Epoch[3] Iteration[1000] Training Loss: 1.06\n",
      "Epoch[3] Validation Loss: 1.209 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.84\n",
      "Epoch[3] Validation Loss: 1.204 \n",
      "Epoch[4] Iteration[0] Training Loss: 0.53\n",
      "Epoch[4] Validation Loss: 1.191 \n",
      "Epoch[4] Iteration[1000] Training Loss: 0.89\n",
      "Epoch[4] Validation Loss: 1.206 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.76\n",
      "Epoch[4] Validation Loss: 1.197 \n",
      "Epoch[5] Iteration[0] Training Loss: 0.39\n",
      "Epoch[5] Validation Loss: 1.195 \n",
      "Epoch[5] Iteration[1000] Training Loss: 0.62\n",
      "Epoch[5] Validation Loss: 1.206 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.55\n",
      "Epoch[5] Validation Loss: 1.205 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.27\n",
      "Epoch[6] Validation Loss: 1.211 \n",
      "Epoch[6] Iteration[1000] Training Loss: 0.41\n",
      "Epoch[6] Validation Loss: 1.232 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.61\n",
      "Epoch[6] Validation Loss: 1.224 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.35\n",
      "Epoch[7] Validation Loss: 1.201 \n",
      "Epoch[7] Iteration[1000] Training Loss: 0.55\n",
      "Epoch[7] Validation Loss: 1.220 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.57\n",
      "Epoch[7] Validation Loss: 1.209 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.20\n",
      "Epoch[8] Validation Loss: 1.194 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.31\n",
      "Epoch[8] Validation Loss: 1.229 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.60\n",
      "Epoch[8] Validation Loss: 1.199 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.32\n",
      "Epoch[9] Validation Loss: 1.204 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.50\n",
      "Epoch[9] Validation Loss: 1.206 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.46\n",
      "Epoch[9] Validation Loss: 1.187 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.22\n",
      "Epoch[10] Validation Loss: 1.190 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.35\n",
      "Epoch[10] Validation Loss: 1.199 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.38\n",
      "Epoch[10] Validation Loss: 1.179 \n",
      "10601852 3180556 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 13.59\n",
      "Epoch[0] Validation Loss: 14.197 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.24\n",
      "Epoch[0] Validation Loss: 1.925 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.16\n",
      "Epoch[0] Validation Loss: 1.802 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.49\n",
      "Epoch[0] Validation Loss: 1.747 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 1.09\n",
      "Epoch[1] Validation Loss: 1.743 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 0.77\n",
      "Epoch[1] Validation Loss: 1.716 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 1.43\n",
      "Epoch[1] Validation Loss: 1.687 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[3000] Training Loss: 0.80\n",
      "Epoch[1] Validation Loss: 1.663 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.84\n",
      "Epoch[2] Validation Loss: 1.661 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.53\n",
      "Epoch[2] Validation Loss: 1.660 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[2000] Training Loss: 0.86\n",
      "Epoch[2] Validation Loss: 1.647 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[3000] Training Loss: 1.23\n",
      "Epoch[2] Validation Loss: 1.634 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.38\n",
      "Epoch[3] Validation Loss: 1.635 \n",
      "Epoch[3] Iteration[1000] Training Loss: 0.82\n",
      "Epoch[3] Validation Loss: 1.641 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.74\n",
      "Epoch[3] Validation Loss: 1.635 \n",
      "Epoch[3] Iteration[3000] Training Loss: 1.03\n",
      "Epoch[3] Validation Loss: 1.625 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.39\n",
      "Epoch[4] Validation Loss: 1.626 \n",
      "Epoch[4] Iteration[1000] Training Loss: 0.64\n",
      "Epoch[4] Validation Loss: 1.633 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.84\n",
      "Epoch[4] Validation Loss: 1.626 \n",
      "Epoch[4] Iteration[3000] Training Loss: 0.97\n",
      "Epoch[4] Validation Loss: 1.620 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[0] Training Loss: 0.43\n",
      "Epoch[5] Validation Loss: 1.620 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[1000] Training Loss: 0.28\n",
      "Epoch[5] Validation Loss: 1.625 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.75\n",
      "Epoch[5] Validation Loss: 1.624 \n",
      "Epoch[5] Iteration[3000] Training Loss: 0.84\n",
      "Epoch[5] Validation Loss: 1.618 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[0] Training Loss: 0.52\n",
      "Epoch[6] Validation Loss: 1.617 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[1000] Training Loss: 0.33\n",
      "Epoch[6] Validation Loss: 1.628 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.52\n",
      "Epoch[6] Validation Loss: 1.623 \n",
      "Epoch[6] Iteration[3000] Training Loss: 0.72\n",
      "Epoch[6] Validation Loss: 1.618 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.57\n",
      "Epoch[7] Validation Loss: 1.617 \n",
      "Save best theta...\n",
      "Epoch[7] Iteration[1000] Training Loss: 0.50\n",
      "Epoch[7] Validation Loss: 1.626 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.47\n",
      "Epoch[7] Validation Loss: 1.624 \n",
      "Epoch[7] Iteration[3000] Training Loss: 0.85\n",
      "Epoch[7] Validation Loss: 1.620 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.45\n",
      "Epoch[8] Validation Loss: 1.618 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.40\n",
      "Epoch[8] Validation Loss: 1.626 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.43\n",
      "Epoch[8] Validation Loss: 1.625 \n",
      "Epoch[8] Iteration[3000] Training Loss: 0.69\n",
      "Epoch[8] Validation Loss: 1.620 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.50\n",
      "Epoch[9] Validation Loss: 1.618 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.32\n",
      "Epoch[9] Validation Loss: 1.622 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.38\n",
      "Epoch[9] Validation Loss: 1.625 \n",
      "Epoch[9] Iteration[3000] Training Loss: 0.38\n",
      "Epoch[9] Validation Loss: 1.618 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.22\n",
      "Epoch[10] Validation Loss: 1.617 \n",
      "Save best theta...\n",
      "Epoch[10] Iteration[1000] Training Loss: 0.31\n",
      "Epoch[10] Validation Loss: 1.625 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.61\n",
      "Epoch[10] Validation Loss: 1.624 \n",
      "Epoch[10] Iteration[3000] Training Loss: 0.48\n",
      "Epoch[10] Validation Loss: 1.618 \n",
      "10601852 3180557 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 13.72\n",
      "Epoch[0] Validation Loss: 14.008 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.58\n",
      "Epoch[0] Validation Loss: 1.858 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.59\n",
      "Epoch[0] Validation Loss: 1.743 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.11\n",
      "Epoch[0] Validation Loss: 1.690 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 2.75\n",
      "Epoch[1] Validation Loss: 1.684 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 0.78\n",
      "Epoch[1] Validation Loss: 1.656 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.78\n",
      "Epoch[1] Validation Loss: 1.629 \n",
      "Save best theta...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Iteration[3000] Training Loss: 1.15\n",
      "Epoch[1] Validation Loss: 1.606 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.51\n",
      "Epoch[2] Validation Loss: 1.604 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.53\n",
      "Epoch[2] Validation Loss: 1.602 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[2000] Training Loss: 1.01\n",
      "Epoch[2] Validation Loss: 1.589 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[3000] Training Loss: 0.79\n",
      "Epoch[2] Validation Loss: 1.578 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.35\n",
      "Epoch[3] Validation Loss: 1.578 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.46\n",
      "Epoch[3] Validation Loss: 1.582 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.55\n",
      "Epoch[3] Validation Loss: 1.578 \n",
      "Epoch[3] Iteration[3000] Training Loss: 0.87\n",
      "Epoch[3] Validation Loss: 1.568 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.30\n",
      "Epoch[4] Validation Loss: 1.568 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 0.40\n",
      "Epoch[4] Validation Loss: 1.576 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.42\n",
      "Epoch[4] Validation Loss: 1.570 \n",
      "Epoch[4] Iteration[3000] Training Loss: 0.45\n",
      "Epoch[4] Validation Loss: 1.566 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[0] Training Loss: 0.49\n",
      "Epoch[5] Validation Loss: 1.564 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[1000] Training Loss: 0.29\n",
      "Epoch[5] Validation Loss: 1.567 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.58\n",
      "Epoch[5] Validation Loss: 1.567 \n",
      "Epoch[5] Iteration[3000] Training Loss: 1.12\n",
      "Epoch[5] Validation Loss: 1.563 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[0] Training Loss: 0.28\n",
      "Epoch[6] Validation Loss: 1.564 \n",
      "Epoch[6] Iteration[1000] Training Loss: 0.76\n",
      "Epoch[6] Validation Loss: 1.568 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.55\n",
      "Epoch[6] Validation Loss: 1.569 \n",
      "Epoch[6] Iteration[3000] Training Loss: 0.60\n",
      "Epoch[6] Validation Loss: 1.563 \n",
      "Save best theta...\n",
      "Epoch[7] Iteration[0] Training Loss: 0.43\n",
      "Epoch[7] Validation Loss: 1.561 \n",
      "Save best theta...\n",
      "Epoch[7] Iteration[1000] Training Loss: 0.65\n",
      "Epoch[7] Validation Loss: 1.567 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.56\n",
      "Epoch[7] Validation Loss: 1.568 \n",
      "Epoch[7] Iteration[3000] Training Loss: 0.44\n",
      "Epoch[7] Validation Loss: 1.564 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.37\n",
      "Epoch[8] Validation Loss: 1.562 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.54\n",
      "Epoch[8] Validation Loss: 1.568 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.43\n",
      "Epoch[8] Validation Loss: 1.566 \n",
      "Epoch[8] Iteration[3000] Training Loss: 0.60\n",
      "Epoch[8] Validation Loss: 1.563 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.25\n",
      "Epoch[9] Validation Loss: 1.563 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.43\n",
      "Epoch[9] Validation Loss: 1.567 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.46\n",
      "Epoch[9] Validation Loss: 1.567 \n",
      "Epoch[9] Iteration[3000] Training Loss: 0.87\n",
      "Epoch[9] Validation Loss: 1.564 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.41\n",
      "Epoch[10] Validation Loss: 1.563 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.31\n",
      "Epoch[10] Validation Loss: 1.568 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.52\n",
      "Epoch[10] Validation Loss: 1.568 \n",
      "Epoch[10] Iteration[3000] Training Loss: 0.39\n",
      "Epoch[10] Validation Loss: 1.565 \n",
      "10601852 3180557 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 12.63\n",
      "Epoch[0] Validation Loss: 13.631 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.47\n",
      "Epoch[0] Validation Loss: 1.672 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.11\n",
      "Epoch[0] Validation Loss: 1.573 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.07\n",
      "Epoch[0] Validation Loss: 1.523 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.86\n",
      "Epoch[1] Validation Loss: 1.518 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 0.75\n",
      "Epoch[1] Validation Loss: 1.492 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.79\n",
      "Epoch[1] Validation Loss: 1.467 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[3000] Training Loss: 0.79\n",
      "Epoch[1] Validation Loss: 1.448 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.50\n",
      "Epoch[2] Validation Loss: 1.445 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.50\n",
      "Epoch[2] Validation Loss: 1.442 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[2000] Training Loss: 0.59\n",
      "Epoch[2] Validation Loss: 1.432 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[3000] Training Loss: 0.72\n",
      "Epoch[2] Validation Loss: 1.423 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.34\n",
      "Epoch[3] Validation Loss: 1.422 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.46\n",
      "Epoch[3] Validation Loss: 1.423 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.54\n",
      "Epoch[3] Validation Loss: 1.421 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[3000] Training Loss: 0.66\n",
      "Epoch[3] Validation Loss: 1.416 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.35\n",
      "Epoch[4] Validation Loss: 1.415 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 0.32\n",
      "Epoch[4] Validation Loss: 1.417 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.48\n",
      "Epoch[4] Validation Loss: 1.417 \n",
      "Epoch[4] Iteration[3000] Training Loss: 0.63\n",
      "Epoch[4] Validation Loss: 1.413 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[0] Training Loss: 0.32\n",
      "Epoch[5] Validation Loss: 1.413 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[1000] Training Loss: 0.39\n",
      "Epoch[5] Validation Loss: 1.416 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.50\n",
      "Epoch[5] Validation Loss: 1.416 \n",
      "Epoch[5] Iteration[3000] Training Loss: 0.40\n",
      "Epoch[5] Validation Loss: 1.412 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[0] Training Loss: 0.33\n",
      "Epoch[6] Validation Loss: 1.411 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[1000] Training Loss: 0.39\n",
      "Epoch[6] Validation Loss: 1.415 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.48\n",
      "Epoch[6] Validation Loss: 1.415 \n",
      "Epoch[6] Iteration[3000] Training Loss: 0.65\n",
      "Epoch[6] Validation Loss: 1.412 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.26\n",
      "Epoch[7] Validation Loss: 1.411 \n",
      "Epoch[7] Iteration[1000] Training Loss: 0.29\n",
      "Epoch[7] Validation Loss: 1.415 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.54\n",
      "Epoch[7] Validation Loss: 1.417 \n",
      "Epoch[7] Iteration[3000] Training Loss: 0.52\n",
      "Epoch[7] Validation Loss: 1.412 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.31\n",
      "Epoch[8] Validation Loss: 1.412 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.30\n",
      "Epoch[8] Validation Loss: 1.415 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.34\n",
      "Epoch[8] Validation Loss: 1.416 \n",
      "Epoch[8] Iteration[3000] Training Loss: 0.61\n",
      "Epoch[8] Validation Loss: 1.413 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.33\n",
      "Epoch[9] Validation Loss: 1.412 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.41\n",
      "Epoch[9] Validation Loss: 1.415 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.41\n",
      "Epoch[9] Validation Loss: 1.416 \n",
      "Epoch[9] Iteration[3000] Training Loss: 0.56\n",
      "Epoch[9] Validation Loss: 1.413 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.25\n",
      "Epoch[10] Validation Loss: 1.413 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.27\n",
      "Epoch[10] Validation Loss: 1.416 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.43\n",
      "Epoch[10] Validation Loss: 1.417 \n",
      "Epoch[10] Iteration[3000] Training Loss: 0.55\n",
      "Epoch[10] Validation Loss: 1.414 \n",
      "10601852 3180557 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 13.20\n",
      "Epoch[0] Validation Loss: 13.157 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.36\n",
      "Epoch[0] Validation Loss: 1.235 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.14\n",
      "Epoch[0] Validation Loss: 1.153 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.09\n",
      "Epoch[0] Validation Loss: 1.106 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.71\n",
      "Epoch[1] Validation Loss: 1.101 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 0.88\n",
      "Epoch[1] Validation Loss: 1.072 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.82\n",
      "Epoch[1] Validation Loss: 1.048 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[3000] Training Loss: 0.91\n",
      "Epoch[1] Validation Loss: 1.030 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.54\n",
      "Epoch[2] Validation Loss: 1.028 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.65\n",
      "Epoch[2] Validation Loss: 1.024 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[2000] Training Loss: 0.56\n",
      "Epoch[2] Validation Loss: 1.018 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[3000] Training Loss: 0.72\n",
      "Epoch[2] Validation Loss: 1.013 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.48\n",
      "Epoch[3] Validation Loss: 1.008 \n",
      "Save best theta...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3] Iteration[1000] Training Loss: 0.47\n",
      "Epoch[3] Validation Loss: 1.010 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.47\n",
      "Epoch[3] Validation Loss: 1.009 \n",
      "Epoch[3] Iteration[3000] Training Loss: 0.61\n",
      "Epoch[3] Validation Loss: 1.006 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.40\n",
      "Epoch[4] Validation Loss: 1.004 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 0.55\n",
      "Epoch[4] Validation Loss: 1.007 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.48\n",
      "Epoch[4] Validation Loss: 1.008 \n",
      "Epoch[4] Iteration[3000] Training Loss: 0.60\n",
      "Epoch[4] Validation Loss: 1.004 \n",
      "Epoch[5] Iteration[0] Training Loss: 0.28\n",
      "Epoch[5] Validation Loss: 1.003 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[1000] Training Loss: 0.37\n",
      "Epoch[5] Validation Loss: 1.007 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.52\n",
      "Epoch[5] Validation Loss: 1.010 \n",
      "Epoch[5] Iteration[3000] Training Loss: 0.64\n",
      "Epoch[5] Validation Loss: 1.004 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.32\n",
      "Epoch[6] Validation Loss: 1.006 \n",
      "Epoch[6] Iteration[1000] Training Loss: 0.39\n",
      "Epoch[6] Validation Loss: 1.008 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.44\n",
      "Epoch[6] Validation Loss: 1.009 \n",
      "Epoch[6] Iteration[3000] Training Loss: 0.44\n",
      "Epoch[6] Validation Loss: 1.007 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.29\n",
      "Epoch[7] Validation Loss: 1.006 \n",
      "Epoch[7] Iteration[1000] Training Loss: 0.40\n",
      "Epoch[7] Validation Loss: 1.009 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.34\n",
      "Epoch[7] Validation Loss: 1.011 \n",
      "Epoch[7] Iteration[3000] Training Loss: 0.62\n",
      "Epoch[7] Validation Loss: 1.006 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.26\n",
      "Epoch[8] Validation Loss: 1.006 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.37\n",
      "Epoch[8] Validation Loss: 1.010 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.33\n",
      "Epoch[8] Validation Loss: 1.011 \n",
      "Epoch[8] Iteration[3000] Training Loss: 0.44\n",
      "Epoch[8] Validation Loss: 1.008 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.30\n",
      "Epoch[9] Validation Loss: 1.007 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.23\n",
      "Epoch[9] Validation Loss: 1.011 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.53\n",
      "Epoch[9] Validation Loss: 1.012 \n",
      "Epoch[9] Iteration[3000] Training Loss: 0.56\n",
      "Epoch[9] Validation Loss: 1.010 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.25\n",
      "Epoch[10] Validation Loss: 1.008 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.40\n",
      "Epoch[10] Validation Loss: 1.013 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.35\n",
      "Epoch[10] Validation Loss: 1.012 \n",
      "Epoch[10] Iteration[3000] Training Loss: 0.38\n",
      "Epoch[10] Validation Loss: 1.009 \n",
      "10601852 3180556 1060186 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 14.07\n",
      "Epoch[0] Validation Loss: 13.270 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.98\n",
      "Epoch[0] Validation Loss: 1.287 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.17\n",
      "Epoch[0] Validation Loss: 1.192 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.12\n",
      "Epoch[0] Validation Loss: 1.143 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 1.56\n",
      "Epoch[1] Validation Loss: 1.138 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 1.83\n",
      "Epoch[1] Validation Loss: 1.110 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.70\n",
      "Epoch[1] Validation Loss: 1.086 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[3000] Training Loss: 1.24\n",
      "Epoch[1] Validation Loss: 1.065 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.50\n",
      "Epoch[2] Validation Loss: 1.063 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.96\n",
      "Epoch[2] Validation Loss: 1.061 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[2000] Training Loss: 0.61\n",
      "Epoch[2] Validation Loss: 1.054 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[3000] Training Loss: 0.75\n",
      "Epoch[2] Validation Loss: 1.042 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.36\n",
      "Epoch[3] Validation Loss: 1.041 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.48\n",
      "Epoch[3] Validation Loss: 1.043 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.49\n",
      "Epoch[3] Validation Loss: 1.041 \n",
      "Epoch[3] Iteration[3000] Training Loss: 0.65\n",
      "Epoch[3] Validation Loss: 1.037 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.58\n",
      "Epoch[4] Validation Loss: 1.037 \n",
      "Epoch[4] Iteration[1000] Training Loss: 0.69\n",
      "Epoch[4] Validation Loss: 1.040 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.51\n",
      "Epoch[4] Validation Loss: 1.039 \n",
      "Epoch[4] Iteration[3000] Training Loss: 0.59\n",
      "Epoch[4] Validation Loss: 1.036 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[0] Training Loss: 0.31\n",
      "Epoch[5] Validation Loss: 1.036 \n",
      "Epoch[5] Iteration[1000] Training Loss: 0.44\n",
      "Epoch[5] Validation Loss: 1.041 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.70\n",
      "Epoch[5] Validation Loss: 1.040 \n",
      "Epoch[5] Iteration[3000] Training Loss: 0.88\n",
      "Epoch[5] Validation Loss: 1.037 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.29\n",
      "Epoch[6] Validation Loss: 1.037 \n",
      "Epoch[6] Iteration[1000] Training Loss: 0.32\n",
      "Epoch[6] Validation Loss: 1.040 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.37\n",
      "Epoch[6] Validation Loss: 1.041 \n",
      "Epoch[6] Iteration[3000] Training Loss: 0.49\n",
      "Epoch[6] Validation Loss: 1.038 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.26\n",
      "Epoch[7] Validation Loss: 1.037 \n",
      "Epoch[7] Iteration[1000] Training Loss: 0.38\n",
      "Epoch[7] Validation Loss: 1.041 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.46\n",
      "Epoch[7] Validation Loss: 1.042 \n",
      "Epoch[7] Iteration[3000] Training Loss: 0.86\n",
      "Epoch[7] Validation Loss: 1.038 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.29\n",
      "Epoch[8] Validation Loss: 1.039 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.35\n",
      "Epoch[8] Validation Loss: 1.042 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.56\n",
      "Epoch[8] Validation Loss: 1.042 \n",
      "Epoch[8] Iteration[3000] Training Loss: 0.55\n",
      "Epoch[8] Validation Loss: 1.038 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.44\n",
      "Epoch[9] Validation Loss: 1.038 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.41\n",
      "Epoch[9] Validation Loss: 1.041 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.51\n",
      "Epoch[9] Validation Loss: 1.043 \n",
      "Epoch[9] Iteration[3000] Training Loss: 0.84\n",
      "Epoch[9] Validation Loss: 1.041 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.28\n",
      "Epoch[10] Validation Loss: 1.040 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.47\n",
      "Epoch[10] Validation Loss: 1.043 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.44\n",
      "Epoch[10] Validation Loss: 1.046 \n",
      "Epoch[10] Iteration[3000] Training Loss: 0.77\n",
      "Epoch[10] Validation Loss: 1.044 \n",
      "10601852 3180556 1060185 1060186\n",
      "Epoch[0] Iteration[0] Training Loss: 12.74\n",
      "Epoch[0] Validation Loss: 13.105 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 2.58\n",
      "Epoch[0] Validation Loss: 1.267 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.14\n",
      "Epoch[0] Validation Loss: 1.172 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.41\n",
      "Epoch[0] Validation Loss: 1.128 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.76\n",
      "Epoch[1] Validation Loss: 1.123 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 0.81\n",
      "Epoch[1] Validation Loss: 1.100 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.82\n",
      "Epoch[1] Validation Loss: 1.078 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[3000] Training Loss: 1.17\n",
      "Epoch[1] Validation Loss: 1.062 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.96\n",
      "Epoch[2] Validation Loss: 1.059 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.81\n",
      "Epoch[2] Validation Loss: 1.057 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[2000] Training Loss: 0.57\n",
      "Epoch[2] Validation Loss: 1.052 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[3000] Training Loss: 0.87\n",
      "Epoch[2] Validation Loss: 1.046 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.89\n",
      "Epoch[3] Validation Loss: 1.045 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.43\n",
      "Epoch[3] Validation Loss: 1.050 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.91\n",
      "Epoch[3] Validation Loss: 1.048 \n",
      "Epoch[3] Iteration[3000] Training Loss: 0.86\n",
      "Epoch[3] Validation Loss: 1.044 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.65\n",
      "Epoch[4] Validation Loss: 1.044 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 0.40\n",
      "Epoch[4] Validation Loss: 1.049 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.46\n",
      "Epoch[4] Validation Loss: 1.052 \n",
      "Epoch[4] Iteration[3000] Training Loss: 0.82\n",
      "Epoch[4] Validation Loss: 1.046 \n",
      "Epoch[5] Iteration[0] Training Loss: 0.43\n",
      "Epoch[5] Validation Loss: 1.046 \n",
      "Epoch[5] Iteration[1000] Training Loss: 0.55\n",
      "Epoch[5] Validation Loss: 1.051 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5] Iteration[2000] Training Loss: 0.62\n",
      "Epoch[5] Validation Loss: 1.052 \n",
      "Epoch[5] Iteration[3000] Training Loss: 0.85\n",
      "Epoch[5] Validation Loss: 1.048 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.26\n",
      "Epoch[6] Validation Loss: 1.047 \n",
      "Epoch[6] Iteration[1000] Training Loss: 0.55\n",
      "Epoch[6] Validation Loss: 1.052 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.83\n",
      "Epoch[6] Validation Loss: 1.053 \n",
      "Epoch[6] Iteration[3000] Training Loss: 0.55\n",
      "Epoch[6] Validation Loss: 1.050 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.39\n",
      "Epoch[7] Validation Loss: 1.050 \n",
      "Epoch[7] Iteration[1000] Training Loss: 0.72\n",
      "Epoch[7] Validation Loss: 1.055 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.70\n",
      "Epoch[7] Validation Loss: 1.054 \n",
      "Epoch[7] Iteration[3000] Training Loss: 1.19\n",
      "Epoch[7] Validation Loss: 1.053 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.43\n",
      "Epoch[8] Validation Loss: 1.051 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.53\n",
      "Epoch[8] Validation Loss: 1.055 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.57\n",
      "Epoch[8] Validation Loss: 1.057 \n",
      "Epoch[8] Iteration[3000] Training Loss: 0.58\n",
      "Epoch[8] Validation Loss: 1.053 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.41\n",
      "Epoch[9] Validation Loss: 1.052 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.46\n",
      "Epoch[9] Validation Loss: 1.056 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.56\n",
      "Epoch[9] Validation Loss: 1.057 \n",
      "Epoch[9] Iteration[3000] Training Loss: 0.68\n",
      "Epoch[9] Validation Loss: 1.055 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.29\n",
      "Epoch[10] Validation Loss: 1.055 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.47\n",
      "Epoch[10] Validation Loss: 1.059 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.82\n",
      "Epoch[10] Validation Loss: 1.059 \n",
      "Epoch[10] Iteration[3000] Training Loss: 0.69\n",
      "Epoch[10] Validation Loss: 1.057 \n",
      "10601852 4240742 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 13.23\n",
      "Epoch[0] Validation Loss: 14.199 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.83\n",
      "Epoch[0] Validation Loss: 1.935 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.81\n",
      "Epoch[0] Validation Loss: 1.810 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.80\n",
      "Epoch[0] Validation Loss: 1.755 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[4000] Training Loss: 1.19\n",
      "Epoch[0] Validation Loss: 1.713 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.59\n",
      "Epoch[1] Validation Loss: 1.708 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 0.57\n",
      "Epoch[1] Validation Loss: 1.689 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.73\n",
      "Epoch[1] Validation Loss: 1.668 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[3000] Training Loss: 1.16\n",
      "Epoch[1] Validation Loss: 1.650 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[4000] Training Loss: 1.19\n",
      "Epoch[1] Validation Loss: 1.632 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.53\n",
      "Epoch[2] Validation Loss: 1.630 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.72\n",
      "Epoch[2] Validation Loss: 1.632 \n",
      "Epoch[2] Iteration[2000] Training Loss: 0.71\n",
      "Epoch[2] Validation Loss: 1.626 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[3000] Training Loss: 0.70\n",
      "Epoch[2] Validation Loss: 1.619 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[4000] Training Loss: 0.87\n",
      "Epoch[2] Validation Loss: 1.611 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.37\n",
      "Epoch[3] Validation Loss: 1.608 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.68\n",
      "Epoch[3] Validation Loss: 1.614 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.90\n",
      "Epoch[3] Validation Loss: 1.614 \n",
      "Epoch[3] Iteration[3000] Training Loss: 0.73\n",
      "Epoch[3] Validation Loss: 1.609 \n",
      "Epoch[3] Iteration[4000] Training Loss: 0.85\n",
      "Epoch[3] Validation Loss: 1.604 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.79\n",
      "Epoch[4] Validation Loss: 1.602 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 0.40\n",
      "Epoch[4] Validation Loss: 1.611 \n",
      "Epoch[4] Iteration[2000] Training Loss: 1.02\n",
      "Epoch[4] Validation Loss: 1.608 \n",
      "Epoch[4] Iteration[3000] Training Loss: 0.64\n",
      "Epoch[4] Validation Loss: 1.605 \n",
      "Epoch[4] Iteration[4000] Training Loss: 1.15\n",
      "Epoch[4] Validation Loss: 1.603 \n",
      "Epoch[5] Iteration[0] Training Loss: 0.79\n",
      "Epoch[5] Validation Loss: 1.598 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[1000] Training Loss: 0.51\n",
      "Epoch[5] Validation Loss: 1.609 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.75\n",
      "Epoch[5] Validation Loss: 1.609 \n",
      "Epoch[5] Iteration[3000] Training Loss: 0.61\n",
      "Epoch[5] Validation Loss: 1.606 \n",
      "Epoch[5] Iteration[4000] Training Loss: 0.78\n",
      "Epoch[5] Validation Loss: 1.599 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.71\n",
      "Epoch[6] Validation Loss: 1.597 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[1000] Training Loss: 0.48\n",
      "Epoch[6] Validation Loss: 1.605 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.67\n",
      "Epoch[6] Validation Loss: 1.606 \n",
      "Epoch[6] Iteration[3000] Training Loss: 0.66\n",
      "Epoch[6] Validation Loss: 1.603 \n",
      "Epoch[6] Iteration[4000] Training Loss: 0.97\n",
      "Epoch[6] Validation Loss: 1.601 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.43\n",
      "Epoch[7] Validation Loss: 1.598 \n",
      "Epoch[7] Iteration[1000] Training Loss: 0.95\n",
      "Epoch[7] Validation Loss: 1.606 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.57\n",
      "Epoch[7] Validation Loss: 1.606 \n",
      "Epoch[7] Iteration[3000] Training Loss: 1.06\n",
      "Epoch[7] Validation Loss: 1.604 \n",
      "Epoch[7] Iteration[4000] Training Loss: 0.76\n",
      "Epoch[7] Validation Loss: 1.599 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.45\n",
      "Epoch[8] Validation Loss: 1.598 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.45\n",
      "Epoch[8] Validation Loss: 1.607 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.90\n",
      "Epoch[8] Validation Loss: 1.607 \n",
      "Epoch[8] Iteration[3000] Training Loss: 0.71\n",
      "Epoch[8] Validation Loss: 1.603 \n",
      "Epoch[8] Iteration[4000] Training Loss: 0.57\n",
      "Epoch[8] Validation Loss: 1.599 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.37\n",
      "Epoch[9] Validation Loss: 1.599 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.66\n",
      "Epoch[9] Validation Loss: 1.604 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.81\n",
      "Epoch[9] Validation Loss: 1.607 \n",
      "Epoch[9] Iteration[3000] Training Loss: 0.84\n",
      "Epoch[9] Validation Loss: 1.603 \n",
      "Epoch[9] Iteration[4000] Training Loss: 0.97\n",
      "Epoch[9] Validation Loss: 1.601 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.59\n",
      "Epoch[10] Validation Loss: 1.600 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.77\n",
      "Epoch[10] Validation Loss: 1.608 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.47\n",
      "Epoch[10] Validation Loss: 1.607 \n",
      "Epoch[10] Iteration[3000] Training Loss: 0.89\n",
      "Epoch[10] Validation Loss: 1.607 \n",
      "Epoch[10] Iteration[4000] Training Loss: 0.83\n",
      "Epoch[10] Validation Loss: 1.601 \n",
      "10601852 4240742 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 13.34\n",
      "Epoch[0] Validation Loss: 14.015 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.35\n",
      "Epoch[0] Validation Loss: 1.850 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.25\n",
      "Epoch[0] Validation Loss: 1.737 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.23\n",
      "Epoch[0] Validation Loss: 1.686 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[4000] Training Loss: 1.36\n",
      "Epoch[0] Validation Loss: 1.647 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.67\n",
      "Epoch[1] Validation Loss: 1.642 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 0.74\n",
      "Epoch[1] Validation Loss: 1.624 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.83\n",
      "Epoch[1] Validation Loss: 1.605 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[3000] Training Loss: 0.80\n",
      "Epoch[1] Validation Loss: 1.589 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[4000] Training Loss: 0.83\n",
      "Epoch[1] Validation Loss: 1.575 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.55\n",
      "Epoch[2] Validation Loss: 1.572 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.48\n",
      "Epoch[2] Validation Loss: 1.574 \n",
      "Epoch[2] Iteration[2000] Training Loss: 0.59\n",
      "Epoch[2] Validation Loss: 1.568 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[3000] Training Loss: 1.05\n",
      "Epoch[2] Validation Loss: 1.563 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[4000] Training Loss: 0.70\n",
      "Epoch[2] Validation Loss: 1.556 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.57\n",
      "Epoch[3] Validation Loss: 1.555 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.53\n",
      "Epoch[3] Validation Loss: 1.559 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.73\n",
      "Epoch[3] Validation Loss: 1.559 \n",
      "Epoch[3] Iteration[3000] Training Loss: 0.66\n",
      "Epoch[3] Validation Loss: 1.555 \n",
      "Epoch[3] Iteration[4000] Training Loss: 0.74\n",
      "Epoch[3] Validation Loss: 1.549 \n",
      "Save best theta...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4] Iteration[0] Training Loss: 0.52\n",
      "Epoch[4] Validation Loss: 1.549 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 0.52\n",
      "Epoch[4] Validation Loss: 1.555 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.62\n",
      "Epoch[4] Validation Loss: 1.556 \n",
      "Epoch[4] Iteration[3000] Training Loss: 0.65\n",
      "Epoch[4] Validation Loss: 1.553 \n",
      "Epoch[4] Iteration[4000] Training Loss: 0.76\n",
      "Epoch[4] Validation Loss: 1.548 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[0] Training Loss: 0.47\n",
      "Epoch[5] Validation Loss: 1.547 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[1000] Training Loss: 0.62\n",
      "Epoch[5] Validation Loss: 1.552 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.51\n",
      "Epoch[5] Validation Loss: 1.555 \n",
      "Epoch[5] Iteration[3000] Training Loss: 0.88\n",
      "Epoch[5] Validation Loss: 1.553 \n",
      "Epoch[5] Iteration[4000] Training Loss: 0.88\n",
      "Epoch[5] Validation Loss: 1.550 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.27\n",
      "Epoch[6] Validation Loss: 1.547 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[1000] Training Loss: 0.39\n",
      "Epoch[6] Validation Loss: 1.554 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.45\n",
      "Epoch[6] Validation Loss: 1.553 \n",
      "Epoch[6] Iteration[3000] Training Loss: 0.69\n",
      "Epoch[6] Validation Loss: 1.554 \n",
      "Epoch[6] Iteration[4000] Training Loss: 0.74\n",
      "Epoch[6] Validation Loss: 1.548 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.64\n",
      "Epoch[7] Validation Loss: 1.548 \n",
      "Epoch[7] Iteration[1000] Training Loss: 0.46\n",
      "Epoch[7] Validation Loss: 1.554 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.63\n",
      "Epoch[7] Validation Loss: 1.554 \n",
      "Epoch[7] Iteration[3000] Training Loss: 0.70\n",
      "Epoch[7] Validation Loss: 1.552 \n",
      "Epoch[7] Iteration[4000] Training Loss: 0.66\n",
      "Epoch[7] Validation Loss: 1.548 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.49\n",
      "Epoch[8] Validation Loss: 1.547 \n",
      "Save best theta...\n",
      "Epoch[8] Iteration[1000] Training Loss: 0.65\n",
      "Epoch[8] Validation Loss: 1.553 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.59\n",
      "Epoch[8] Validation Loss: 1.554 \n",
      "Epoch[8] Iteration[3000] Training Loss: 0.60\n",
      "Epoch[8] Validation Loss: 1.552 \n",
      "Epoch[8] Iteration[4000] Training Loss: 1.34\n",
      "Epoch[8] Validation Loss: 1.548 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.45\n",
      "Epoch[9] Validation Loss: 1.547 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.34\n",
      "Epoch[9] Validation Loss: 1.554 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.71\n",
      "Epoch[9] Validation Loss: 1.555 \n",
      "Epoch[9] Iteration[3000] Training Loss: 0.72\n",
      "Epoch[9] Validation Loss: 1.553 \n",
      "Epoch[9] Iteration[4000] Training Loss: 0.59\n",
      "Epoch[9] Validation Loss: 1.549 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.74\n",
      "Epoch[10] Validation Loss: 1.548 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.47\n",
      "Epoch[10] Validation Loss: 1.556 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.54\n",
      "Epoch[10] Validation Loss: 1.555 \n",
      "Epoch[10] Iteration[3000] Training Loss: 0.89\n",
      "Epoch[10] Validation Loss: 1.554 \n",
      "Epoch[10] Iteration[4000] Training Loss: 1.01\n",
      "Epoch[10] Validation Loss: 1.550 \n",
      "10601852 4240742 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 13.40\n",
      "Epoch[0] Validation Loss: 13.673 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.36\n",
      "Epoch[0] Validation Loss: 1.663 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.18\n",
      "Epoch[0] Validation Loss: 1.572 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.20\n",
      "Epoch[0] Validation Loss: 1.525 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[4000] Training Loss: 0.90\n",
      "Epoch[0] Validation Loss: 1.489 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.88\n",
      "Epoch[1] Validation Loss: 1.483 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 0.75\n",
      "Epoch[1] Validation Loss: 1.463 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.69\n",
      "Epoch[1] Validation Loss: 1.446 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[3000] Training Loss: 1.13\n",
      "Epoch[1] Validation Loss: 1.433 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[4000] Training Loss: 0.90\n",
      "Epoch[1] Validation Loss: 1.418 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.48\n",
      "Epoch[2] Validation Loss: 1.417 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.68\n",
      "Epoch[2] Validation Loss: 1.416 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[2000] Training Loss: 0.75\n",
      "Epoch[2] Validation Loss: 1.414 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[3000] Training Loss: 0.69\n",
      "Epoch[2] Validation Loss: 1.408 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[4000] Training Loss: 0.72\n",
      "Epoch[2] Validation Loss: 1.404 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.50\n",
      "Epoch[3] Validation Loss: 1.401 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.57\n",
      "Epoch[3] Validation Loss: 1.404 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.69\n",
      "Epoch[3] Validation Loss: 1.404 \n",
      "Epoch[3] Iteration[3000] Training Loss: 0.74\n",
      "Epoch[3] Validation Loss: 1.404 \n",
      "Epoch[3] Iteration[4000] Training Loss: 0.66\n",
      "Epoch[3] Validation Loss: 1.398 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.46\n",
      "Epoch[4] Validation Loss: 1.398 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 0.51\n",
      "Epoch[4] Validation Loss: 1.401 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.43\n",
      "Epoch[4] Validation Loss: 1.403 \n",
      "Epoch[4] Iteration[3000] Training Loss: 0.49\n",
      "Epoch[4] Validation Loss: 1.402 \n",
      "Epoch[4] Iteration[4000] Training Loss: 0.66\n",
      "Epoch[4] Validation Loss: 1.401 \n",
      "Epoch[5] Iteration[0] Training Loss: 0.46\n",
      "Epoch[5] Validation Loss: 1.398 \n",
      "Epoch[5] Iteration[1000] Training Loss: 0.44\n",
      "Epoch[5] Validation Loss: 1.400 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.62\n",
      "Epoch[5] Validation Loss: 1.403 \n",
      "Epoch[5] Iteration[3000] Training Loss: 0.61\n",
      "Epoch[5] Validation Loss: 1.402 \n",
      "Epoch[5] Iteration[4000] Training Loss: 0.69\n",
      "Epoch[5] Validation Loss: 1.397 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[0] Training Loss: 0.47\n",
      "Epoch[6] Validation Loss: 1.398 \n",
      "Epoch[6] Iteration[1000] Training Loss: 0.43\n",
      "Epoch[6] Validation Loss: 1.400 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.43\n",
      "Epoch[6] Validation Loss: 1.402 \n",
      "Epoch[6] Iteration[3000] Training Loss: 0.51\n",
      "Epoch[6] Validation Loss: 1.402 \n",
      "Epoch[6] Iteration[4000] Training Loss: 0.75\n",
      "Epoch[6] Validation Loss: 1.399 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.32\n",
      "Epoch[7] Validation Loss: 1.397 \n",
      "Epoch[7] Iteration[1000] Training Loss: 0.45\n",
      "Epoch[7] Validation Loss: 1.401 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.54\n",
      "Epoch[7] Validation Loss: 1.403 \n",
      "Epoch[7] Iteration[3000] Training Loss: 0.65\n",
      "Epoch[7] Validation Loss: 1.402 \n",
      "Epoch[7] Iteration[4000] Training Loss: 0.69\n",
      "Epoch[7] Validation Loss: 1.397 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.37\n",
      "Epoch[8] Validation Loss: 1.397 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.46\n",
      "Epoch[8] Validation Loss: 1.401 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.63\n",
      "Epoch[8] Validation Loss: 1.403 \n",
      "Epoch[8] Iteration[3000] Training Loss: 0.74\n",
      "Epoch[8] Validation Loss: 1.401 \n",
      "Epoch[8] Iteration[4000] Training Loss: 0.68\n",
      "Epoch[8] Validation Loss: 1.400 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.37\n",
      "Epoch[9] Validation Loss: 1.397 \n",
      "Save best theta...\n",
      "Epoch[9] Iteration[1000] Training Loss: 0.54\n",
      "Epoch[9] Validation Loss: 1.401 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.50\n",
      "Epoch[9] Validation Loss: 1.406 \n",
      "Epoch[9] Iteration[3000] Training Loss: 0.54\n",
      "Epoch[9] Validation Loss: 1.404 \n",
      "Epoch[9] Iteration[4000] Training Loss: 0.88\n",
      "Epoch[9] Validation Loss: 1.398 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.42\n",
      "Epoch[10] Validation Loss: 1.399 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.44\n",
      "Epoch[10] Validation Loss: 1.402 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.68\n",
      "Epoch[10] Validation Loss: 1.404 \n",
      "Epoch[10] Iteration[3000] Training Loss: 0.57\n",
      "Epoch[10] Validation Loss: 1.404 \n",
      "Epoch[10] Iteration[4000] Training Loss: 0.83\n",
      "Epoch[10] Validation Loss: 1.398 \n",
      "10601852 4240742 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 13.08\n",
      "Epoch[0] Validation Loss: 13.194 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.37\n",
      "Epoch[0] Validation Loss: 1.246 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.15\n",
      "Epoch[0] Validation Loss: 1.157 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.04\n",
      "Epoch[0] Validation Loss: 1.110 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[4000] Training Loss: 1.65\n",
      "Epoch[0] Validation Loss: 1.072 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.70\n",
      "Epoch[1] Validation Loss: 1.068 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 0.91\n",
      "Epoch[1] Validation Loss: 1.049 \n",
      "Save best theta...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Iteration[2000] Training Loss: 1.30\n",
      "Epoch[1] Validation Loss: 1.034 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[3000] Training Loss: 0.73\n",
      "Epoch[1] Validation Loss: 1.021 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[4000] Training Loss: 0.83\n",
      "Epoch[1] Validation Loss: 1.010 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 1.15\n",
      "Epoch[2] Validation Loss: 1.008 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.60\n",
      "Epoch[2] Validation Loss: 1.008 \n",
      "Epoch[2] Iteration[2000] Training Loss: 0.60\n",
      "Epoch[2] Validation Loss: 1.007 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[3000] Training Loss: 0.74\n",
      "Epoch[2] Validation Loss: 1.003 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[4000] Training Loss: 1.14\n",
      "Epoch[2] Validation Loss: 0.997 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 1.02\n",
      "Epoch[3] Validation Loss: 0.996 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.49\n",
      "Epoch[3] Validation Loss: 1.000 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.60\n",
      "Epoch[3] Validation Loss: 1.001 \n",
      "Epoch[3] Iteration[3000] Training Loss: 0.67\n",
      "Epoch[3] Validation Loss: 0.999 \n",
      "Epoch[3] Iteration[4000] Training Loss: 0.70\n",
      "Epoch[3] Validation Loss: 0.995 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.43\n",
      "Epoch[4] Validation Loss: 0.994 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 0.46\n",
      "Epoch[4] Validation Loss: 0.999 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.63\n",
      "Epoch[4] Validation Loss: 1.002 \n",
      "Epoch[4] Iteration[3000] Training Loss: 0.77\n",
      "Epoch[4] Validation Loss: 1.000 \n",
      "Epoch[4] Iteration[4000] Training Loss: 0.80\n",
      "Epoch[4] Validation Loss: 0.995 \n",
      "Epoch[5] Iteration[0] Training Loss: 0.50\n",
      "Epoch[5] Validation Loss: 0.994 \n",
      "Epoch[5] Iteration[1000] Training Loss: 0.41\n",
      "Epoch[5] Validation Loss: 1.000 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.59\n",
      "Epoch[5] Validation Loss: 1.001 \n",
      "Epoch[5] Iteration[3000] Training Loss: 0.90\n",
      "Epoch[5] Validation Loss: 0.999 \n",
      "Epoch[5] Iteration[4000] Training Loss: 0.69\n",
      "Epoch[5] Validation Loss: 0.996 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.44\n",
      "Epoch[6] Validation Loss: 0.995 \n",
      "Epoch[6] Iteration[1000] Training Loss: 0.42\n",
      "Epoch[6] Validation Loss: 1.000 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.47\n",
      "Epoch[6] Validation Loss: 1.003 \n",
      "Epoch[6] Iteration[3000] Training Loss: 0.74\n",
      "Epoch[6] Validation Loss: 1.002 \n",
      "Epoch[6] Iteration[4000] Training Loss: 0.73\n",
      "Epoch[6] Validation Loss: 0.997 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.64\n",
      "Epoch[7] Validation Loss: 0.997 \n",
      "Epoch[7] Iteration[1000] Training Loss: 0.44\n",
      "Epoch[7] Validation Loss: 1.001 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.58\n",
      "Epoch[7] Validation Loss: 1.003 \n",
      "Epoch[7] Iteration[3000] Training Loss: 0.66\n",
      "Epoch[7] Validation Loss: 1.003 \n",
      "Epoch[7] Iteration[4000] Training Loss: 0.89\n",
      "Epoch[7] Validation Loss: 0.999 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.68\n",
      "Epoch[8] Validation Loss: 0.998 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.45\n",
      "Epoch[8] Validation Loss: 1.002 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.80\n",
      "Epoch[8] Validation Loss: 1.004 \n",
      "Epoch[8] Iteration[3000] Training Loss: 0.91\n",
      "Epoch[8] Validation Loss: 1.004 \n",
      "Epoch[8] Iteration[4000] Training Loss: 0.90\n",
      "Epoch[8] Validation Loss: 1.000 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.35\n",
      "Epoch[9] Validation Loss: 0.998 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.48\n",
      "Epoch[9] Validation Loss: 1.003 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.67\n",
      "Epoch[9] Validation Loss: 1.004 \n",
      "Epoch[9] Iteration[3000] Training Loss: 0.62\n",
      "Epoch[9] Validation Loss: 1.003 \n",
      "Epoch[9] Iteration[4000] Training Loss: 0.79\n",
      "Epoch[9] Validation Loss: 0.999 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.31\n",
      "Epoch[10] Validation Loss: 0.999 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.50\n",
      "Epoch[10] Validation Loss: 1.003 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.99\n",
      "Epoch[10] Validation Loss: 1.004 \n",
      "Epoch[10] Iteration[3000] Training Loss: 0.40\n",
      "Epoch[10] Validation Loss: 1.003 \n",
      "Epoch[10] Iteration[4000] Training Loss: 1.16\n",
      "Epoch[10] Validation Loss: 1.001 \n",
      "10601852 4240741 1060186 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 15.21\n",
      "Epoch[0] Validation Loss: 13.308 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.75\n",
      "Epoch[0] Validation Loss: 1.269 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.02\n",
      "Epoch[0] Validation Loss: 1.177 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.18\n",
      "Epoch[0] Validation Loss: 1.132 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[4000] Training Loss: 1.02\n",
      "Epoch[0] Validation Loss: 1.098 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.66\n",
      "Epoch[1] Validation Loss: 1.091 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 1.09\n",
      "Epoch[1] Validation Loss: 1.075 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.68\n",
      "Epoch[1] Validation Loss: 1.061 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[3000] Training Loss: 0.74\n",
      "Epoch[1] Validation Loss: 1.048 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[4000] Training Loss: 0.96\n",
      "Epoch[1] Validation Loss: 1.038 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.52\n",
      "Epoch[2] Validation Loss: 1.035 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.57\n",
      "Epoch[2] Validation Loss: 1.038 \n",
      "Epoch[2] Iteration[2000] Training Loss: 1.22\n",
      "Epoch[2] Validation Loss: 1.037 \n",
      "Epoch[2] Iteration[3000] Training Loss: 0.74\n",
      "Epoch[2] Validation Loss: 1.031 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[4000] Training Loss: 1.27\n",
      "Epoch[2] Validation Loss: 1.024 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.66\n",
      "Epoch[3] Validation Loss: 1.024 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.95\n",
      "Epoch[3] Validation Loss: 1.029 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.83\n",
      "Epoch[3] Validation Loss: 1.034 \n",
      "Epoch[3] Iteration[3000] Training Loss: 0.57\n",
      "Epoch[3] Validation Loss: 1.027 \n",
      "Epoch[3] Iteration[4000] Training Loss: 0.76\n",
      "Epoch[3] Validation Loss: 1.023 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 1.08\n",
      "Epoch[4] Validation Loss: 1.022 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 1.30\n",
      "Epoch[4] Validation Loss: 1.031 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.98\n",
      "Epoch[4] Validation Loss: 1.031 \n",
      "Epoch[4] Iteration[3000] Training Loss: 0.59\n",
      "Epoch[4] Validation Loss: 1.029 \n",
      "Epoch[4] Iteration[4000] Training Loss: 0.86\n",
      "Epoch[4] Validation Loss: 1.024 \n",
      "Epoch[5] Iteration[0] Training Loss: 0.90\n",
      "Epoch[5] Validation Loss: 1.024 \n",
      "Epoch[5] Iteration[1000] Training Loss: 0.52\n",
      "Epoch[5] Validation Loss: 1.030 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.78\n",
      "Epoch[5] Validation Loss: 1.031 \n",
      "Epoch[5] Iteration[3000] Training Loss: 0.88\n",
      "Epoch[5] Validation Loss: 1.030 \n",
      "Epoch[5] Iteration[4000] Training Loss: 0.87\n",
      "Epoch[5] Validation Loss: 1.026 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.41\n",
      "Epoch[6] Validation Loss: 1.024 \n",
      "Epoch[6] Iteration[1000] Training Loss: 0.74\n",
      "Epoch[6] Validation Loss: 1.031 \n",
      "Epoch[6] Iteration[2000] Training Loss: 1.12\n",
      "Epoch[6] Validation Loss: 1.032 \n",
      "Epoch[6] Iteration[3000] Training Loss: 0.78\n",
      "Epoch[6] Validation Loss: 1.031 \n",
      "Epoch[6] Iteration[4000] Training Loss: 0.97\n",
      "Epoch[6] Validation Loss: 1.026 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.49\n",
      "Epoch[7] Validation Loss: 1.025 \n",
      "Epoch[7] Iteration[1000] Training Loss: 0.83\n",
      "Epoch[7] Validation Loss: 1.033 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.63\n",
      "Epoch[7] Validation Loss: 1.033 \n",
      "Epoch[7] Iteration[3000] Training Loss: 1.09\n",
      "Epoch[7] Validation Loss: 1.032 \n",
      "Epoch[7] Iteration[4000] Training Loss: 1.09\n",
      "Epoch[7] Validation Loss: 1.027 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.49\n",
      "Epoch[8] Validation Loss: 1.026 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.77\n",
      "Epoch[8] Validation Loss: 1.031 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.71\n",
      "Epoch[8] Validation Loss: 1.034 \n",
      "Epoch[8] Iteration[3000] Training Loss: 0.76\n",
      "Epoch[8] Validation Loss: 1.032 \n",
      "Epoch[8] Iteration[4000] Training Loss: 1.00\n",
      "Epoch[8] Validation Loss: 1.028 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.47\n",
      "Epoch[9] Validation Loss: 1.027 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.45\n",
      "Epoch[9] Validation Loss: 1.032 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.65\n",
      "Epoch[9] Validation Loss: 1.034 \n",
      "Epoch[9] Iteration[3000] Training Loss: 0.60\n",
      "Epoch[9] Validation Loss: 1.033 \n",
      "Epoch[9] Iteration[4000] Training Loss: 0.94\n",
      "Epoch[9] Validation Loss: 1.028 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.60\n",
      "Epoch[10] Validation Loss: 1.027 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[10] Iteration[1000] Training Loss: 0.58\n",
      "Epoch[10] Validation Loss: 1.033 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.52\n",
      "Epoch[10] Validation Loss: 1.035 \n",
      "Epoch[10] Iteration[3000] Training Loss: 0.89\n",
      "Epoch[10] Validation Loss: 1.033 \n",
      "Epoch[10] Iteration[4000] Training Loss: 0.96\n",
      "Epoch[10] Validation Loss: 1.029 \n",
      "10601852 5300927 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 13.44\n",
      "Epoch[0] Validation Loss: 14.184 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.16\n",
      "Epoch[0] Validation Loss: 1.924 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.50\n",
      "Epoch[0] Validation Loss: 1.804 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.12\n",
      "Epoch[0] Validation Loss: 1.752 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[4000] Training Loss: 1.08\n",
      "Epoch[0] Validation Loss: 1.711 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[5000] Training Loss: 1.29\n",
      "Epoch[0] Validation Loss: 1.679 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.67\n",
      "Epoch[1] Validation Loss: 1.675 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 0.63\n",
      "Epoch[1] Validation Loss: 1.664 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 1.24\n",
      "Epoch[1] Validation Loss: 1.646 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[3000] Training Loss: 1.26\n",
      "Epoch[1] Validation Loss: 1.633 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[4000] Training Loss: 0.91\n",
      "Epoch[1] Validation Loss: 1.621 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[5000] Training Loss: 0.82\n",
      "Epoch[1] Validation Loss: 1.611 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 1.14\n",
      "Epoch[2] Validation Loss: 1.610 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.49\n",
      "Epoch[2] Validation Loss: 1.614 \n",
      "Epoch[2] Iteration[2000] Training Loss: 0.87\n",
      "Epoch[2] Validation Loss: 1.610 \n",
      "Epoch[2] Iteration[3000] Training Loss: 1.30\n",
      "Epoch[2] Validation Loss: 1.607 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[4000] Training Loss: 0.73\n",
      "Epoch[2] Validation Loss: 1.603 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[5000] Training Loss: 0.67\n",
      "Epoch[2] Validation Loss: 1.598 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.54\n",
      "Epoch[3] Validation Loss: 1.596 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.63\n",
      "Epoch[3] Validation Loss: 1.602 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.70\n",
      "Epoch[3] Validation Loss: 1.605 \n",
      "Epoch[3] Iteration[3000] Training Loss: 0.76\n",
      "Epoch[3] Validation Loss: 1.600 \n",
      "Epoch[3] Iteration[4000] Training Loss: 0.84\n",
      "Epoch[3] Validation Loss: 1.596 \n",
      "Epoch[3] Iteration[5000] Training Loss: 1.34\n",
      "Epoch[3] Validation Loss: 1.592 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.52\n",
      "Epoch[4] Validation Loss: 1.592 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 0.71\n",
      "Epoch[4] Validation Loss: 1.597 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.68\n",
      "Epoch[4] Validation Loss: 1.600 \n",
      "Epoch[4] Iteration[3000] Training Loss: 0.66\n",
      "Epoch[4] Validation Loss: 1.600 \n",
      "Epoch[4] Iteration[4000] Training Loss: 0.70\n",
      "Epoch[4] Validation Loss: 1.595 \n",
      "Epoch[4] Iteration[5000] Training Loss: 1.22\n",
      "Epoch[4] Validation Loss: 1.592 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[0] Training Loss: 0.77\n",
      "Epoch[5] Validation Loss: 1.592 \n",
      "Epoch[5] Iteration[1000] Training Loss: 0.75\n",
      "Epoch[5] Validation Loss: 1.598 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.77\n",
      "Epoch[5] Validation Loss: 1.600 \n",
      "Epoch[5] Iteration[3000] Training Loss: 0.64\n",
      "Epoch[5] Validation Loss: 1.598 \n",
      "Epoch[5] Iteration[4000] Training Loss: 0.68\n",
      "Epoch[5] Validation Loss: 1.595 \n",
      "Epoch[5] Iteration[5000] Training Loss: 0.87\n",
      "Epoch[5] Validation Loss: 1.592 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.35\n",
      "Epoch[6] Validation Loss: 1.591 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[1000] Training Loss: 0.54\n",
      "Epoch[6] Validation Loss: 1.600 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.88\n",
      "Epoch[6] Validation Loss: 1.600 \n",
      "Epoch[6] Iteration[3000] Training Loss: 0.77\n",
      "Epoch[6] Validation Loss: 1.600 \n",
      "Epoch[6] Iteration[4000] Training Loss: 0.75\n",
      "Epoch[6] Validation Loss: 1.595 \n",
      "Epoch[6] Iteration[5000] Training Loss: 0.96\n",
      "Epoch[6] Validation Loss: 1.591 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.40\n",
      "Epoch[7] Validation Loss: 1.591 \n",
      "Save best theta...\n",
      "Epoch[7] Iteration[1000] Training Loss: 1.04\n",
      "Epoch[7] Validation Loss: 1.598 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.59\n",
      "Epoch[7] Validation Loss: 1.601 \n",
      "Epoch[7] Iteration[3000] Training Loss: 1.11\n",
      "Epoch[7] Validation Loss: 1.599 \n",
      "Epoch[7] Iteration[4000] Training Loss: 0.77\n",
      "Epoch[7] Validation Loss: 1.596 \n",
      "Epoch[7] Iteration[5000] Training Loss: 0.82\n",
      "Epoch[7] Validation Loss: 1.592 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.48\n",
      "Epoch[8] Validation Loss: 1.591 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.91\n",
      "Epoch[8] Validation Loss: 1.598 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.53\n",
      "Epoch[8] Validation Loss: 1.602 \n",
      "Epoch[8] Iteration[3000] Training Loss: 0.62\n",
      "Epoch[8] Validation Loss: 1.602 \n",
      "Epoch[8] Iteration[4000] Training Loss: 0.82\n",
      "Epoch[8] Validation Loss: 1.597 \n",
      "Epoch[8] Iteration[5000] Training Loss: 1.14\n",
      "Epoch[8] Validation Loss: 1.592 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.56\n",
      "Epoch[9] Validation Loss: 1.593 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.80\n",
      "Epoch[9] Validation Loss: 1.598 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.78\n",
      "Epoch[9] Validation Loss: 1.601 \n",
      "Epoch[9] Iteration[3000] Training Loss: 0.99\n",
      "Epoch[9] Validation Loss: 1.602 \n",
      "Epoch[9] Iteration[4000] Training Loss: 1.56\n",
      "Epoch[9] Validation Loss: 1.597 \n",
      "Epoch[9] Iteration[5000] Training Loss: 0.79\n",
      "Epoch[9] Validation Loss: 1.592 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.35\n",
      "Epoch[10] Validation Loss: 1.590 \n",
      "Save best theta...\n",
      "Epoch[10] Iteration[1000] Training Loss: 1.00\n",
      "Epoch[10] Validation Loss: 1.597 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.56\n",
      "Epoch[10] Validation Loss: 1.601 \n",
      "Epoch[10] Iteration[3000] Training Loss: 0.93\n",
      "Epoch[10] Validation Loss: 1.598 \n",
      "Epoch[10] Iteration[4000] Training Loss: 0.56\n",
      "Epoch[10] Validation Loss: 1.597 \n",
      "Epoch[10] Iteration[5000] Training Loss: 0.69\n",
      "Epoch[10] Validation Loss: 1.592 \n",
      "10601852 5300927 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 12.39\n",
      "Epoch[0] Validation Loss: 14.009 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.16\n",
      "Epoch[0] Validation Loss: 1.845 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.69\n",
      "Epoch[0] Validation Loss: 1.742 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.11\n",
      "Epoch[0] Validation Loss: 1.692 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[4000] Training Loss: 1.54\n",
      "Epoch[0] Validation Loss: 1.653 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[5000] Training Loss: 0.98\n",
      "Epoch[0] Validation Loss: 1.621 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.68\n",
      "Epoch[1] Validation Loss: 1.617 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 0.89\n",
      "Epoch[1] Validation Loss: 1.603 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.65\n",
      "Epoch[1] Validation Loss: 1.591 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[3000] Training Loss: 0.73\n",
      "Epoch[1] Validation Loss: 1.578 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[4000] Training Loss: 0.92\n",
      "Epoch[1] Validation Loss: 1.567 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[5000] Training Loss: 0.85\n",
      "Epoch[1] Validation Loss: 1.559 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.89\n",
      "Epoch[2] Validation Loss: 1.557 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.57\n",
      "Epoch[2] Validation Loss: 1.559 \n",
      "Epoch[2] Iteration[2000] Training Loss: 0.79\n",
      "Epoch[2] Validation Loss: 1.559 \n",
      "Epoch[2] Iteration[3000] Training Loss: 0.72\n",
      "Epoch[2] Validation Loss: 1.556 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[4000] Training Loss: 0.83\n",
      "Epoch[2] Validation Loss: 1.551 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[5000] Training Loss: 0.83\n",
      "Epoch[2] Validation Loss: 1.546 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 0.62\n",
      "Epoch[3] Validation Loss: 1.545 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.54\n",
      "Epoch[3] Validation Loss: 1.550 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.64\n",
      "Epoch[3] Validation Loss: 1.550 \n",
      "Epoch[3] Iteration[3000] Training Loss: 0.88\n",
      "Epoch[3] Validation Loss: 1.549 \n",
      "Epoch[3] Iteration[4000] Training Loss: 0.75\n",
      "Epoch[3] Validation Loss: 1.545 \n",
      "Epoch[3] Iteration[5000] Training Loss: 0.85\n",
      "Epoch[3] Validation Loss: 1.541 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.61\n",
      "Epoch[4] Validation Loss: 1.541 \n",
      "Save best theta...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4] Iteration[1000] Training Loss: 0.46\n",
      "Epoch[4] Validation Loss: 1.546 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.75\n",
      "Epoch[4] Validation Loss: 1.549 \n",
      "Epoch[4] Iteration[3000] Training Loss: 0.84\n",
      "Epoch[4] Validation Loss: 1.548 \n",
      "Epoch[4] Iteration[4000] Training Loss: 0.75\n",
      "Epoch[4] Validation Loss: 1.545 \n",
      "Epoch[4] Iteration[5000] Training Loss: 0.75\n",
      "Epoch[4] Validation Loss: 1.541 \n",
      "Epoch[5] Iteration[0] Training Loss: 0.35\n",
      "Epoch[5] Validation Loss: 1.540 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[1000] Training Loss: 0.57\n",
      "Epoch[5] Validation Loss: 1.545 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.67\n",
      "Epoch[5] Validation Loss: 1.548 \n",
      "Epoch[5] Iteration[3000] Training Loss: 0.79\n",
      "Epoch[5] Validation Loss: 1.547 \n",
      "Epoch[5] Iteration[4000] Training Loss: 1.03\n",
      "Epoch[5] Validation Loss: 1.545 \n",
      "Epoch[5] Iteration[5000] Training Loss: 0.82\n",
      "Epoch[5] Validation Loss: 1.540 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.41\n",
      "Epoch[6] Validation Loss: 1.540 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[1000] Training Loss: 0.95\n",
      "Epoch[6] Validation Loss: 1.546 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.78\n",
      "Epoch[6] Validation Loss: 1.547 \n",
      "Epoch[6] Iteration[3000] Training Loss: 0.55\n",
      "Epoch[6] Validation Loss: 1.547 \n",
      "Epoch[6] Iteration[4000] Training Loss: 1.04\n",
      "Epoch[6] Validation Loss: 1.544 \n",
      "Epoch[6] Iteration[5000] Training Loss: 1.42\n",
      "Epoch[6] Validation Loss: 1.540 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.81\n",
      "Epoch[7] Validation Loss: 1.540 \n",
      "Epoch[7] Iteration[1000] Training Loss: 0.52\n",
      "Epoch[7] Validation Loss: 1.545 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.69\n",
      "Epoch[7] Validation Loss: 1.548 \n",
      "Epoch[7] Iteration[3000] Training Loss: 0.68\n",
      "Epoch[7] Validation Loss: 1.547 \n",
      "Epoch[7] Iteration[4000] Training Loss: 0.68\n",
      "Epoch[7] Validation Loss: 1.544 \n",
      "Epoch[7] Iteration[5000] Training Loss: 0.73\n",
      "Epoch[7] Validation Loss: 1.540 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.61\n",
      "Epoch[8] Validation Loss: 1.540 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.68\n",
      "Epoch[8] Validation Loss: 1.545 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.65\n",
      "Epoch[8] Validation Loss: 1.547 \n",
      "Epoch[8] Iteration[3000] Training Loss: 0.70\n",
      "Epoch[8] Validation Loss: 1.547 \n",
      "Epoch[8] Iteration[4000] Training Loss: 1.23\n",
      "Epoch[8] Validation Loss: 1.545 \n",
      "Epoch[8] Iteration[5000] Training Loss: 1.04\n",
      "Epoch[8] Validation Loss: 1.541 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.57\n",
      "Epoch[9] Validation Loss: 1.540 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.45\n",
      "Epoch[9] Validation Loss: 1.545 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.52\n",
      "Epoch[9] Validation Loss: 1.548 \n",
      "Epoch[9] Iteration[3000] Training Loss: 1.27\n",
      "Epoch[9] Validation Loss: 1.548 \n",
      "Epoch[9] Iteration[4000] Training Loss: 0.78\n",
      "Epoch[9] Validation Loss: 1.546 \n",
      "Epoch[9] Iteration[5000] Training Loss: 0.84\n",
      "Epoch[9] Validation Loss: 1.541 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.45\n",
      "Epoch[10] Validation Loss: 1.541 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.45\n",
      "Epoch[10] Validation Loss: 1.546 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.72\n",
      "Epoch[10] Validation Loss: 1.548 \n",
      "Epoch[10] Iteration[3000] Training Loss: 0.91\n",
      "Epoch[10] Validation Loss: 1.548 \n",
      "Epoch[10] Iteration[4000] Training Loss: 0.78\n",
      "Epoch[10] Validation Loss: 1.546 \n",
      "Epoch[10] Iteration[5000] Training Loss: 0.71\n",
      "Epoch[10] Validation Loss: 1.542 \n",
      "10601852 5300927 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 14.78\n",
      "Epoch[0] Validation Loss: 13.656 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.76\n",
      "Epoch[0] Validation Loss: 1.664 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.18\n",
      "Epoch[0] Validation Loss: 1.575 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.09\n",
      "Epoch[0] Validation Loss: 1.529 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[4000] Training Loss: 0.97\n",
      "Epoch[0] Validation Loss: 1.492 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[5000] Training Loss: 1.11\n",
      "Epoch[0] Validation Loss: 1.463 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.54\n",
      "Epoch[1] Validation Loss: 1.459 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 1.25\n",
      "Epoch[1] Validation Loss: 1.445 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.82\n",
      "Epoch[1] Validation Loss: 1.434 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[3000] Training Loss: 0.90\n",
      "Epoch[1] Validation Loss: 1.424 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[4000] Training Loss: 0.94\n",
      "Epoch[1] Validation Loss: 1.415 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[5000] Training Loss: 1.73\n",
      "Epoch[1] Validation Loss: 1.406 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 1.51\n",
      "Epoch[2] Validation Loss: 1.403 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 1.19\n",
      "Epoch[2] Validation Loss: 1.406 \n",
      "Epoch[2] Iteration[2000] Training Loss: 0.69\n",
      "Epoch[2] Validation Loss: 1.405 \n",
      "Epoch[2] Iteration[3000] Training Loss: 0.63\n",
      "Epoch[2] Validation Loss: 1.403 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[4000] Training Loss: 0.77\n",
      "Epoch[2] Validation Loss: 1.399 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[5000] Training Loss: 1.42\n",
      "Epoch[2] Validation Loss: 1.393 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[0] Training Loss: 1.14\n",
      "Epoch[3] Validation Loss: 1.392 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 0.73\n",
      "Epoch[3] Validation Loss: 1.397 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.53\n",
      "Epoch[3] Validation Loss: 1.400 \n",
      "Epoch[3] Iteration[3000] Training Loss: 0.69\n",
      "Epoch[3] Validation Loss: 1.398 \n",
      "Epoch[3] Iteration[4000] Training Loss: 0.88\n",
      "Epoch[3] Validation Loss: 1.394 \n",
      "Epoch[3] Iteration[5000] Training Loss: 0.78\n",
      "Epoch[3] Validation Loss: 1.391 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.63\n",
      "Epoch[4] Validation Loss: 1.390 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[1000] Training Loss: 0.67\n",
      "Epoch[4] Validation Loss: 1.394 \n",
      "Epoch[4] Iteration[2000] Training Loss: 0.64\n",
      "Epoch[4] Validation Loss: 1.397 \n",
      "Epoch[4] Iteration[3000] Training Loss: 0.60\n",
      "Epoch[4] Validation Loss: 1.398 \n",
      "Epoch[4] Iteration[4000] Training Loss: 0.66\n",
      "Epoch[4] Validation Loss: 1.394 \n",
      "Epoch[4] Iteration[5000] Training Loss: 0.82\n",
      "Epoch[4] Validation Loss: 1.390 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[0] Training Loss: 0.48\n",
      "Epoch[5] Validation Loss: 1.390 \n",
      "Epoch[5] Iteration[1000] Training Loss: 0.85\n",
      "Epoch[5] Validation Loss: 1.394 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.80\n",
      "Epoch[5] Validation Loss: 1.397 \n",
      "Epoch[5] Iteration[3000] Training Loss: 0.86\n",
      "Epoch[5] Validation Loss: 1.397 \n",
      "Epoch[5] Iteration[4000] Training Loss: 1.29\n",
      "Epoch[5] Validation Loss: 1.394 \n",
      "Epoch[5] Iteration[5000] Training Loss: 0.82\n",
      "Epoch[5] Validation Loss: 1.391 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.55\n",
      "Epoch[6] Validation Loss: 1.390 \n",
      "Epoch[6] Iteration[1000] Training Loss: 0.54\n",
      "Epoch[6] Validation Loss: 1.395 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.65\n",
      "Epoch[6] Validation Loss: 1.398 \n",
      "Epoch[6] Iteration[3000] Training Loss: 0.72\n",
      "Epoch[6] Validation Loss: 1.397 \n",
      "Epoch[6] Iteration[4000] Training Loss: 1.15\n",
      "Epoch[6] Validation Loss: 1.394 \n",
      "Epoch[6] Iteration[5000] Training Loss: 0.80\n",
      "Epoch[6] Validation Loss: 1.390 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.37\n",
      "Epoch[7] Validation Loss: 1.390 \n",
      "Save best theta...\n",
      "Epoch[7] Iteration[1000] Training Loss: 0.61\n",
      "Epoch[7] Validation Loss: 1.395 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.78\n",
      "Epoch[7] Validation Loss: 1.397 \n",
      "Epoch[7] Iteration[3000] Training Loss: 0.67\n",
      "Epoch[7] Validation Loss: 1.397 \n",
      "Epoch[7] Iteration[4000] Training Loss: 0.88\n",
      "Epoch[7] Validation Loss: 1.395 \n",
      "Epoch[7] Iteration[5000] Training Loss: 1.06\n",
      "Epoch[7] Validation Loss: 1.392 \n",
      "Epoch[8] Iteration[0] Training Loss: 0.45\n",
      "Epoch[8] Validation Loss: 1.390 \n",
      "Save best theta...\n",
      "Epoch[8] Iteration[1000] Training Loss: 0.54\n",
      "Epoch[8] Validation Loss: 1.394 \n",
      "Epoch[8] Iteration[2000] Training Loss: 0.85\n",
      "Epoch[8] Validation Loss: 1.397 \n",
      "Epoch[8] Iteration[3000] Training Loss: 0.64\n",
      "Epoch[8] Validation Loss: 1.397 \n",
      "Epoch[8] Iteration[4000] Training Loss: 0.65\n",
      "Epoch[8] Validation Loss: 1.394 \n",
      "Epoch[8] Iteration[5000] Training Loss: 0.78\n",
      "Epoch[8] Validation Loss: 1.391 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.50\n",
      "Epoch[9] Validation Loss: 1.391 \n",
      "Epoch[9] Iteration[1000] Training Loss: 0.63\n",
      "Epoch[9] Validation Loss: 1.395 \n",
      "Epoch[9] Iteration[2000] Training Loss: 0.50\n",
      "Epoch[9] Validation Loss: 1.398 \n",
      "Epoch[9] Iteration[3000] Training Loss: 0.75\n",
      "Epoch[9] Validation Loss: 1.398 \n",
      "Epoch[9] Iteration[4000] Training Loss: 0.78\n",
      "Epoch[9] Validation Loss: 1.396 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9] Iteration[5000] Training Loss: 0.77\n",
      "Epoch[9] Validation Loss: 1.393 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.80\n",
      "Epoch[10] Validation Loss: 1.391 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.80\n",
      "Epoch[10] Validation Loss: 1.396 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.58\n",
      "Epoch[10] Validation Loss: 1.398 \n",
      "Epoch[10] Iteration[3000] Training Loss: 0.81\n",
      "Epoch[10] Validation Loss: 1.398 \n",
      "Epoch[10] Iteration[4000] Training Loss: 0.67\n",
      "Epoch[10] Validation Loss: 1.397 \n",
      "Epoch[10] Iteration[5000] Training Loss: 0.82\n",
      "Epoch[10] Validation Loss: 1.391 \n",
      "10601852 5300927 1060185 1060185\n",
      "Epoch[0] Iteration[0] Training Loss: 14.59\n",
      "Epoch[0] Validation Loss: 13.210 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.96\n",
      "Epoch[0] Validation Loss: 1.251 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.14\n",
      "Epoch[0] Validation Loss: 1.156 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.10\n",
      "Epoch[0] Validation Loss: 1.110 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[4000] Training Loss: 1.38\n",
      "Epoch[0] Validation Loss: 1.076 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[5000] Training Loss: 1.67\n",
      "Epoch[0] Validation Loss: 1.049 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.68\n",
      "Epoch[1] Validation Loss: 1.044 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 1.57\n",
      "Epoch[1] Validation Loss: 1.033 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[2000] Training Loss: 0.72\n",
      "Epoch[1] Validation Loss: 1.023 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[3000] Training Loss: 0.93\n",
      "Epoch[1] Validation Loss: 1.017 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[4000] Training Loss: 1.42\n",
      "Epoch[1] Validation Loss: 1.007 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[5000] Training Loss: 0.74\n",
      "Epoch[1] Validation Loss: 0.999 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.81\n",
      "Epoch[2] Validation Loss: 0.997 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 0.56\n",
      "Epoch[2] Validation Loss: 1.002 \n",
      "Epoch[2] Iteration[2000] Training Loss: 0.86\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 10.91 GiB total capacity; 9.19 GiB already allocated; 50.12 MiB free; 313.49 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7067b22675bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                     \u001b[0mval_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                     \u001b[0mval_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 10.91 GiB total capacity; 9.19 GiB already allocated; 50.12 MiB free; 313.49 MiB cached)"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 5e-3\n",
    "k = 1\n",
    "lamb = 5e-7\n",
    "batch_size = 1024\n",
    "\n",
    "# To keep track to best hyperparameters and results\n",
    "results = {}\n",
    "\n",
    "\"\"\"\n",
    "split_values = [\n",
    "    # 0.2 fixed window\n",
    "    [0.6,0.8], \n",
    "    [0.5,0.7], \n",
    "    [0.4,0.6], \n",
    "    [0.3,0.5], \n",
    "    [0.2,0.4], \n",
    "    [0.1,0.3], \n",
    "    [0,0.2],\n",
    "    # 0.3 fixed window\n",
    "    [0.5,0.8], \n",
    "    [0.4,0.7], \n",
    "    [0.3,0.6], \n",
    "    [0.2,0.5], \n",
    "    [0.1,0.4], \n",
    "    [0,0.3], \n",
    "    # 0.4 fixed window\n",
    "    [0.4,0.8], \n",
    "    [0.3,0.7], \n",
    "    [0.2,0.6], \n",
    "    [0.1,0.5], \n",
    "    [0,0.4], \n",
    "    # 0.5 fixed window\n",
    "    [0.3,0.8], \n",
    "    [0.2,0.7], \n",
    "    [0.1,0.6], \n",
    "    [0,0.5], \n",
    "    # 0.6 fixed window\n",
    "    [0.2,0.8], \n",
    "    [0.1,0.7], \n",
    "    [0,0.6]\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "split_values = [\n",
    "    # 0.2 fixed window\n",
    "    [0.6,0.85], \n",
    "    [0.5,0.7], \n",
    "    [0.4,0.6], \n",
    "    [0.3,0.5], \n",
    "    [0.2,0.4], \n",
    "    [0.1,0.3], \n",
    "    [0,0.2],\n",
    "    # 0.3 fixed window\n",
    "    [0.5,0.8], \n",
    "    [0.4,0.7], \n",
    "    [0.3,0.6], \n",
    "    [0.2,0.5], \n",
    "    [0.1,0.4], \n",
    "    [0,0.3], \n",
    "    # 0.4 fixed window\n",
    "    [0.4,0.8], \n",
    "    [0.3,0.7], \n",
    "    [0.2,0.6], \n",
    "    [0.1,0.5], \n",
    "    [0,0.4], \n",
    "    # 0.5 fixed window\n",
    "    [0.3,0.8], \n",
    "    [0.2,0.7], \n",
    "    [0.1,0.6], \n",
    "    [0,0.5], \n",
    "    # 0.6 fixed window\n",
    "    [0.2,0.8], \n",
    "    [0.1,0.7], \n",
    "    [0,0.6]\n",
    "]\n",
    "\n",
    "for values in split_values:\n",
    "    \n",
    "    # Implement a moving window - test and valid datasets are 10% each\n",
    "    start = values[0]\n",
    "    split = values[1]\n",
    "    \n",
    "    train_start = int(N * start)\n",
    "    train_split = int(N * split)   \n",
    "    valid_split =  int(N * (split+0.1))\n",
    "    test_split =  int(N * (split+0.2))\n",
    "\n",
    "    train_x = data.loc[train_start:train_split, ['gPlusPlaceId','gPlusUserId','num_reviews', \\\n",
    "                                      'unixReviewTime','year','month','day']]\n",
    "    train_y = data.loc[train_start:train_split, 'rating':'rating']\n",
    "    valid_x = data.loc[train_split+1:valid_split, ['gPlusPlaceId','gPlusUserId','num_reviews', \\\n",
    "                                      'unixReviewTime','year','month','day']]\n",
    "    valid_y = data.loc[train_split+1:valid_split, 'rating':'rating']\n",
    "    test_x = data.loc[valid_split+1:test_split, ['gPlusPlaceId','gPlusUserId','num_reviews', \\\n",
    "                                      'unixReviewTime','year','month','day']]\n",
    "    test_y = data.loc[valid_split+1:test_split, 'rating':'rating']\n",
    "\n",
    "    print(N, train_x.index.size, valid_x.index.size,test_x.index.size)\n",
    "\n",
    "    model = MF(n_user, n_place, k=k)\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lamb)\n",
    "\n",
    "    def chunks(X, Y, size):\n",
    "        \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "        starts = list(range(0, len(X), size))\n",
    "        shuffle(starts)\n",
    "        for i in starts:\n",
    "            yield (X[i:i + size], Y[i:i + size])\n",
    "\n",
    "    # To keep track to best hyperparameters and results\n",
    "    best_loss = 0\n",
    "\n",
    "    losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(10+1):\n",
    "\n",
    "        i = 0\n",
    "        for feature, target in chunks(np.array(train_x), np.array(train_y), batch_size):\n",
    "            # This zeros the gradients on every parameter. \n",
    "            # This is easy to miss and hard to troubleshoot.\n",
    "            optimizer.zero_grad()\n",
    "            # Convert \n",
    "            feature = Variable(torch.from_numpy(feature))\n",
    "            target = Variable(torch.from_numpy(target).type(torch.FloatTensor))\n",
    "\n",
    "            if cuda:\n",
    "                feature = feature.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "            # model in training mode    \n",
    "            model.train()\n",
    "\n",
    "            # Compute a prediction for these features\n",
    "            prediction = model.forward(feature)\n",
    "            # Compute a loss given what the true target outcome was\n",
    "            loss = model.loss(prediction, target)\n",
    "            # break\n",
    "            # Backpropagate: compute the direction / gradient every model parameter\n",
    "            # defined in your __init__ should move in in order to minimize this loss\n",
    "            # However, we're not actually changing these parameters, we're just storing\n",
    "            # how they should change.\n",
    "\n",
    "            loss.backward()\n",
    "            # Now take a step & update the model parameters. The optimizer uses the gradient at \n",
    "            # defined on every parameter in our model and nudges it in that direction.\n",
    "            optimizer.step()\n",
    "\n",
    "            if i%1000 == 0 and epoch%1 == 0:\n",
    "                print(\"Epoch[{}] Iteration[{}] Training Loss: {:.2f}\".format(epoch, i, loss.data))\n",
    "\n",
    "            # Record the loss per example\n",
    "            losses.append(loss.cpu().data.numpy() / len(feature))\n",
    "\n",
    "            if i%1000 == 0 and epoch%1 == 0:\n",
    "\n",
    "                val_feature = torch.from_numpy(np.array(valid_x))\n",
    "                val_target = torch.from_numpy(np.array(valid_y)).type(torch.FloatTensor)\n",
    "\n",
    "                if cuda:\n",
    "                    val_feature = val_feature.cuda()\n",
    "                    val_target = val_target.cuda()\n",
    "\n",
    "                # model in test mode    \n",
    "                model.eval()\n",
    "\n",
    "                val_pred = model.forward(val_feature)\n",
    "                vloss = model.loss(val_pred, val_target)\n",
    "                print(\"Epoch[{}] Validation Loss: {:.3f} \".format(epoch, vloss.data))\n",
    "\n",
    "                # Record the validation loss per example\n",
    "                valid_losses.append(vloss.cpu().data.numpy()/len(val_feature))\n",
    "\n",
    "                if best_loss is 0:\n",
    "                    best_loss = vloss\n",
    "                    results[(lr,lamb,k,start,split)] = vloss\n",
    "                    print(\"Save best theta...\")\n",
    "                else:\n",
    "                    if vloss < best_loss:\n",
    "                        best_loss = vloss\n",
    "                        results[(lr,lamb,k,start,split)] = vloss\n",
    "                        print(\"Save best theta...\")\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.5738, device='cuda:0', grad_fn=<MseLossBackward>), 0.01, 1e-06]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First run - k=1, lr = 0.01, lamb=1e-6, Full history 70%train-15%valid-15%test\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0.001, 1e-06, 1): tensor(1.5802, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005, 1e-06, 1): tensor(1.5661, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.01, 1e-06, 1): tensor(1.5746, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.05, 1e-06, 1): tensor(1.7742, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.1, 1e-06, 1): tensor(2.1349, device='cuda:0', grad_fn=<MseLossBackward>)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate on lr - Full history 70%train-15%valid-15%test\n",
    "# lr = 5e-3 is acceptable\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0.005, 1e-08, 1): tensor(1.6073, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005, 1e-07, 1): tensor(1.5646, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005, 5e-07, 1): tensor(1.5533, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005, 1e-06, 1): tensor(1.5664, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005, 5e-06, 1): tensor(1.6181, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005, 1e-05, 1): tensor(1.6410, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  0.0001,\n",
       "  1): tensor(1.6890, device='cuda:0', grad_fn=<MseLossBackward>)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate on lamb - Full history 70%train-15%valid-15%test; lr = 5e-3\n",
    "# lamb = 5e-07 is acceptable\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.2): tensor(1.1732, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.3): tensor(1.0440, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.4): tensor(1.0229, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.5): tensor(0.9913, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.6): tensor(1.3874, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.7): tensor(1.5389, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.8): tensor(1.5898, device='cuda:0', grad_fn=<MseLossBackward>)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate on moving window (full history): X%train-10%valid-10%test; lr=5e-3, lamb=5e-7\n",
    "# Variance --> 90% variance of validation set as X > 50%\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0,\n",
       "  0.2): tensor(1.1760, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0,\n",
       "  0.3): tensor(1.0439, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0,\n",
       "  0.4): tensor(1.0225, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0,\n",
       "  0.5): tensor(0.9975, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.1,\n",
       "  0.3): tensor(1.0645, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.1,\n",
       "  0.4): tensor(1.0357, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.1,\n",
       "  0.5): tensor(0.9942, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.1,\n",
       "  0.6): tensor(1.3896, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.2,\n",
       "  0.4): tensor(1.0504, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.2,\n",
       "  0.5): tensor(1.0032, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.2,\n",
       "  0.6): tensor(1.3968, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.2,\n",
       "  0.7): tensor(1.5396, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.3,\n",
       "  0.5): tensor(1.0284, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.3,\n",
       "  0.6): tensor(1.4113, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.3,\n",
       "  0.7): tensor(1.5465, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.3,\n",
       "  0.8): tensor(1.5898, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.4,\n",
       "  0.6): tensor(1.4397, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.4,\n",
       "  0.7): tensor(1.5612, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.4,\n",
       "  0.8): tensor(1.5974, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.5,\n",
       "  0.7): tensor(1.5953, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.5,\n",
       "  0.8): tensor(1.6170, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " (0.005,\n",
       "  5e-07,\n",
       "  1,\n",
       "  0.6,\n",
       "  0.8): tensor(1.6582, device='cuda:0', grad_fn=<MseLossBackward>)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate on moving fixed window: X%train-10%valid-10%test; lr=5e-3, lamb=5e-7\n",
    "# lamb = 5e-07 is acceptable\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split = 80.0%\n",
      "1.3461472153508236\n",
      "1.7674499788103724\n",
      "1.8243738761503898\n",
      "Train split = 70.0%\n",
      "1.2960605307672615\n",
      "1.6961873735369226\n",
      "1.7674499788103724\n",
      "Train split = 60.0%\n",
      "1.2580491692030635\n",
      "1.524080754233291\n",
      "1.6961873735369226\n",
      "Train split = 50.0%\n",
      "1.2909144334309721\n",
      "1.093538923045579\n",
      "1.524080754233291\n",
      "Train split = 40.0%\n",
      "1.3368149521792445\n",
      "1.1073010945009147\n",
      "1.093538923045579\n",
      "Train split = 30.0%\n",
      "1.418168469775325\n",
      "1.092115782188113\n",
      "1.1073010945009147\n",
      "Train split = 20.0%\n",
      "1.5763985212026381\n",
      "1.062104694635307\n",
      "1.092115782188113\n"
     ]
    }
   ],
   "source": [
    "split_values = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n",
    "\n",
    "for split in split_values:\n",
    "    \n",
    "    # Implement a moving window - test and valid datasets are 10% each\n",
    "    train_split = int(N * split)   \n",
    "    valid_split =  int(N * (split+0.1))\n",
    "    test_split =  int(N * (split+0.2))\n",
    "\n",
    "    train_x = data.loc[:train_split, ['gPlusPlaceId','gPlusUserId','num_reviews', \\\n",
    "                                      'unixReviewTime','year','month','day']]\n",
    "    train_y = data.loc[:train_split, 'rating':'rating']\n",
    "    valid_x = data.loc[train_split+1:valid_split, ['gPlusPlaceId','gPlusUserId','num_reviews', \\\n",
    "                                      'unixReviewTime','year','month','day']]\n",
    "    valid_y = data.loc[train_split+1:valid_split, 'rating':'rating']\n",
    "    test_x = data.loc[valid_split+1:test_split, ['gPlusPlaceId','gPlusUserId','num_reviews', \\\n",
    "                                      'unixReviewTime','year','month','day']]\n",
    "    test_y = data.loc[valid_split+1:test_split, 'rating':'rating']\n",
    "    \n",
    "    print(\"Train split = {}%\".format(split*100))\n",
    "    print(np.var(train_y[\"rating\"]))\n",
    "    print(np.var(valid_y[\"rating\"]))\n",
    "    print(np.var(test_y[\"rating\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
