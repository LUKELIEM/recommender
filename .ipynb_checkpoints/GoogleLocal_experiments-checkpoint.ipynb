{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd \n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 - Alpha-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alpha_Only(nn.Module):\n",
    "    \n",
    "    def __init__(self, mean=0):\n",
    "        super(Alpha_Only, self).__init__()\n",
    "        \n",
    "        # alpha only\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, train_x):\n",
    "        item_id = train_x[:, 0]\n",
    "        user_id = train_x[:, 1]\n",
    "        \n",
    "        prediction = (self.bias)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "        return F.mse_loss(prediction, target.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 - Alpha_Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alpha_Theta(nn.Module):\n",
    "    \n",
    "    def __init__(self, mean=0):\n",
    "        super(Alpha_Theta, self).__init__()\n",
    "        \n",
    "        # alpha + theta only\n",
    "        self.theta = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, train_x):\n",
    "        freq = train_x[:, 2].float()\n",
    "        \n",
    "        prediction = (self.bias)+self.theta*freq\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "        return F.mse_loss(prediction, target.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 - Alpha_Theta_MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF_theta(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_user, n_item, k=1):\n",
    "        super(MF_theta, self).__init__()\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        \n",
    "        # gammas (users and items)\n",
    "        self.user = nn.Embedding(n_user, k)\n",
    "        self.item = nn.Embedding(n_item, k)\n",
    "        \n",
    "        # alpha and betas (users and items)\n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        \n",
    "        self.theta = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, train_x):\n",
    "        item_id = train_x[:, 0]\n",
    "        user_id = train_x[:, 1]\n",
    "        freq = train_x[:, 2].float()\n",
    "        vector_user = self.user(user_id)\n",
    "        vector_item = self.item(item_id)\n",
    "        \n",
    "        # Pull out biases\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        biases = (self.bias + self.theta*freq + bias_user + bias_item)\n",
    "        \n",
    "        ui_interaction = torch.sum(vector_user * vector_item, dim=1)\n",
    "        \n",
    "        # Add bias prediction to the interaction prediction\n",
    "        prediction = ui_interaction + biases\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "        return F.mse_loss(prediction, target.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11453845 entries, 0 to 11453844\n",
      "Data columns (total 5 columns):\n",
      "gPlusPlaceId      int64\n",
      "gPlusUserId       int64\n",
      "rating            float64\n",
      "unixReviewTime    object\n",
      "num_reviews       int64\n",
      "dtypes: float64(1), int64(3), object(1)\n",
      "memory usage: 436.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gPlusPlaceId</th>\n",
       "      <th>gPlusUserId</th>\n",
       "      <th>rating</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>num_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1368311</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1372686659</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>370282</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1342870724</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>237940</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1390653513</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>249417</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1389187706</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1181533</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1390486279</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gPlusPlaceId  gPlusUserId  rating unixReviewTime  num_reviews\n",
       "0       1368311            0     3.0     1372686659            3\n",
       "1        370282            1     5.0     1342870724            3\n",
       "2        237940            2     5.0     1390653513            1\n",
       "3        249417            2     5.0     1389187706            2\n",
       "4       1181533            2     4.0     1390486279            1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../datasets/google_local/reviews_freq.csv\")\n",
    "display(data.info())\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5054567 3116785\n"
     ]
    }
   ],
   "source": [
    "n_user = len(data['gPlusUserId'].unique())\n",
    "n_place = len(data['gPlusPlaceId'].unique())\n",
    "\n",
    "print(n_user,n_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11453845 8017692 1718077 1718076\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data\n",
    "shuffled_data = data.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_data.head()\n",
    "\n",
    "N = shuffled_data.index.size\n",
    "\n",
    "train_split = int(N * 0.70)\n",
    "valid_split =  int(N * 0.85)\n",
    "\n",
    "train_x = shuffled_data.loc[:train_split, ['gPlusPlaceId','gPlusUserId','num_reviews']]\n",
    "train_y = shuffled_data.loc[:train_split, 'rating':'rating']\n",
    "valid_x = shuffled_data.loc[train_split+1:valid_split, ['gPlusPlaceId','gPlusUserId','num_reviews']]\n",
    "valid_y = shuffled_data.loc[train_split+1:valid_split, 'rating':'rating']\n",
    "test_x = shuffled_data.loc[valid_split+1:, ['gPlusPlaceId','gPlusUserId','num_reviews']]\n",
    "test_y = shuffled_data.loc[valid_split+1:, 'rating':'rating']\n",
    "\n",
    "print(N, train_x.index.size, valid_x.index.size,test_x.index.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gPlusPlaceId</th>\n",
       "      <th>gPlusUserId</th>\n",
       "      <th>num_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1362493</td>\n",
       "      <td>1883179</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1244850</td>\n",
       "      <td>3217020</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>771251</td>\n",
       "      <td>1608666</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1830167</td>\n",
       "      <td>276218</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1222900</td>\n",
       "      <td>848399</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1098674</td>\n",
       "      <td>660751</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1769168</td>\n",
       "      <td>2721991</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1175141</td>\n",
       "      <td>4882577</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>249338</td>\n",
       "      <td>540961</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1860408</td>\n",
       "      <td>1892466</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gPlusPlaceId  gPlusUserId  num_reviews\n",
       "0       1362493      1883179           44\n",
       "1       1244850      3217020           45\n",
       "2        771251      1608666           12\n",
       "3       1830167       276218           22\n",
       "4       1222900       848399            4\n",
       "5       1098674       660751           12\n",
       "6       1769168      2721991           28\n",
       "7       1175141      4882577           17\n",
       "8        249338       540961           21\n",
       "9       1860408      1892466           10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lr = 1e-2\n",
    "lamb = 1e-6\n",
    "k=1\n",
    "batch_size = 1024\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "print (cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Iteration[0] Training Loss: 2038.10\n",
      "Epoch[0] Validation Loss: 2207.594 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.77\n",
      "Epoch[0] Validation Loss: 1.703 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.42\n",
      "Epoch[0] Validation Loss: 1.435 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.39\n",
      "Epoch[0] Validation Loss: 1.364 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[4000] Training Loss: 1.25\n",
      "Epoch[0] Validation Loss: 1.316 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[5000] Training Loss: 1.28\n",
      "Epoch[0] Validation Loss: 1.282 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[6000] Training Loss: 1.22\n",
      "Epoch[0] Validation Loss: 1.261 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[7000] Training Loss: 1.23\n",
      "Epoch[0] Validation Loss: 1.250 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.94\n",
      "Epoch[1] Validation Loss: 1.254 \n",
      "Epoch[1] Iteration[1000] Training Loss: 0.93\n",
      "Epoch[1] Validation Loss: 1.254 \n",
      "Epoch[1] Iteration[2000] Training Loss: 1.18\n",
      "Epoch[1] Validation Loss: 1.254 \n",
      "Epoch[1] Iteration[3000] Training Loss: 1.16\n",
      "Epoch[1] Validation Loss: 1.255 \n",
      "Epoch[1] Iteration[4000] Training Loss: 1.20\n",
      "Epoch[1] Validation Loss: 1.248 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[5000] Training Loss: 1.34\n",
      "Epoch[1] Validation Loss: 1.247 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[6000] Training Loss: 1.26\n",
      "Epoch[1] Validation Loss: 1.241 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[7000] Training Loss: 1.24\n",
      "Epoch[1] Validation Loss: 1.239 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 1.18\n",
      "Epoch[2] Validation Loss: 1.238 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 1.13\n",
      "Epoch[2] Validation Loss: 1.250 \n",
      "Epoch[2] Iteration[2000] Training Loss: 1.12\n",
      "Epoch[2] Validation Loss: 1.251 \n",
      "Epoch[2] Iteration[3000] Training Loss: 1.24\n",
      "Epoch[2] Validation Loss: 1.265 \n",
      "Epoch[2] Iteration[4000] Training Loss: 1.26\n",
      "Epoch[2] Validation Loss: 1.250 \n",
      "Epoch[2] Iteration[5000] Training Loss: 1.27\n",
      "Epoch[2] Validation Loss: 1.242 \n",
      "Epoch[2] Iteration[6000] Training Loss: 1.16\n",
      "Epoch[2] Validation Loss: 1.241 \n",
      "Epoch[2] Iteration[7000] Training Loss: 1.22\n",
      "Epoch[2] Validation Loss: 1.242 \n",
      "Epoch[3] Iteration[0] Training Loss: 1.00\n",
      "Epoch[3] Validation Loss: 1.251 \n",
      "Epoch[3] Iteration[1000] Training Loss: 1.12\n",
      "Epoch[3] Validation Loss: 1.251 \n",
      "Epoch[3] Iteration[2000] Training Loss: 1.30\n",
      "Epoch[3] Validation Loss: 1.360 \n",
      "Epoch[3] Iteration[3000] Training Loss: 1.06\n",
      "Epoch[3] Validation Loss: 1.258 \n",
      "Epoch[3] Iteration[4000] Training Loss: 1.16\n",
      "Epoch[3] Validation Loss: 1.246 \n",
      "Epoch[3] Iteration[5000] Training Loss: 1.17\n",
      "Epoch[3] Validation Loss: 1.244 \n",
      "Epoch[3] Iteration[6000] Training Loss: 1.30\n",
      "Epoch[3] Validation Loss: 1.260 \n",
      "Epoch[3] Iteration[7000] Training Loss: 1.24\n",
      "Epoch[3] Validation Loss: 1.238 \n",
      "Save best theta...\n",
      "Epoch[4] Iteration[0] Training Loss: 0.99\n",
      "Epoch[4] Validation Loss: 1.242 \n",
      "Epoch[4] Iteration[1000] Training Loss: 1.36\n",
      "Epoch[4] Validation Loss: 1.247 \n",
      "Epoch[4] Iteration[2000] Training Loss: 1.18\n",
      "Epoch[4] Validation Loss: 1.250 \n",
      "Epoch[4] Iteration[3000] Training Loss: 1.21\n",
      "Epoch[4] Validation Loss: 1.251 \n",
      "Epoch[4] Iteration[4000] Training Loss: 1.29\n",
      "Epoch[4] Validation Loss: 1.255 \n",
      "Epoch[4] Iteration[5000] Training Loss: 1.02\n",
      "Epoch[4] Validation Loss: 1.242 \n",
      "Epoch[4] Iteration[6000] Training Loss: 1.21\n",
      "Epoch[4] Validation Loss: 1.242 \n",
      "Epoch[4] Iteration[7000] Training Loss: 1.27\n",
      "Epoch[4] Validation Loss: 1.242 \n",
      "Epoch[5] Iteration[0] Training Loss: 1.18\n",
      "Epoch[5] Validation Loss: 1.242 \n",
      "Epoch[5] Iteration[1000] Training Loss: 0.84\n",
      "Epoch[5] Validation Loss: 1.249 \n",
      "Epoch[5] Iteration[2000] Training Loss: 1.03\n",
      "Epoch[5] Validation Loss: 1.251 \n",
      "Epoch[5] Iteration[3000] Training Loss: 1.19\n",
      "Epoch[5] Validation Loss: 1.249 \n",
      "Epoch[5] Iteration[4000] Training Loss: 1.09\n",
      "Epoch[5] Validation Loss: 1.245 \n",
      "Epoch[5] Iteration[5000] Training Loss: 1.13\n",
      "Epoch[5] Validation Loss: 1.248 \n",
      "Epoch[5] Iteration[6000] Training Loss: 1.26\n",
      "Epoch[5] Validation Loss: 1.261 \n",
      "Epoch[5] Iteration[7000] Training Loss: 1.18\n",
      "Epoch[5] Validation Loss: 1.254 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.72\n",
      "Epoch[6] Validation Loss: 1.252 \n",
      "Epoch[6] Iteration[1000] Training Loss: 0.82\n",
      "Epoch[6] Validation Loss: 1.250 \n",
      "Epoch[6] Iteration[2000] Training Loss: 0.90\n",
      "Epoch[6] Validation Loss: 1.253 \n",
      "Epoch[6] Iteration[3000] Training Loss: 1.23\n",
      "Epoch[6] Validation Loss: 1.250 \n",
      "Epoch[6] Iteration[4000] Training Loss: 1.17\n",
      "Epoch[6] Validation Loss: 1.250 \n",
      "Epoch[6] Iteration[5000] Training Loss: 1.37\n",
      "Epoch[6] Validation Loss: 1.242 \n",
      "Epoch[6] Iteration[6000] Training Loss: 1.41\n",
      "Epoch[6] Validation Loss: 1.249 \n",
      "Epoch[6] Iteration[7000] Training Loss: 1.19\n",
      "Epoch[6] Validation Loss: 1.239 \n",
      "Epoch[7] Iteration[0] Training Loss: 0.89\n",
      "Epoch[7] Validation Loss: 1.238 \n",
      "Save best theta...\n",
      "Epoch[7] Iteration[1000] Training Loss: 1.24\n",
      "Epoch[7] Validation Loss: 1.251 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.96\n",
      "Epoch[7] Validation Loss: 1.255 \n",
      "Epoch[7] Iteration[3000] Training Loss: 1.17\n",
      "Epoch[7] Validation Loss: 1.255 \n",
      "Epoch[7] Iteration[4000] Training Loss: 1.27\n",
      "Epoch[7] Validation Loss: 1.246 \n",
      "Epoch[7] Iteration[5000] Training Loss: 1.25\n",
      "Epoch[7] Validation Loss: 1.244 \n",
      "Epoch[7] Iteration[6000] Training Loss: 1.26\n",
      "Epoch[7] Validation Loss: 1.241 \n",
      "Epoch[7] Iteration[7000] Training Loss: 1.32\n",
      "Epoch[7] Validation Loss: 1.244 \n",
      "Epoch[8] Iteration[0] Training Loss: 1.30\n",
      "Epoch[8] Validation Loss: 1.237 \n",
      "Save best theta...\n",
      "Epoch[8] Iteration[1000] Training Loss: 1.28\n",
      "Epoch[8] Validation Loss: 1.250 \n",
      "Epoch[8] Iteration[2000] Training Loss: 1.30\n",
      "Epoch[8] Validation Loss: 1.290 \n",
      "Epoch[8] Iteration[3000] Training Loss: 1.27\n",
      "Epoch[8] Validation Loss: 1.249 \n",
      "Epoch[8] Iteration[4000] Training Loss: 1.26\n",
      "Epoch[8] Validation Loss: 1.273 \n",
      "Epoch[8] Iteration[5000] Training Loss: 1.16\n",
      "Epoch[8] Validation Loss: 1.244 \n",
      "Epoch[8] Iteration[6000] Training Loss: 1.25\n",
      "Epoch[8] Validation Loss: 1.240 \n",
      "Epoch[8] Iteration[7000] Training Loss: 1.31\n",
      "Epoch[8] Validation Loss: 1.240 \n",
      "Epoch[9] Iteration[0] Training Loss: 0.61\n",
      "Epoch[9] Validation Loss: 1.237 \n",
      "Save best theta...\n",
      "Epoch[9] Iteration[1000] Training Loss: 0.70\n",
      "Epoch[9] Validation Loss: 1.249 \n",
      "Epoch[9] Iteration[2000] Training Loss: 1.21\n",
      "Epoch[9] Validation Loss: 1.252 \n",
      "Epoch[9] Iteration[3000] Training Loss: 1.17\n",
      "Epoch[9] Validation Loss: 1.260 \n",
      "Epoch[9] Iteration[4000] Training Loss: 1.21\n",
      "Epoch[9] Validation Loss: 1.245 \n",
      "Epoch[9] Iteration[5000] Training Loss: 1.19\n",
      "Epoch[9] Validation Loss: 1.253 \n",
      "Epoch[9] Iteration[6000] Training Loss: 1.29\n",
      "Epoch[9] Validation Loss: 1.240 \n",
      "Epoch[9] Iteration[7000] Training Loss: 1.15\n",
      "Epoch[9] Validation Loss: 1.243 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.86\n",
      "Epoch[10] Validation Loss: 1.242 \n",
      "Epoch[10] Iteration[1000] Training Loss: 0.72\n",
      "Epoch[10] Validation Loss: 1.248 \n",
      "Epoch[10] Iteration[2000] Training Loss: 0.89\n",
      "Epoch[10] Validation Loss: 1.253 \n",
      "Epoch[10] Iteration[3000] Training Loss: 1.18\n",
      "Epoch[10] Validation Loss: 1.250 \n",
      "Epoch[10] Iteration[4000] Training Loss: 1.06\n",
      "Epoch[10] Validation Loss: 1.253 \n",
      "Epoch[10] Iteration[5000] Training Loss: 1.24\n",
      "Epoch[10] Validation Loss: 1.242 \n",
      "Epoch[10] Iteration[6000] Training Loss: 1.26\n",
      "Epoch[10] Validation Loss: 1.248 \n",
      "Epoch[10] Iteration[7000] Training Loss: 1.33\n",
      "Epoch[10] Validation Loss: 1.239 \n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "\n",
    "# This code utilizes ignite engine's create_supervised_trainer()\n",
    "# But we need something more basic\n",
    "\n",
    "# model = MF(n_user, n_item, k=k)\n",
    "# model = Bias_Only(n_user, n_item)\n",
    "\n",
    "# Experiment 1 - model = Alpha_Only()\n",
    "# Experiment 2 - model = Alpha_Theta()\n",
    "\n",
    "model = MF_theta(n_user, n_place, k=k)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lamb)\n",
    "\n",
    "def chunks(X, Y, size):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    starts = list(range(0, len(X), size))\n",
    "    shuffle(starts)\n",
    "    for i in starts:\n",
    "        yield (X[i:i + size], Y[i:i + size])\n",
    "        \n",
    "# To keep track to best hyperparameters and results\n",
    "best_loss = 0\n",
    "best = []\n",
    "\n",
    "losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(10+1):\n",
    "    \n",
    "    i = 0\n",
    "    for feature, target in chunks(np.array(train_x), np.array(train_y), batch_size):\n",
    "        # This zeros the gradients on every parameter. \n",
    "        # This is easy to miss and hard to troubleshoot.\n",
    "        optimizer.zero_grad()\n",
    "        # Convert \n",
    "        feature = Variable(torch.from_numpy(feature))\n",
    "        target = Variable(torch.from_numpy(target).type(torch.FloatTensor))\n",
    "        \n",
    "        if cuda:\n",
    "            feature = feature.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "        # model in training mode    \n",
    "        model.train()\n",
    "            \n",
    "        # Compute a prediction for these features\n",
    "        prediction = model.forward(feature)\n",
    "        # Compute a loss given what the true target outcome was\n",
    "        loss = model.loss(prediction, target)\n",
    "        # break\n",
    "        # Backpropagate: compute the direction / gradient every model parameter\n",
    "        # defined in your __init__ should move in in order to minimize this loss\n",
    "        # However, we're not actually changing these parameters, we're just storing\n",
    "        # how they should change.\n",
    "\n",
    "        loss.backward()\n",
    "        # Now take a step & update the model parameters. The optimizer uses the gradient at \n",
    "        # defined on every parameter in our model and nudges it in that direction.\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%1000 == 0 and epoch%1 == 0:\n",
    "            print(\"Epoch[{}] Iteration[{}] Training Loss: {:.2f}\".format(epoch, i, loss.data))\n",
    "\n",
    "        # Record the loss per example\n",
    "        losses.append(loss.cpu().data.numpy() / len(feature))\n",
    "        \n",
    "        if i%1000 == 0 and epoch%1 == 0:\n",
    "            \n",
    "            val_feature = torch.from_numpy(np.array(valid_x))\n",
    "            val_target = torch.from_numpy(np.array(valid_y)).type(torch.FloatTensor)\n",
    "            \n",
    "            if cuda:\n",
    "                val_feature = val_feature.cuda()\n",
    "                val_target = val_target.cuda()\n",
    "                \n",
    "            # model in test mode    \n",
    "            model.eval()\n",
    "\n",
    "            val_pred = model.forward(val_feature)\n",
    "            vloss = model.loss(val_pred, val_target)\n",
    "            print(\"Epoch[{}] Validation Loss: {:.3f} \".format(epoch, vloss.data))\n",
    "            \n",
    "            # Record the validation loss per example\n",
    "            valid_losses.append(vloss.cpu().data.numpy()/len(val_feature))\n",
    "            \n",
    "            if best_loss is 0:\n",
    "                best_loss = vloss\n",
    "                best = [vloss,lr,lamb]\n",
    "                print(\"Save best theta...\")\n",
    "            else:\n",
    "                if vloss < best_loss:\n",
    "                    best_loss = vloss\n",
    "                    best = [vloss,lr,lamb]\n",
    "                    print(\"Save best theta...\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.4290, device='cuda:0', grad_fn=<MseLossBackward>), 0.01, 1e-06]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alpha-only model --> alpha = mean\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0432], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.4249, device='cuda:0', grad_fn=<MseLossBackward>), 0.01, 1e-06]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alpha-Theta model --> alpha ~ mean, popularity has slight uplift to rating\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.0180], device='cuda:0')\n",
      "tensor([0.0014], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.bias.data)\n",
    "print(model.theta.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.2371, device='cuda:0', grad_fn=<MseLossBackward>), 0.01, 1e-06]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MF-Theta model --> Does not outperform MF(k=1) model\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.9391], device='cuda:0')\n",
      "tensor([0.0001], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.bias.data)\n",
    "print(model.theta.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
