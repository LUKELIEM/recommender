{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import numpy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(path):\n",
    "  f = open(path, 'rt')\n",
    "  f.readline()\n",
    "  for l in f:\n",
    "    yield l.strip().split(',')\n",
    "    \n",
    "def calc_model_stats(pred,label):\n",
    "    \n",
    "    TP,FP,TN,FN = calc_metrics(pred,label)\n",
    "\n",
    "    # print(\"Stats\")\n",
    "    # print(TP,FP,TN,FN)\n",
    "\n",
    "    # print(\"Predict N: {} ({}%)\".format(TN+FN,(TN+FN)/(TP+TN+FP+FN)))\n",
    "    # print(\"Predict P: {} ({}%)\".format(TP+FP,(TP+FP)/(TP+TN+FP+FN)))\n",
    "\n",
    "    accuracy, TPR, TNR, BER = calc_error_rates(TP, FP, TN, FN)\n",
    "\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    # print(\"TPR: {}\".format(TPR))\n",
    "    # print(\"TNR: {}\".format(TNR))\n",
    "    # print(\"BER: {}\".format(BER))\n",
    "    \n",
    "    return\n",
    " \n",
    "def calc_metrics(predictions, labels):\n",
    "    # Calculate True positives, false positives, etc.\n",
    "\n",
    "    TP_ = numpy.logical_and(predictions, labels)\n",
    "    FP_ = numpy.logical_and(predictions, numpy.logical_not(labels))\n",
    "    TN_ = numpy.logical_and(numpy.logical_not(predictions), numpy.logical_not(labels))\n",
    "    FN_ = numpy.logical_and(numpy.logical_not(predictions), labels)\n",
    "\n",
    "    TP=sum(TP_)\n",
    "    FP=sum(FP_)\n",
    "    TN=sum(TN_)\n",
    "    FN=sum(FN_)\n",
    "    \n",
    "    return TP,FP,TN,FN\n",
    "\n",
    "def calc_error_rates(TP, FP, TN, FN):\n",
    "    # Calculate accuracy, TPR, TNR and BER\n",
    "    accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    TPR = TP/(TP+FN)\n",
    "    TNR = TN/(TN+FP)\n",
    "    BER = 1.0 - (TPR+TNR)/2\n",
    "    \n",
    "    return accuracy, TPR, TNR, BER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate lists and sets for Similarity Calculations\n",
    "\n",
    "#### Jaccard Similarity between Books\n",
    "\n",
    "Jsim(i,j) = Intersection(U_i, U_j)/ Union(U_i, U_j)  \n",
    "\n",
    "where U_i, U_j are set of readers who have read book i and book j  \n",
    "\n",
    "#### Pearson Similarity between Books\n",
    "\n",
    "\n",
    "Psim(i,j) = Sum_Product_crij((R_u_i-AR_u), (R_u_j-AR_u))/ SqRt(Sum_crij(Sq(R_u_i-AR_u)))SqRt(Sum_crij(Sq(R_u_j-AR_u)))  \n",
    "\n",
    "where \n",
    "* crij - the set of readers who have read both book i and j  \n",
    "* R_u_i = Rating that reader u gives book i  \n",
    "* AR_u = Average rating given by reader u  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0670b93d3747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbook_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrating\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0muser_Books_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0muser_Books_Ratings_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "user_Books_full = defaultdict(set)  # Set of (user, books_read)\n",
    "user_Books_Ratings_full = defaultdict(set)  # Set of (user, set(book_read, rating))\n",
    "book_Readers_full = defaultdict(set)  # Set of (book, readers)\n",
    "book_ids=[]\n",
    "\n",
    "for user,book,rating in X:\n",
    "  user_Books_full[user].add(book)\n",
    "  user_Books_Ratings_full[user].add((book,rating))\n",
    "  book_Readers_full[book].add(user)\n",
    "  if book not in book_ids:\n",
    "        book_ids.append(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11357, 11357, 7170, 7170)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_Books_full),len(user_Books_Ratings_full),len(book_Readers_full), len(book_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_Books_train = defaultdict(set)\n",
    "user_Books_Ratings_train = defaultdict(set)\n",
    "book_Readers_train = defaultdict(set)\n",
    "book_Ratings_train = defaultdict(set)\n",
    "\n",
    "for user,book,rating in Xtrain:\n",
    "  user_Books_train[user].add(book)\n",
    "  user_Books_Ratings_train[user].add((book,rating))\n",
    "  book_Readers_train[book].add(user)\n",
    "  book_Ratings_train[book].add(rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11357, 11357, 7170, 7170)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_Books_train),len(user_Books_Ratings_train),len(book_Readers_train),len(book_Ratings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_pad(l, n):\n",
    "    # if len(l) > n, the method truncates the list l to length n \n",
    "    # if n > len(l), the method pads list l with zero up to length n \n",
    "    return l[:n] + [0]*(n-len(l))\n",
    "\n",
    "def find_rating(reader,book):\n",
    "    rating = 0\n",
    "    for b, rating in user_Books_Ratings_train[reader]:\n",
    "        if book == b:\n",
    "            return int(rating)\n",
    "    return int(rating)\n",
    "\n",
    "def average_rating(reader):\n",
    "    ratings = []\n",
    "    for _, rating in user_Books_Ratings_train[reader]:\n",
    "        ratings.append(int(rating))\n",
    "    \n",
    "    if len(ratings) == 0:\n",
    "        return 0\n",
    "    \n",
    "    return sum(ratings)/len(ratings)\n",
    "\n",
    "def unread_book(user, read_books):\n",
    "    # Find a random unread book for a specific user\n",
    "    \n",
    "    book = random.choice(book_ids)  # pick a book from full library\n",
    "    while book in read_books:\n",
    "        book = random.choice(book_ids)\n",
    "    return book\n",
    "\n",
    "\"\"\"\n",
    "Jaccard Similarity\n",
    "\"\"\"\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "\"\"\"\n",
    "When evaluating whether a reader will read a certain book, we can approach it in two ways:\n",
    "(1) Is the book similar to all the other books the reader has read?\n",
    "(2) Is the reader similar to all the other readers who have read the same book?\n",
    "\"\"\"\n",
    "\n",
    "def items_jsim(book1, book2):\n",
    "    # generate readers set for the 2 books based on train dataset\n",
    "    s1 = book_Readers_train[book1]\n",
    "    s2 = book_Readers_train[book2]\n",
    "    return Jaccard(s1, s2)\n",
    "\n",
    "def users_jsim(reader1, reader2):\n",
    "    # generate Books set for the 2 readers based on train dataset\n",
    "    s1 = user_Books_train[reader1]\n",
    "    s2 = user_Books_train[reader2]\n",
    "    return Jaccard(s1, s2)\n",
    "\n",
    "def jsim_mostSimilar_items(r,b):\n",
    "    # Find books read by reader r that are most similar to book b\n",
    "    # and return Jaccard(b, book) in descending order\n",
    "    jsims = []\n",
    "    \n",
    "    # Go through the books read by reader r\n",
    "    for other_book in user_Books_train[r]:\n",
    "        if b == other_book: \n",
    "           continue   # skip if the book is b\n",
    "        jsim = items_jsim(b, other_book)\n",
    "        jsims.append(jsim)\n",
    "       \n",
    "    jsims.sort(reverse=True)  # sort in descending order of Jaccard Similarity\n",
    "    return jsims  \n",
    "\n",
    "def jsim_mostSimilar_users(b,r):\n",
    "    # Find readers who read book b that are most similar to the reader r\n",
    "    # and return Jaccard(r, reader) in descending order\n",
    "    jsims = []\n",
    "    \n",
    "    # Go through the readers who read book b\n",
    "    for reader in book_Readers_train[b]:\n",
    "        if r == reader: \n",
    "           continue   # skip if the reader is r\n",
    "        jsim = users_jsim(r, reader)\n",
    "        jsims.append(jsim)\n",
    "       \n",
    "    jsims.sort(reverse=True)  # sort in descending order of Jaccard Similarity\n",
    "    return jsims  \n",
    "\n",
    "\n",
    "def jaccard_predict(user, book, threshold):\n",
    "    \"\"\"\n",
    "    user_jsims = jsim_mostSimilar_users(book,user)\n",
    "    if len(user_jsims) is 0:\n",
    "        max_jsim = 0\n",
    "    else:\n",
    "        max_jsim = user_jsims[0]\n",
    "    \"\"\"    \n",
    "    book_jsims = jsim_mostSimilar_items(user,book)\n",
    "    if len(book_jsims) is 0:\n",
    "        max_jsim = 0\n",
    "    else:\n",
    "        max_jsim = book_jsims[0]\n",
    "        \n",
    "    if max_jsim > threshold:\n",
    "        predict = True\n",
    "    else:\n",
    "        predict = False\n",
    "    return predict, max_jsim\n",
    "\n",
    "\"\"\"\n",
    "Pearson Similarity\n",
    "\n",
    "When evaluating whether a reader will read a certain book, we can approach it in two ways:\n",
    "(1) Is the book similar to all the other books the reader has read?\n",
    "(2) Is the reader similar to all the other readers who have read the same book?\n",
    "\"\"\"\n",
    "\n",
    "def users_Pearson(r1,r2):\n",
    "    \n",
    "    books_r1 = set([])\n",
    "    ratings_r1 = []\n",
    "    \n",
    "    for book,rating in user_Books_Ratings_train[r1]:\n",
    "        # print (book,rating)\n",
    "        books_r1.add(book)\n",
    "        ratings_r1.append(int(rating))\n",
    "        \n",
    "    if len(ratings_r1) == 0:\n",
    "        avRating_r1 = 0\n",
    "    else:\n",
    "        avRating_r1 = sum(ratings_r1)/len(ratings_r1)\n",
    "        \n",
    "    # print(books_r1)\n",
    "    # print(avRating_r1)\n",
    "    \n",
    "    books_r2 = set([])\n",
    "    ratings_r2 = []\n",
    "    \n",
    "    for book,rating in user_Books_Ratings_train[r2]:\n",
    "        # print (book,rating)\n",
    "        books_r2.add(book)\n",
    "        ratings_r2.append(int(rating))\n",
    "        \n",
    "    if len(ratings_r2) == 0:\n",
    "        avRating_r2 = 0\n",
    "    else:\n",
    "        avRating_r2 = sum(ratings_r2)/len(ratings_r2)  \n",
    "    \n",
    "    # print(books_r2)\n",
    "    # print(avRating_r2)\n",
    "        \n",
    "    common = books_r1.intersection(books_r2)\n",
    "    if len(common) == 0:  # return psim=0 if no common book\n",
    "        return 0\n",
    "        \n",
    "    # print(common)\n",
    "    \n",
    "    sumProducts = []\n",
    "    sumSq_R1 = []\n",
    "    sumSq_R2 = []\n",
    "    for book in common:\n",
    "        pR_r1 = find_rating(r1,book) - avRating_r1\n",
    "        pR_r2 = find_rating(r2,book) - avRating_r2\n",
    "        # print(find_rating(r1,book), pR_r1)\n",
    "        # print(find_rating(r2,book), pR_r2)\n",
    "        sumProducts.append(pR_r1*pR_r2)\n",
    "        sumSq_R1.append(pR_r1**2)\n",
    "        sumSq_R2.append(pR_r2**2)\n",
    "        \n",
    "    if sum(sumSq_R1) == 0 or sum(sumSq_R2) == 0:\n",
    "        return 0\n",
    "        \n",
    "    # print(sumProducts)\n",
    "    # print(sumSq_R1)\n",
    "    # print(sumSq_R2)\n",
    "    \n",
    "    psim = sum(sumProducts)/math.sqrt(sum(sumSq_R1)*sum(sumSq_R2))\n",
    "    return psim\n",
    "\n",
    "def items_Pearson(b1,b2):\n",
    "    \n",
    "    # Create a set of common readers for book b1 and b2\n",
    "    readers_b1 = set([])\n",
    "    for reader in book_Readers_train[b1]:\n",
    "        readers_b1.add(reader)\n",
    "        \n",
    "    readers_b2 = set([])\n",
    "    for reader in book_Readers_train[b2]:\n",
    "        readers_b2.add(reader)\n",
    "        \n",
    "    common = readers_b1.intersection(readers_b2)\n",
    "    if len(common) == 0:  # return psim=0 if no common readers\n",
    "        return 0\n",
    "        \n",
    "    # print(common)\n",
    "    \n",
    "    sumProducts = []\n",
    "    sumSq_R1 = []\n",
    "    sumSq_R2 = []\n",
    "    for reader in common:\n",
    "        avRating = average_rating(reader)\n",
    "        # print(avRating)\n",
    "        pR_b1 = find_rating(reader,b1) - avRating\n",
    "        pR_b2 = find_rating(reader,b2) - avRating\n",
    "        # print(find_rating(reader,b1), pR_b1)\n",
    "        # print(find_rating(reader,b2), pR_b2)\n",
    "        sumProducts.append(pR_b1*pR_b2)\n",
    "        sumSq_R1.append(pR_b1**2)\n",
    "        sumSq_R2.append(pR_b2**2)\n",
    "        \n",
    "    if sum(sumSq_R1) == 0 or sum(sumSq_R2) == 0:\n",
    "        return 0\n",
    "        \n",
    "    # print(sumProducts)\n",
    "    # print(sumSq_R1)\n",
    "    # print(sumSq_R2)\n",
    "    \n",
    "    psim = sum(sumProducts)/math.sqrt(sum(sumSq_R1)*sum(sumSq_R2))\n",
    "    return psim\n",
    "\n",
    "def psim_mostSimilar_users(b,r):\n",
    "    # Find readers who read book b that are most similar to the reader r\n",
    "    # and return Pearson(r, reader) in descending order\n",
    "    psims = []\n",
    "    \n",
    "    # Go through the readers who read book b\n",
    "    for reader in book_Readers_train[b]:\n",
    "        if r == reader: \n",
    "           continue   # skip if the reader is r\n",
    "        psim = users_Pearson(r, reader)\n",
    "        # print (reader, psim)\n",
    "        psims.append(psim)\n",
    "       \n",
    "    psims.sort(reverse=True)  # sort in descending order of Jaccard Similarity\n",
    "    return psims  \n",
    "\n",
    "def psim_mostSimilar_items(r,b):\n",
    "    # Find books read by reader r that are most similar to the book b\n",
    "    # and return Pearson(b, book) in descending order\n",
    "    psims = []\n",
    "    \n",
    "    # Go through the books read by reader r \n",
    "    for book in user_Books_train[r]:\n",
    "        if book == b: \n",
    "           continue   # skip if the book is b\n",
    "        psim = items_Pearson(b, book)\n",
    "        # print (book, psim)\n",
    "        psims.append(psim)\n",
    "       \n",
    "    psims.sort(reverse=True)  # sort in descending order of Jaccard Similarity\n",
    "    return psims  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training Dataset into Train and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 190001 9999\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for user,book,rating in readCSV(\"assignment1/train_Interactions.csv\"):\n",
    "  dataset.append([user,book,rating])\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "X = [values[0:2] for values in dataset]\n",
    "y = [int(values[-1]) for values in dataset]\n",
    "\n",
    "N = len(dataset)\n",
    "Ntrain = 190001\n",
    "\n",
    "Xtrain = X[:Ntrain]\n",
    "Xvalid = X[Ntrain:]\n",
    "\n",
    "ytrain = y[:Ntrain]\n",
    "yvalid = y[Ntrain:]\n",
    "\n",
    "print(N, len(ytrain),len(yvalid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adopt code from Workbook 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11357 7170\n",
      "['u98257448', 'u91435928', 'u09361436']\n",
      "['b50180436', 'b96396550', 'b35131310']\n"
     ]
    }
   ],
   "source": [
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "\n",
    "for user,book,rating in dataset:\n",
    "    ratingsPerUser[user].append(rating)\n",
    "    ratingsPerItem[book].append(rating)\n",
    "\n",
    "N = len(Xtrain)\n",
    "nUsers = len(ratingsPerUser)\n",
    "nItems = len(ratingsPerItem)\n",
    "users = list(ratingsPerUser.keys())\n",
    "items = list(ratingsPerItem.keys())\n",
    "\n",
    "print (nUsers, nItems)\n",
    "print (users[:3])\n",
    "print (items[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of ratings in training set: 3.8967426487229013\n"
     ]
    }
   ],
   "source": [
    "ratingMean = sum([y for y in ytrain])/len(ytrain)\n",
    "\n",
    "print(\"Mean of ratings in training set: {}\".format(ratingMean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ratingMean\n",
    "\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item]\n",
    "\n",
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))\n",
    "    \n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(user, book) for user,book in Xtrain]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost\n",
    "\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(dataset)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for value in zip(Xtrain,ytrain):\n",
    "        x,rating = value\n",
    "        user = x[0]\n",
    "        book = x[1]\n",
    "        pred = prediction(user,book)\n",
    "        diff = pred - rating\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[user] += 2/N*diff\n",
    "        dItemBiases[book] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4731564256297192"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alwaysPredictMean = [ratingMean for d in Xtrain]\n",
    "\n",
    "MSE(alwaysPredictMean, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4731564256256267"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.var(ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (9) Lambda=1.0, MSE=1.509 for Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.4731564256297192\n",
      "MSE = 1.455708491441564\n",
      "MSE = 1.4730066120374512\n",
      "MSE = 1.4730068696756777\n",
      "MSE = 1.4730066177516945\n",
      "MSE = 1.4730066121675425\n",
      "MSE = 1.4730066120404\n",
      "MSE = 1.473006612037573\n",
      "MSE = 1.4730066120374525\n",
      "MSE = 1.4730066120374512\n",
      "MSE = 1.4730066120374496\n",
      "MSE = 1.4730066120374496\n",
      "MSE = 1.4730066120374508\n",
      "MSE = 1.4730066120374496\n",
      "MSE = 1.473006612037451\n",
      "MSE = 1.4730066120374496\n",
      "MSE = 1.4730066120374496\n",
      "MSE = 1.47300661203745\n",
      "MSE = 1.4730066120374496\n",
      "MSE = 1.4730066120374508\n",
      "MSE = 1.4730066120374496\n"
     ]
    }
   ],
   "source": [
    "lamb = 1.0\n",
    "theta,_,_ = scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),derivative, args = (ytrain, lamb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract alpha and betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpack(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8967426487228924"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4982149392405215\n"
     ]
    }
   ],
   "source": [
    "predictions = [prediction(user, book) for user,book in Xvalid]\n",
    "cost = MSE(predictions, yvalid)\n",
    "\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10) 'u11591742' and 'b57299824' have highest betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('u11591742', -0.0015229974227424891),\n",
       " ('u39338306', -0.0012046010652446205),\n",
       " ('u48159222', -0.0008497381739954822),\n",
       " ('u00087472', -0.0007321491665724385),\n",
       " ('u15620043', -0.0006482229595105764),\n",
       " ('u51233768', -0.0006183425164041663),\n",
       " ('u76571258', -0.0006093620730744994),\n",
       " ('u74183154', -0.0005933062271676205),\n",
       " ('u80513837', -0.0005463358657893766),\n",
       " ('u00713548', -0.0005189052238680386)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_userBiases  = sorted(userBiases.items(), key=lambda x:x[1])\n",
    "\n",
    "sorted_userBiases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b57299824', -0.00027942781558469847),\n",
       " ('b47900823', -0.00022407721980972485),\n",
       " ('b71442874', -0.00021917761743954465),\n",
       " ('b38779837', -0.0002093262057223682),\n",
       " ('b81528778', -0.00020731350949277974),\n",
       " ('b84203038', -0.00020083507432501843),\n",
       " ('b22593148', -0.00019115000810877167),\n",
       " ('b47305082', -0.00018581330078309044),\n",
       " ('b04083790', -0.0001815139073458508),\n",
       " ('b38443676', -0.00018083376718593718)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_itemBiases  = sorted(itemBiases.items(), key=lambda x:x[1])\n",
    "\n",
    "sorted_itemBiases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 8e-06\n",
      "MSE = 1.4805441297166555\n",
      "MSE = 2.2975032938044913\n",
      "MSE = 1.4730802389857092\n",
      "MSE = 1.47283439867937\n",
      "MSE = 1.47245754248562\n",
      "MSE = 1.4712028251923286\n",
      "MSE = 1.468214525005397\n",
      "MSE = 1.4603205464946034\n",
      "MSE = 1.4412012310426663\n",
      "MSE = 1.3985802959211986\n",
      "MSE = 1.3240231683245256\n",
      "MSE = 1.2422098107741841\n",
      "MSE = 1.2013979809025106\n",
      "MSE = 1.1948764468321484\n",
      "MSE = 1.1947004618980506\n",
      "MSE = 1.1943469354789478\n",
      "MSE = 1.1909235784485865\n",
      "MSE = 1.1822434666020247\n",
      "MSE = 1.1608308765149138\n",
      "MSE = 1.1234835769880327\n",
      "MSE = 1.0825351175209923\n",
      "MSE = 1.0616086532523175\n",
      "MSE = 104.1917756629133\n",
      "MSE = 1.0574743990729425\n",
      "MSE = 1.055236289984109\n",
      "MSE = 1.0540357232641846\n",
      "MSE = 1.049017357338321\n",
      "MSE = 1.033377106858927\n",
      "MSE = 1.011601662942796\n",
      "MSE = 0.9888519577319868\n",
      "MSE = 0.9804194113980523\n",
      "MSE = 0.9785713266891976\n",
      "MSE = 0.9646743416651443\n",
      "MSE = 0.9510175622929047\n",
      "MSE = 0.9518991056066791\n",
      "MSE = 0.9494408362594754\n",
      "MSE = 0.9484913864572868\n",
      "MSE = 0.9366580187161991\n",
      "MSE = 0.9332381583327394\n",
      "MSE = 0.9326796220528818\n",
      "MSE = 0.9324726928516331\n",
      "MSE = 0.9299579229331655\n",
      "MSE = 1.221225362115514\n",
      "MSE = 0.930208751714385\n",
      "MSE = 0.9231678099583427\n",
      "MSE = 0.9226769421590321\n",
      "MSE = 0.9190472670363259\n",
      "MSE = 1.8845039661733578\n",
      "MSE = 0.9188030986939252\n",
      "MSE = 0.9178889191923496\n",
      "MSE = 0.9179808734538272\n",
      "MSE = 0.9164407692687244\n",
      "MSE = 0.9156452748785123\n",
      "MSE = 0.9155160746385578\n",
      "MSE = 0.9153485776364774\n",
      "MSE = 0.9151504105818357\n",
      "MSE = 0.9143388684566325\n",
      "MSE = 0.9134251212835282\n",
      "MSE = 0.9108068113615688\n",
      "MSE = 0.9122898412954559\n",
      "MSE = 0.9113913001085949\n",
      "MSE = 0.9106321554679607\n",
      "MSE = 0.9109279383242049\n",
      "MSE = 0.9114693330983306\n",
      "MSE = 0.9112833154759551\n",
      "MSE = 0.9119006116696724\n",
      "MSE = 0.9169758211920697\n",
      "MSE = 0.9110941325938168\n",
      "MSE = 0.9105675429579448\n",
      "MSE = 0.910335671941188\n",
      "MSE = 0.9102946576824631\n",
      "MSE = 0.9102226478754628\n",
      "MSE = 0.9101549996208808\n",
      "MSE = 0.9101719866944091\n",
      "MSE = 0.9109860985657334\n",
      "MSE = 0.9103173587214699\n",
      "MSE = 0.9105482883319141\n",
      "MSE = 0.9106475750488902\n",
      "MSE = 0.9107897931854476\n",
      "MSE = 0.9112232250236593\n",
      "MSE = 0.9109084407076962\n",
      "MSE = 0.9106658961246321\n",
      "MSE = 0.9103296021402382\n",
      "MSE = 0.9102771451267014\n",
      "MSE = 0.9100804867125702\n",
      "MSE = 0.909753456679266\n",
      "MSE = 0.909724019642887\n",
      "MSE = 0.9096644344901893\n",
      "MSE = 0.9093436511597133\n",
      "MSE = 0.9094854649654086\n",
      "MSE = 0.9095677929246605\n",
      "MSE = 0.9096880479793058\n",
      "MSE = 0.9097003411465059\n",
      "MSE = 0.9097014398482081\n",
      "MSE = 0.9096016867898282\n",
      "MSE = 0.9093924170842419\n",
      "MSE = 0.9090035393927662\n",
      "MSE = 0.9091985729678891\n",
      "MSE = 0.9089515854107565\n",
      "MSE = 0.9087788279650931\n",
      "MSE = 0.9087430798520238\n",
      "MSE = 0.908728262289753\n",
      "MSE = 0.9381911955501917\n",
      "MSE = 0.9087112691325057\n",
      "MSE = 0.9086863494954539\n",
      "MSE = 0.9086471163080778\n",
      "MSE = 0.9086842800816461\n",
      "MSE = 0.9086933494830229\n",
      "MSE = 0.908723445557574\n",
      "MSE = 0.9081823394598624\n",
      "MSE = 0.9084247686455235\n",
      "MSE = 0.9085751358724311\n",
      "MSE = 0.9085530188199838\n",
      "MSE = 0.9085444205103282\n",
      "MSE = 0.9085256061619498\n",
      "MSE = 0.9084936443309712\n",
      "MSE = 0.9084327186576437\n",
      "MSE = 0.9082675445220737\n",
      "MSE = 0.9082395022834083\n",
      "MSE = 0.9080854064386615\n",
      "MSE = 0.9081641218512834\n",
      "MSE = 0.9081526561882209\n",
      "MSE = 0.908114823380825\n",
      "MSE = 0.9080703266470894\n",
      "MSE = 0.9080005738194088\n",
      "MSE = 0.9079242892402497\n",
      "MSE = 0.9073298589807521\n",
      "MSE = 0.9076423473971663\n",
      "MSE = 0.9076506755105155\n",
      "MSE = 0.9077444075600315\n",
      "MSE = 0.9077817653099427\n",
      "MSE = 0.9079436649520891\n",
      "MSE = 0.9077734438920558\n",
      "MSE = 0.9077872661286466\n",
      "MSE = 0.9077776812629103\n",
      "MSE = 0.9077490992648306\n",
      "MSE = 0.9077260163286799\n",
      "MSE = 0.9077300028753469\n",
      "MSE = 0.9076082584604848\n",
      "MSE = 0.9076639881335092\n",
      "MSE = 0.9076220898895038\n",
      "MSE = 0.9076285608583499\n",
      "MSE = 0.9077087059031035\n",
      "MSE = 0.9077432242509217\n",
      "MSE = 0.9080550998808135\n",
      "MSE = 0.9077460843094426\n",
      "MSE = 0.9078075522895135\n",
      "MSE = 0.9078233916577723\n",
      "MSE = 0.907540313430952\n",
      "MSE = 0.907595415042383\n",
      "MSE = 0.90757345585795\n",
      "MSE = 0.908258171162068\n",
      "MSE = 0.9075849438398843\n",
      "MSE = 0.9075753086154429\n",
      "MSE = 0.9075738073624682\n",
      "MSE = 0.9075735245145866\n",
      "MSE = 0.9075734693439068\n",
      "MSE = 0.9075734585099912\n",
      "MSE = 0.9075734563795813\n",
      "MSE = 0.9075734559604838\n",
      "MSE = 0.9075734558782556\n",
      "MSE = 0.9075734558620818\n",
      "MSE = 0.9075734558727491\n",
      "MSE = 0.9075734558641863\n",
      "MSE = 0.9075734558624605\n",
      "MSE = 0.9075734558620818\n",
      "Save best theta...\n",
      "1.1108039952167454\n",
      "Lambda = 1e-05\n",
      "MSE = 1.4810478855346667\n",
      "MSE = 2.2929368491247373\n",
      "MSE = 1.4730843294408031\n",
      "MSE = 1.4728343984023375\n",
      "MSE = 1.4724679750812373\n",
      "MSE = 1.471218246291605\n",
      "MSE = 1.4682641646322032\n",
      "MSE = 1.4604339366322152\n",
      "MSE = 1.4414759506098964\n",
      "MSE = 1.3991118768743513\n",
      "MSE = 1.3247053133878246\n",
      "MSE = 1.2425301564720366\n",
      "MSE = 1.2013583386072795\n",
      "MSE = 1.194922431198185\n",
      "MSE = 1.1948457888097792\n",
      "MSE = 1.1945434299801687\n",
      "MSE = 1.19133515903619\n",
      "MSE = 1.18292714365086\n",
      "MSE = 1.1618766959372562\n",
      "MSE = 1.124766722095555\n",
      "MSE = 1.0836385423496135\n",
      "MSE = 1.0618453586028234\n",
      "MSE = 119.09046809435006\n",
      "MSE = 1.0577603397075885\n",
      "MSE = 1.0556149338860323\n",
      "MSE = 1.0544587273518613\n",
      "MSE = 1.049423479320352\n",
      "MSE = 1.0337152691016158\n",
      "MSE = 1.011906087449072\n",
      "MSE = 0.9887512467329409\n",
      "MSE = 0.9804130113197722\n",
      "MSE = 0.9799049959525784\n",
      "MSE = 0.9783750687876808\n",
      "MSE = 0.9751328218546074\n",
      "MSE = 0.9697600380863757\n",
      "MSE = 0.9677747472164752\n",
      "MSE = 0.9641756056721443\n",
      "MSE = 0.9595752717576795\n",
      "MSE = 0.9571009729775906\n",
      "MSE = 0.956619782279862\n",
      "MSE = 0.954003328321312\n",
      "MSE = 0.9385453293733169\n",
      "MSE = 1.103141751900643\n",
      "MSE = 0.9387613314375771\n",
      "MSE = 0.937923444556958\n",
      "MSE = 0.9365675910571787\n",
      "MSE = 0.9348399736438482\n",
      "MSE = 0.9326702619271959\n",
      "MSE = 0.9259994293060643\n",
      "MSE = 0.9265389352006528\n",
      "MSE = 0.9258418118352342\n",
      "MSE = 0.9256584880788747\n",
      "MSE = 0.9259331259703231\n",
      "MSE = 0.9259430266224108\n",
      "MSE = 0.9402298419549903\n",
      "MSE = 0.9255248913583554\n",
      "MSE = 0.9255002749135285\n",
      "MSE = 0.9248383776420976\n",
      "MSE = 0.9234171630382008\n",
      "MSE = 0.9214938550304022\n",
      "MSE = 0.9210816453005595\n",
      "MSE = 0.919989560553983\n",
      "MSE = 0.920362489178643\n",
      "MSE = 0.9202608149014618\n",
      "MSE = 0.9200352214454581\n",
      "MSE = 0.9198548080241249\n",
      "MSE = 0.919358794930107\n",
      "MSE = 0.9194132207016977\n",
      "MSE = 0.9157054957729299\n",
      "MSE = 0.917717258377936\n",
      "MSE = 0.9177205489179697\n",
      "MSE = 0.9176023152076708\n",
      "MSE = 0.917492879035585\n",
      "MSE = 0.9159706378376191\n",
      "MSE = 0.9237538868161094\n",
      "MSE = 0.9157292810685984\n",
      "MSE = 0.9156050670398269\n",
      "MSE = 0.9155343929117313\n",
      "MSE = 0.9154433237417383\n",
      "MSE = 0.9152784710567868\n",
      "MSE = 0.9149406455119352\n",
      "MSE = 0.9146421966648741\n",
      "MSE = 0.9118236046163084\n",
      "MSE = 0.9138146654115533\n",
      "MSE = 0.9139681113690465\n",
      "MSE = 0.9140248497210338\n",
      "MSE = 0.9140620782317839\n",
      "MSE = 0.914220084585211\n",
      "MSE = 0.920890337918312\n",
      "MSE = 0.9140433828861091\n",
      "MSE = 0.9142374367240268\n",
      "MSE = 0.9144310104960476\n",
      "MSE = 0.9139342314069524\n",
      "MSE = 0.9130536864840768\n",
      "MSE = 0.9125496769470649\n",
      "MSE = 0.9124275515687966\n",
      "MSE = 0.9107425563571495\n",
      "MSE = 0.9121863872237209\n",
      "MSE = 0.9121528359518861\n",
      "MSE = 0.9121716953853444\n",
      "MSE = 0.9124480229800708\n",
      "MSE = 0.9120513132834616\n",
      "MSE = 0.9123370229680813\n",
      "MSE = 0.9126806129029922\n",
      "MSE = 0.9127527015862518\n",
      "MSE = 0.91278274303668\n",
      "MSE = 0.9127268988674452\n",
      "MSE = 0.9125540439804485\n",
      "MSE = 0.9121941486010062\n",
      "MSE = 0.9120490191380798\n",
      "MSE = 0.910502240452567\n",
      "MSE = 0.9112252702994845\n",
      "MSE = 0.9113584163560582\n",
      "MSE = 0.9113834968008082\n",
      "MSE = 0.9115153307187621\n",
      "MSE = 0.9114079364913481\n",
      "MSE = 0.9113937457398373\n",
      "MSE = 0.9107088203038248\n",
      "MSE = 0.9111626881225671\n",
      "MSE = 0.9111030859949576\n",
      "MSE = 0.9109798523629455\n",
      "MSE = 0.910863963689178\n",
      "MSE = 0.9107247883143924\n",
      "MSE = 0.9106752567519487\n",
      "MSE = 0.910684486576472\n",
      "MSE = 0.9107795560612325\n",
      "MSE = 0.9092641882955584\n",
      "MSE = 0.9106404688803602\n",
      "MSE = 0.924456710339881\n",
      "MSE = 0.9106676677477226\n",
      "MSE = 0.9106450072752674\n",
      "MSE = 0.910641390521492\n",
      "MSE = 0.9106406631730464\n",
      "MSE = 0.9106411431284686\n",
      "MSE = 0.9106407639513944\n",
      "MSE = 0.9106406844200632\n",
      "MSE = 0.9106406631730464\n",
      "Save best theta...\n",
      "1.1090961226343612\n",
      "Lambda = 1.1e-05\n",
      "MSE = 1.4807440185067458\n",
      "MSE = 2.2956793288501545\n",
      "MSE = 1.4730818784843316\n",
      "MSE = 1.4728344008593506\n",
      "MSE = 1.4724618493233683\n",
      "MSE = 1.4712093385610137\n",
      "MSE = 1.4682353394317251\n",
      "MSE = 1.4603676693980447\n",
      "MSE = 1.4413105572643756\n",
      "MSE = 1.398762204386926\n",
      "MSE = 1.3241336262779102\n",
      "MSE = 1.24194557574999\n",
      "MSE = 1.2010701901973446\n",
      "MSE = 1.1948588382937049\n",
      "MSE = 1.1948362336146152\n",
      "MSE = 1.1945507223391558\n",
      "MSE = 1.1913928174729072\n",
      "MSE = 1.1830380237439533\n",
      "MSE = 1.1619762729340581\n",
      "MSE = 1.1248396351648269\n",
      "MSE = 1.083769910382358\n",
      "MSE = 1.061766707534314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 105.76255610054432\n",
      "MSE = 1.0577691485372116\n",
      "MSE = 1.0557591618186075\n",
      "MSE = 1.054564757093724\n",
      "MSE = 1.0490476600905911\n",
      "MSE = 1.032607880589378\n",
      "MSE = 1.0103063798835161\n",
      "MSE = 0.9967412014877979\n",
      "MSE = 0.9920438917310186\n",
      "MSE = 0.9798102890461735\n",
      "MSE = 0.9780386440306521\n",
      "MSE = 0.9761513250214595\n",
      "MSE = 0.9717939651752765\n",
      "MSE = 0.9660331019888979\n",
      "MSE = 1.0037419506054432\n",
      "MSE = 0.964365127012074\n",
      "MSE = 0.9599806391830504\n",
      "MSE = 0.9586880240616912\n",
      "MSE = 0.9584415607629565\n",
      "MSE = 0.9582416613323613\n",
      "MSE = 0.9573910784172943\n",
      "MSE = 0.9554122866155547\n",
      "MSE = 0.9509967891634968\n",
      "MSE = 0.9514182772278326\n",
      "MSE = 0.9491705229540813\n",
      "MSE = 0.9386603597670584\n",
      "MSE = 0.9338792292111261\n",
      "MSE = 0.9302212901929066\n",
      "MSE = 0.930393711650286\n",
      "MSE = 0.9301633064231212\n",
      "MSE = 0.9298022666350748\n",
      "MSE = 0.9292453034277305\n",
      "MSE = 0.9284333816589587\n",
      "MSE = 0.9276407950392705\n",
      "MSE = 0.9264983334393158\n",
      "MSE = 0.9300959259844594\n",
      "MSE = 0.9263887570365507\n",
      "MSE = 0.9252138432932245\n",
      "MSE = 0.9246286609528219\n",
      "MSE = 0.9245158464850157\n",
      "MSE = 0.9248049573184416\n",
      "MSE = 0.9248359183307483\n",
      "MSE = 0.9248595281968319\n",
      "MSE = 0.9247706052946637\n",
      "MSE = 0.9245561879933091\n",
      "MSE = 0.9244539679484561\n",
      "MSE = 0.924395810303742\n",
      "MSE = 0.9247441155544693\n",
      "MSE = 0.924004425365928\n",
      "MSE = 0.9242197184821204\n",
      "MSE = 0.9237114094574137\n",
      "MSE = 0.9227869713022313\n",
      "MSE = 0.9205003831525698\n",
      "MSE = 0.9180746948352319\n",
      "MSE = 0.9177337946432579\n",
      "MSE = 0.9173490103078107\n",
      "MSE = 0.9166142961626218\n",
      "MSE = 0.9161691310108772\n",
      "MSE = 0.9151054014002395\n",
      "MSE = 0.9149484014899272\n",
      "MSE = 0.9147305435597115\n",
      "MSE = 0.9132902114441565\n",
      "MSE = 0.9127466279272176\n",
      "MSE = 0.9124037677503924\n",
      "MSE = 0.9124411708832422\n",
      "MSE = 0.9123075589728408\n",
      "MSE = 0.9123761908167118\n",
      "MSE = 0.9125564168977813\n",
      "MSE = 0.9123581521229798\n",
      "MSE = 0.9123152889547416\n",
      "MSE = 0.9123246142734878\n",
      "MSE = 0.9118223553358422\n",
      "MSE = 0.9121384321380204\n",
      "MSE = 0.9122023077494745\n",
      "MSE = 0.9122756256373789\n",
      "MSE = 0.9123704995985144\n",
      "MSE = 0.9126069593322202\n",
      "MSE = 0.9127033127346833\n",
      "MSE = 0.9125398302710087\n",
      "MSE = 0.9125252071635984\n",
      "MSE = 0.9124720550835296\n",
      "MSE = 0.912461072259898\n",
      "MSE = 0.9165873306617269\n",
      "MSE = 0.9124717408210358\n",
      "MSE = 0.9124670573970735\n",
      "MSE = 0.912885136339745\n",
      "MSE = 0.9124972930415416\n",
      "MSE = 0.9125260510549676\n",
      "MSE = 0.9125532224253063\n",
      "MSE = 0.9126139600000848\n",
      "MSE = 0.9126902886246221\n",
      "MSE = 0.9127788926315915\n",
      "MSE = 0.9133443225321584\n",
      "MSE = 0.9127290818503567\n",
      "MSE = 0.9127234201474269\n",
      "MSE = 0.9126579665482332\n",
      "MSE = 0.9126185305131527\n",
      "MSE = 0.9126044440114706\n",
      "MSE = 0.9126805025016387\n",
      "MSE = 0.9126149250265441\n",
      "MSE = 0.9125680396967811\n",
      "MSE = 0.9125441367230391\n",
      "MSE = 0.9125842462880623\n",
      "MSE = 0.9125365882061762\n",
      "MSE = 0.9125736050444825\n",
      "MSE = 0.9125856702818146\n",
      "MSE = 0.9124932677399483\n",
      "MSE = 0.9125609068021057\n",
      "MSE = 0.9125341447934835\n",
      "MSE = 0.9124630009308778\n",
      "MSE = 0.912405222783755\n",
      "MSE = 0.9130890505273084\n",
      "MSE = 0.912396770441809\n",
      "MSE = 0.912379847422912\n",
      "MSE = 0.9128282950882732\n",
      "MSE = 0.9123667343368214\n",
      "MSE = 0.9121383390570221\n",
      "MSE = 0.915446581402387\n",
      "MSE = 0.9120016772294481\n",
      "MSE = 0.9119024531454661\n",
      "MSE = 0.9118776556898593\n",
      "MSE = 0.9126727304846823\n",
      "MSE = 0.9118800535396681\n",
      "MSE = 0.911878203475551\n",
      "MSE = 0.9118611896061929\n",
      "MSE = 0.9118396225470583\n",
      "MSE = 0.9117862564633786\n",
      "MSE = 0.9116609257771312\n",
      "MSE = 0.9117757971115187\n",
      "MSE = 0.9117258580178369\n",
      "MSE = 0.9117071501629822\n",
      "MSE = 0.911665255022367\n",
      "MSE = 0.9116176822591406\n",
      "MSE = 0.9117773429356616\n",
      "MSE = 0.9116303513791115\n",
      "MSE = 0.9115163052450301\n",
      "MSE = 0.9119739851072425\n",
      "MSE = 0.9115186205276838\n",
      "MSE = 0.9115188884988952\n",
      "MSE = 0.9115232402492374\n",
      "MSE = 0.9115129960130279\n",
      "MSE = 0.9116022840466484\n",
      "MSE = 0.9115157315823991\n",
      "MSE = 0.911513244522949\n",
      "MSE = 0.9115130200455721\n",
      "MSE = 0.9115129983508234\n",
      "MSE = 0.9115129962405266\n",
      "MSE = 0.9115129960351873\n",
      "MSE = 0.9115129960151284\n",
      "MSE = 0.9115129960132137\n",
      "MSE = 0.911512996013044\n",
      "MSE = 0.9115129960130439\n",
      "MSE = 0.9115129960131194\n",
      "MSE = 0.911512996013163\n",
      "MSE = 0.911512996013119\n",
      "MSE = 0.911512996013119\n",
      "MSE = 0.9115129960131416\n",
      "MSE = 0.911512996013119\n",
      "Save best theta...\n",
      "1.108149717595015\n",
      "Lambda = 1.2e-05\n",
      "MSE = 1.4806717878325504\n",
      "MSE = 2.2963365972830085\n",
      "MSE = 1.4730812885979563\n",
      "MSE = 1.4728344007379446\n",
      "MSE = 1.472460321933876\n",
      "MSE = 1.4712070525866365\n",
      "MSE = 1.4682279633961781\n",
      "MSE = 1.4603503877575927\n",
      "MSE = 1.4412658688021125\n",
      "MSE = 1.3986580010107326\n",
      "MSE = 1.3239277157606004\n",
      "MSE = 1.2416648475147347\n",
      "MSE = 1.200899049769118\n",
      "MSE = 1.1948374910812427\n",
      "MSE = 1.194865158759622\n",
      "MSE = 1.1945994419289132\n",
      "MSE = 1.1915141455395715\n",
      "MSE = 1.1832443124117649\n",
      "MSE = 1.1622480001766324\n",
      "MSE = 1.1251437390627677\n",
      "MSE = 1.0840880224441165\n",
      "MSE = 1.0617888665872728\n",
      "MSE = 97.14709329790588\n",
      "MSE = 1.0578576097832963\n",
      "MSE = 1.0559388136756338\n",
      "MSE = 1.0547168959410818\n",
      "MSE = 1.0488330618261485\n",
      "MSE = 1.0318719753346333\n",
      "MSE = 1.00924304238483\n",
      "MSE = 1.0085425697938257\n",
      "MSE = 0.9924254806418432\n",
      "MSE = 0.9788660536621733\n",
      "MSE = 0.9761801143803309\n",
      "MSE = 0.9736228504741283\n",
      "MSE = 0.9685163391831549\n",
      "MSE = 0.9639837256161434\n",
      "MSE = 0.9598117478697615\n",
      "MSE = 0.9609415842447302\n",
      "MSE = 1.097793258087621\n",
      "MSE = 0.9597609243147859\n",
      "MSE = 0.9593282636353144\n",
      "MSE = 0.9588820014374291\n",
      "MSE = 0.9576903946257391\n",
      "MSE = 0.9551730742863171\n",
      "MSE = 0.9557044067110464\n",
      "MSE = 0.9528681176326527\n",
      "MSE = 0.948622607841108\n",
      "MSE = 0.9412927579786516\n",
      "MSE = 0.9390097400180029\n",
      "MSE = 0.9387649431456984\n",
      "MSE = 0.9385342827851368\n",
      "MSE = 1.0006261899404316\n",
      "MSE = 0.9382194698733134\n",
      "MSE = 0.9375499119539343\n",
      "MSE = 0.9368078717792608\n",
      "MSE = 0.9344075039887331\n",
      "MSE = 0.9319797477742446\n",
      "MSE = 0.9266250202935097\n",
      "MSE = 0.9251848914219544\n",
      "MSE = 0.9236164508935297\n",
      "MSE = 0.9221338219258393\n",
      "MSE = 0.9225776535853822\n",
      "MSE = 0.9216605528072004\n",
      "MSE = 0.926205688725519\n",
      "MSE = 0.9211503879929718\n",
      "MSE = 0.9185771256966722\n",
      "MSE = 0.9159852606063186\n",
      "MSE = 0.9162275220972874\n",
      "MSE = 0.9166656520943585\n",
      "MSE = 0.9169408417957771\n",
      "MSE = 0.9697506791621684\n",
      "MSE = 0.917262657182561\n",
      "MSE = 0.9174431984502235\n",
      "MSE = 0.9178005299294807\n",
      "MSE = 0.9180848663897613\n",
      "MSE = 0.9180054739423167\n",
      "MSE = 0.9179873225160371\n",
      "MSE = 0.9177020447481842\n",
      "MSE = 0.9174071699071691\n",
      "MSE = 0.9181371118565689\n",
      "MSE = 0.9179439451584298\n",
      "MSE = 0.9173273186459023\n",
      "MSE = 0.9172533973195648\n",
      "MSE = 0.9175223010722783\n",
      "MSE = 0.9176670733605775\n",
      "MSE = 0.9177906866700641\n",
      "MSE = 0.9178898151768065\n",
      "MSE = 0.9178794495378483\n",
      "MSE = 0.9177777844234299\n",
      "MSE = 0.9174208531536617\n",
      "MSE = 0.9167169810953929\n",
      "MSE = 0.9153807571068319\n",
      "MSE = 0.9151990747138011\n",
      "MSE = 0.9152862100563364\n",
      "MSE = 0.9149961802562819\n",
      "MSE = 0.9150467382295002\n",
      "MSE = 0.9149705734352643\n",
      "MSE = 0.9149690966810728\n",
      "MSE = 0.9148149080468744\n",
      "MSE = 0.9145139458336505\n",
      "MSE = 0.9138091786148758\n",
      "MSE = 0.9134355468440307\n",
      "MSE = 0.9133191126623085\n",
      "MSE = 0.9134348625123997\n",
      "MSE = 0.913544762643715\n",
      "MSE = 0.9136249919412125\n",
      "MSE = 0.9136979185853081\n",
      "MSE = 0.9136781758966241\n",
      "MSE = 0.9138849925774409\n",
      "MSE = 0.9136228521857126\n",
      "MSE = 0.9134179427973588\n",
      "MSE = 0.9131419041372563\n",
      "MSE = 0.9130165799815859\n",
      "MSE = 0.9123264913754427\n",
      "MSE = 0.9128668712197368\n",
      "MSE = 0.9129109496180645\n",
      "MSE = 0.912958400168416\n",
      "MSE = 0.9129763934978216\n",
      "MSE = 0.913635470397856\n",
      "MSE = 0.9130147665895267\n",
      "MSE = 0.912980574416805\n",
      "MSE = 0.9129768734382937\n",
      "MSE = 0.9129764489152784\n",
      "MSE = 0.9129763999010323\n",
      "MSE = 0.9129763942377185\n",
      "MSE = 0.9129763935832635\n",
      "MSE = 0.9129763935076658\n",
      "MSE = 0.9129763934989197\n",
      "MSE = 0.9129763934979532\n",
      "MSE = 0.9129763934986038\n",
      "MSE = 0.9129763934980109\n",
      "MSE = 0.9129763934980285\n",
      "MSE = 0.9129763934983255\n",
      "MSE = 0.9129763934980436\n",
      "MSE = 0.9129763934980285\n",
      "Save best theta...\n",
      "1.1080827441140344\n",
      "Lambda = 1.3e-05\n",
      "MSE = 1.4807937637413722\n",
      "MSE = 2.2952278700132305\n",
      "MSE = 1.4730822831382167\n",
      "MSE = 1.472834401212684\n",
      "MSE = 1.4724628880577706\n",
      "MSE = 1.4712108805485662\n",
      "MSE = 1.4682402505964167\n",
      "MSE = 1.4603783639055254\n",
      "MSE = 1.4413325772482235\n",
      "MSE = 1.3987806358178865\n",
      "MSE = 1.324059572732017\n",
      "MSE = 1.2416659440374969\n",
      "MSE = 1.2008406223489712\n",
      "MSE = 1.1948621334067908\n",
      "MSE = 1.194937481265959\n",
      "MSE = 1.1946942242634584\n",
      "MSE = 1.191700000251437\n",
      "MSE = 1.183542506736451\n",
      "MSE = 1.1626779062719534\n",
      "MSE = 1.125657134899073\n",
      "MSE = 1.0845706128297827\n",
      "MSE = 1.0619161218328834\n",
      "MSE = 92.37831165698968\n",
      "MSE = 1.058026482141417\n",
      "MSE = 1.056171195049876\n",
      "MSE = 1.0549449118967538\n",
      "MSE = 1.048872971813027\n",
      "MSE = 1.0316511382671973\n",
      "MSE = 1.0088713202976396\n",
      "MSE = 1.0069249102023914\n",
      "MSE = 0.991661953458907\n",
      "MSE = 0.9780958003348075\n",
      "MSE = 0.9751054390044274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.9722110003889675\n",
      "MSE = 0.9671457407134592\n",
      "MSE = 0.9642703689137863\n",
      "MSE = 0.9610877437639804\n",
      "MSE = 0.9610958128075426\n",
      "MSE = 0.960027747528872\n",
      "MSE = 0.9593527164182097\n",
      "MSE = 0.9565655094002392\n",
      "MSE = 0.9537735811777921\n",
      "MSE = 0.9493619296206965\n",
      "MSE = 1.2670741596021977\n",
      "MSE = 0.9477755284013457\n",
      "MSE = 0.9416206926697637\n",
      "MSE = 0.9389399216626872\n",
      "MSE = 0.938794329713487\n",
      "MSE = 0.939319613546046\n",
      "MSE = 0.9403628508264337\n",
      "MSE = 0.9429574735116717\n",
      "MSE = 0.9395376891873632\n",
      "MSE = 0.9410157838582244\n",
      "MSE = 0.9408671864971365\n",
      "MSE = 0.9392948141157442\n",
      "MSE = 0.9368337984415894\n",
      "MSE = 0.9300417127611366\n",
      "MSE = 1.447120018969379\n",
      "MSE = 0.9309884307709236\n",
      "MSE = 0.928438645886971\n",
      "MSE = 0.9419943268733131\n",
      "MSE = 0.927690974111649\n",
      "MSE = 0.9273925992583073\n",
      "MSE = 0.9254843580967936\n",
      "MSE = 0.9925825870999461\n",
      "MSE = 0.9252220470579163\n",
      "MSE = 0.9228147837862536\n",
      "MSE = 0.9209517471557699\n",
      "MSE = 0.9213260979954097\n",
      "MSE = 0.9216204773431506\n",
      "MSE = 0.9214485146709392\n",
      "MSE = 0.9206021537370788\n",
      "MSE = 0.9205877809178634\n",
      "MSE = 0.9209459256033166\n",
      "MSE = 0.9203874566480819\n",
      "MSE = 0.9195237752658473\n",
      "MSE = 0.9187878353848375\n",
      "MSE = 0.918493000780643\n",
      "MSE = 0.9180988087005917\n",
      "MSE = 0.9190303999112024\n",
      "MSE = 0.9180183152838731\n",
      "MSE = 0.9178635193384661\n",
      "MSE = 0.917697629905149\n",
      "MSE = 0.9181703704392343\n",
      "MSE = 0.9185590268125216\n",
      "MSE = 0.9185881364668251\n",
      "MSE = 1.1220273674983154\n",
      "MSE = 0.918453530764505\n",
      "MSE = 0.9182412303985272\n",
      "MSE = 0.917773080065367\n",
      "MSE = 0.9180271773226891\n",
      "MSE = 0.9179176049988165\n",
      "MSE = 0.9176197740978976\n",
      "MSE = 1.0367311104028307\n",
      "MSE = 0.9175936035454945\n",
      "MSE = 0.9170782378580308\n",
      "MSE = 0.9165125910569217\n",
      "MSE = 0.9167432824780253\n",
      "MSE = 0.916920794956287\n",
      "MSE = 0.9169028405649932\n",
      "MSE = 0.9170766988465517\n",
      "MSE = 0.9167685625638423\n",
      "MSE = 0.9164966724537522\n",
      "MSE = 0.9155339755633609\n",
      "MSE = 0.9159866899932346\n",
      "MSE = 0.9159201847769822\n",
      "MSE = 0.9157531298639023\n",
      "MSE = 0.9157107118266624\n",
      "MSE = 0.9157286353050184\n",
      "MSE = 0.9156249197597082\n",
      "MSE = 0.9156176641967737\n",
      "MSE = 0.915668547688609\n",
      "MSE = 0.9156956381403084\n",
      "MSE = 0.9156737972246725\n",
      "MSE = 0.9155191697584568\n",
      "MSE = 0.936017309517987\n",
      "MSE = 0.9155512320890794\n",
      "MSE = 0.9155272392938367\n",
      "MSE = 0.9153707823442349\n",
      "MSE = 0.9151160945776416\n",
      "MSE = 0.9150344328579513\n",
      "MSE = 0.9149989307019962\n",
      "MSE = 0.9150188391858126\n",
      "MSE = 0.9150093048870641\n",
      "MSE = 0.9149840549515624\n",
      "MSE = 0.9149520008877187\n",
      "MSE = 0.9148729592291134\n",
      "MSE = 0.916181314304414\n",
      "MSE = 0.9148806110510459\n",
      "MSE = 0.9147857016478709\n",
      "MSE = 0.9148726504385661\n",
      "MSE = 0.9147866885686649\n",
      "MSE = 0.914763447051613\n",
      "MSE = 0.9135136861518427\n",
      "MSE = 0.9146935393875758\n",
      "MSE = 0.9146459698050976\n",
      "MSE = 0.9146871265498329\n",
      "MSE = 0.9146503812904035\n",
      "MSE = 0.9146464844016103\n",
      "MSE = 0.914646030406024\n",
      "MSE = 0.9146459769496578\n",
      "MSE = 0.9146459706474666\n",
      "MSE = 0.9146459699043956\n",
      "MSE = 0.9146459698167625\n",
      "MSE = 0.9146459698064999\n",
      "MSE = 0.9146459698052046\n",
      "MSE = 0.9146459698050995\n",
      "MSE = 0.914645969805099\n",
      "MSE = 0.9146459698051549\n",
      "MSE = 0.9146459698050994\n",
      "MSE = 0.9146459698050994\n",
      "MSE = 0.9146459698051346\n",
      "MSE = 0.9146459698051\n",
      "MSE = 0.9146459698050994\n",
      "Save best theta...\n",
      "1.107547823541491\n",
      "Lambda = 1.4e-05\n",
      "MSE = 1.4803735889925644\n",
      "MSE = 2.2990720082122715\n",
      "MSE = 1.4730788228861516\n",
      "MSE = 1.4728343960934847\n",
      "MSE = 1.4724536927343908\n",
      "MSE = 1.4711968463490126\n",
      "MSE = 1.4681952744171087\n",
      "MSE = 1.4602740834105767\n",
      "MSE = 1.4410739444771938\n",
      "MSE = 1.3982408023999202\n",
      "MSE = 1.3232089179256612\n",
      "MSE = 1.2408585532650487\n",
      "MSE = 1.2004722842842952\n",
      "MSE = 1.1947644181702597\n",
      "MSE = 1.1948894006698432\n",
      "MSE = 1.1946597579982408\n",
      "MSE = 1.191703145102898\n",
      "MSE = 1.183576593583468\n",
      "MSE = 1.1626543136249705\n",
      "MSE = 1.1255730580415042\n",
      "MSE = 1.0846296264815865\n",
      "MSE = 1.0617920310086146\n",
      "MSE = 75.53007727587615\n",
      "MSE = 1.0580224107011877\n",
      "MSE = 1.0562705153006196\n",
      "MSE = 1.0549162510134489\n",
      "MSE = 1.0478132166386966\n",
      "MSE = 1.0292982504343202\n",
      "MSE = 1.0057785781257196\n",
      "MSE = 0.9797343603931729\n",
      "MSE = 0.970248341633178\n",
      "MSE = 0.9681920279908567\n",
      "MSE = 0.9642557970614564\n",
      "MSE = 0.956945409858745\n",
      "MSE = 0.9417699170978526\n",
      "MSE = 8.72175784819331\n",
      "MSE = 0.9422713747922473\n",
      "MSE = 0.9423807870806808\n",
      "MSE = 0.9421379948304128\n",
      "MSE = 0.9415375978684463\n",
      "MSE = 0.9393712495060087\n",
      "MSE = 0.9360268448134996\n",
      "MSE = 0.9850554164227348\n",
      "MSE = 0.9357962965204357\n",
      "MSE = 0.9325532226543561\n",
      "MSE = 0.9304421841198096\n",
      "MSE = 0.9267496413200499\n",
      "MSE = 0.9257620393062024\n",
      "MSE = 0.9329287062380979\n",
      "MSE = 0.9260432537154402\n",
      "MSE = 0.9262585199904811\n",
      "MSE = 0.9263656828091422\n",
      "MSE = 0.9267568166815185\n",
      "MSE = 0.9268244599779917\n",
      "MSE = 0.9401962289840552\n",
      "MSE = 0.9265052837676275\n",
      "MSE = 0.9261390020487564\n",
      "MSE = 0.9261460707386383\n",
      "MSE = 0.9258652216226008\n",
      "MSE = 0.9255213892587904\n",
      "MSE = 0.9255624593137002\n",
      "MSE = 0.9250827605280746\n",
      "MSE = 0.9250442300401921\n",
      "MSE = 0.9246361519832335\n",
      "MSE = 0.9238572783219116\n",
      "MSE = 0.9232183110646067\n",
      "MSE = 0.9223431762199433\n",
      "MSE = 0.9213903572982239\n",
      "MSE = 0.9217356455776032\n",
      "MSE = 0.9212268991306595\n",
      "MSE = 0.9206694550998531\n",
      "MSE = 0.9208531482691681\n",
      "MSE = 0.9206494026634519\n",
      "MSE = 0.9207201653140613\n",
      "MSE = 0.9202019737523655\n",
      "MSE = 0.9194869324506902\n",
      "MSE = 0.9180807308006866\n",
      "MSE = 0.9179934328064077\n",
      "MSE = 0.9178255690492014\n",
      "MSE = 0.9174146938093111\n",
      "MSE = 0.9167900669441352\n",
      "MSE = 0.9163600824074808\n",
      "MSE = 0.9161457971541964\n",
      "MSE = 0.9162548964500248\n",
      "MSE = 0.9164134767348686\n",
      "MSE = 0.9236161783303398\n",
      "MSE = 0.9164839788480291\n",
      "MSE = 0.9164304573211923\n",
      "MSE = 0.9166562242492879\n",
      "MSE = 0.9164851180661726\n",
      "MSE = 0.9167496565035906\n",
      "MSE = 0.9165531508544756\n",
      "MSE = 0.9168136364935017\n",
      "MSE = 0.9166217125890063\n",
      "MSE = 0.916890789944894\n",
      "MSE = 0.9166902726875208\n",
      "MSE = 0.917233680351322\n",
      "MSE = 0.916633472920085\n",
      "MSE = 0.9168521464546486\n",
      "MSE = 0.9170324314068814\n",
      "MSE = 0.9168521469131073\n",
      "MSE = 0.9160214876563273\n",
      "MSE = 0.9165543018250348\n",
      "MSE = 0.916510657636832\n",
      "MSE = 0.9162990600629606\n",
      "MSE = 0.9181147208818456\n",
      "MSE = 0.9162968965689119\n",
      "MSE = 0.9163006035821245\n",
      "MSE = 0.9163593208371958\n",
      "MSE = 0.9164364637907227\n",
      "MSE = 0.9163760654591351\n",
      "MSE = 0.9164865175703196\n",
      "MSE = 0.9168072903029079\n",
      "MSE = 0.9165075919007035\n",
      "MSE = 0.9164900959646294\n",
      "MSE = 0.9164871971593006\n",
      "MSE = 0.9164866493008992\n",
      "MSE = 0.9164865432054998\n",
      "MSE = 0.9164865225627111\n",
      "MSE = 0.9164865185426267\n",
      "MSE = 0.9164865177596249\n",
      "MSE = 0.9164865176071605\n",
      "MSE = 0.916486517577504\n",
      "MSE = 0.9164865175717283\n",
      "MSE = 0.9164865175705051\n",
      "MSE = 0.916486517570326\n",
      "MSE = 0.9164865175703267\n",
      "MSE = 0.9164865175704131\n",
      "MSE = 0.9164865175703296\n",
      "MSE = 0.91648651757033\n",
      "MSE = 0.9164865175703664\n",
      "MSE = 0.9164865175703937\n",
      "MSE = 0.9164864008595516\n",
      "MSE = 0.9164860349749002\n",
      "MSE = 0.9164860094604648\n",
      "MSE = 0.9164859977465801\n",
      "MSE = 0.9164859740827667\n",
      "MSE = 0.9164867968404093\n",
      "MSE = 0.9164895922859277\n",
      "MSE = 0.916496894118552\n",
      "MSE = 0.9165058098234357\n",
      "MSE = 0.916509991813888\n",
      "MSE = 0.9165085064970713\n",
      "MSE = 0.9165072790719233\n",
      "MSE = 0.9165056754941173\n",
      "MSE = 0.9165037353269399\n",
      "MSE = 0.9165035521011975\n",
      "MSE = 0.9165104025878664\n",
      "MSE = 0.9165332237631225\n",
      "MSE = 0.9165645137290336\n",
      "MSE = 0.9165913071210224\n",
      "MSE = 0.916588773981119\n",
      "MSE = 0.9165860533406623\n",
      "MSE = 0.9165847058454172\n",
      "MSE = 0.9165867702223899\n",
      "MSE = 0.9165517856331803\n",
      "MSE = 0.916568200226791\n",
      "MSE = 0.9165936872376009\n",
      "MSE = 0.9166636094215027\n",
      "MSE = 0.9167156378596752\n",
      "MSE = 0.9167422632115918\n",
      "MSE = 0.916782533693856\n",
      "MSE = 0.916859823513201\n",
      "MSE = 0.934026247571676\n",
      "MSE = 0.9168632251212685\n",
      "MSE = 0.9168603360004349\n",
      "MSE = 0.9168599160321654\n",
      "MSE = 0.916859840733767\n",
      "MSE = 0.9168598267365244\n",
      "MSE = 0.916859824117203\n",
      "MSE = 0.9168598236263855\n",
      "MSE = 0.9168598235343585\n",
      "MSE = 0.9168598235172208\n",
      "MSE = 0.9168598235139614\n",
      "MSE = 0.9168598235133402\n",
      "MSE = 0.9168598235132104\n",
      "MSE = 0.9168598235132001\n",
      "MSE = 0.9168598235132001\n",
      "MSE = 0.9168598235132036\n",
      "MSE = 0.9168598235132035\n",
      "MSE = 0.9168598235132038\n",
      "MSE = 0.9168598235132035\n",
      "Save best theta...\n",
      "1.1070277944583267\n",
      "Lambda = 1.5e-05\n",
      "MSE = 1.4803792757977186\n",
      "MSE = 2.2990195118944454\n",
      "MSE = 1.473078870388795\n",
      "MSE = 1.4728343964213397\n",
      "MSE = 1.4724538256085529\n",
      "MSE = 1.4711970558655638\n",
      "MSE = 1.468195913607254\n",
      "MSE = 1.4602752694952814\n",
      "MSE = 1.441074679466005\n",
      "MSE = 1.3982291675835261\n",
      "MSE = 1.3231426460363018\n",
      "MSE = 1.2407017230024702\n",
      "MSE = 1.2003612951583957\n",
      "MSE = 1.194772565956408\n",
      "MSE = 1.1949434006104411\n",
      "MSE = 1.1947340202562329\n",
      "MSE = 1.19185705504107\n",
      "MSE = 1.1838248010002743\n",
      "MSE = 1.1629954062337364\n",
      "MSE = 1.1259687110421481\n",
      "MSE = 1.0850486009017077\n",
      "MSE = 1.0618977646178267\n",
      "MSE = 69.39055330497926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.058185690058122\n",
      "MSE = 1.0564893970431979\n",
      "MSE = 1.0550816888301067\n",
      "MSE = 1.0474779574781796\n",
      "MSE = 1.0283836615326876\n",
      "MSE = 1.0045501501976897\n",
      "MSE = 0.9821259962735708\n",
      "MSE = 0.9829830352545926\n",
      "MSE = 0.9747100159312461\n",
      "MSE = 0.9674659639191807\n",
      "MSE = 0.9533492574694037\n",
      "MSE = 0.9459666687428229\n",
      "MSE = 0.9442386297937894\n",
      "MSE = 0.9448156403040556\n",
      "MSE = 0.9438206995215884\n",
      "MSE = 0.942508593401262\n",
      "MSE = 0.9419405177149218\n",
      "MSE = 0.9386062703805149\n",
      "MSE = 0.9382362790943142\n",
      "MSE = 0.9335537614279774\n",
      "MSE = 0.9324818823027092\n",
      "MSE = 0.9320807624004755\n",
      "MSE = 0.9320823838449707\n",
      "MSE = 0.9317012903233746\n",
      "MSE = 0.9314576256290085\n",
      "MSE = 0.9297290863752482\n",
      "MSE = 1.5397941261367567\n",
      "MSE = 0.9296228230996194\n",
      "MSE = 0.928284077258966\n",
      "MSE = 0.9271570060255965\n",
      "MSE = 0.9267759308622486\n",
      "MSE = 0.9264788073300421\n",
      "MSE = 0.9254864905233111\n",
      "MSE = 0.9231348540886759\n",
      "MSE = 0.9235986313582699\n",
      "MSE = 0.9227448003310729\n",
      "MSE = 0.921542685345424\n",
      "MSE = 0.920624061057265\n",
      "MSE = 0.9225892240688237\n",
      "MSE = 0.9212541347340449\n",
      "MSE = 0.9458654237031073\n",
      "MSE = 0.9212211013718905\n",
      "MSE = 0.9213058493831813\n",
      "MSE = 0.9210261063951904\n",
      "MSE = 0.9284035477349496\n",
      "MSE = 0.9212042041589997\n",
      "MSE = 0.9206263076679188\n",
      "MSE = 0.9198307739620241\n",
      "MSE = 0.9194466629644198\n",
      "MSE = 0.9190738215553748\n",
      "MSE = 0.9187983774638664\n",
      "MSE = 0.9187850273944479\n",
      "MSE = 0.9190785415495163\n",
      "MSE = 0.9190330547838838\n",
      "MSE = 0.919046958479013\n",
      "MSE = 0.9190536131094091\n",
      "MSE = 0.9190265662086626\n",
      "MSE = 0.9190067305240579\n",
      "MSE = 0.9190688000240554\n",
      "MSE = 0.9195125509024272\n",
      "MSE = 0.9194933723290155\n",
      "MSE = 0.9194545773132314\n",
      "MSE = 0.9193796481983589\n",
      "MSE = 0.9193401552862226\n",
      "MSE = 0.9193404941232985\n",
      "MSE = 0.9193419871788596\n",
      "MSE = 0.9193250628799331\n",
      "MSE = 0.9193777272446143\n",
      "MSE = 0.9193833851252654\n",
      "MSE = 0.9194303297657221\n",
      "MSE = 0.9194934095999235\n",
      "MSE = 0.9195768378990637\n",
      "MSE = 0.9195236478562524\n",
      "MSE = 0.9194284481501295\n",
      "MSE = 0.9192452477965581\n",
      "MSE = 0.9192212322833593\n",
      "MSE = 0.9192736310153958\n",
      "MSE = 0.9401192355995538\n",
      "MSE = 0.9192603216404267\n",
      "MSE = 0.9194344804032185\n",
      "MSE = 0.9193157712005325\n",
      "MSE = 0.9195367897270237\n",
      "MSE = 0.919373059394161\n",
      "MSE = 0.9194585842795324\n",
      "MSE = 0.9193916666443906\n",
      "MSE = 0.9194531819724585\n",
      "MSE = 0.9194074951286737\n",
      "MSE = 0.9194610866925481\n",
      "MSE = 0.9194196973742398\n",
      "MSE = 0.9194655360576924\n",
      "MSE = 0.9194310224902581\n",
      "MSE = 0.9194741277445251\n",
      "MSE = 0.9194431135093406\n",
      "MSE = 0.9194848392029881\n",
      "MSE = 0.9194960378716259\n",
      "MSE = 0.919503020562646\n",
      "MSE = 0.9194746247698005\n",
      "MSE = 0.9200993620571118\n",
      "MSE = 0.9194373696397995\n",
      "MSE = 0.919335517185233\n",
      "MSE = 0.9192229297621357\n",
      "MSE = 0.9191786259141016\n",
      "MSE = 0.9191627299530525\n",
      "MSE = 0.9190989502674592\n",
      "MSE = 0.9191088119847554\n",
      "MSE = 0.9191033268193145\n",
      "MSE = 0.9190458605688123\n",
      "MSE = 0.9189200359284112\n",
      "MSE = 0.9185964184220421\n",
      "MSE = 0.9186205023467037\n",
      "MSE = 0.9185629177323296\n",
      "MSE = 0.9185408927813279\n",
      "MSE = 0.9185211790089751\n",
      "MSE = 0.9185128582482196\n",
      "MSE = 0.9185086235100637\n",
      "MSE = 0.9185003549382809\n",
      "MSE = 0.9185000087992534\n",
      "MSE = 0.9184079934580951\n",
      "MSE = 0.9183867413222454\n",
      "MSE = 0.9183551914591119\n",
      "MSE = 0.918331033738688\n",
      "MSE = 0.9196799695486106\n",
      "MSE = 0.9183550144479639\n",
      "MSE = 0.9183323131309341\n",
      "MSE = 0.9183311045365875\n",
      "MSE = 0.9183310376642476\n",
      "MSE = 0.9183310339563676\n",
      "MSE = 0.91833103375077\n",
      "MSE = 0.9183310337393371\n",
      "MSE = 0.9183310337387341\n",
      "MSE = 0.9183310337386882\n",
      "MSE = 0.9183310337386881\n",
      "MSE = 0.918331033738707\n",
      "MSE = 0.918331033738709\n",
      "MSE = 0.918331033738707\n",
      "Save best theta...\n",
      "1.1068034364209327\n",
      "Lambda = 5e-05\n",
      "MSE = 1.4802824707145839\n",
      "MSE = 2.299914896697464\n",
      "MSE = 1.4730780601060838\n",
      "MSE = 1.4728343994753774\n",
      "MSE = 1.4724516050169374\n",
      "MSE = 1.47119356051715\n",
      "MSE = 1.4681838129621934\n",
      "MSE = 1.4602367190539822\n",
      "MSE = 1.4409070915055824\n",
      "MSE = 1.3974814196381677\n",
      "MSE = 1.3208078706910993\n",
      "MSE = 1.236699610842922\n",
      "MSE = 1.198965327639261\n",
      "MSE = 1.1969562186786489\n",
      "MSE = 1.198209643569736\n",
      "MSE = 1.1986448896577149\n",
      "MSE = 1.1981293029593758\n",
      "MSE = 1.1928211161103635\n",
      "MSE = 1.1749332463952784\n",
      "MSE = 1.13993851951721\n",
      "MSE = 1.1119530173363157\n",
      "MSE = 1.0834415306524285\n",
      "MSE = 1.0747642403510447\n",
      "MSE = 1.0718156692762457\n",
      "MSE = 1.06002823648609\n",
      "MSE = 1.0441778880964154\n",
      "MSE = 1.0218065430012995\n",
      "MSE = 1.0263689104723694\n",
      "MSE = 1.0222354274838268\n",
      "MSE = 1.0216800698412036\n",
      "MSE = 1.024839556993888\n",
      "MSE = 1.024367309660669\n",
      "MSE = 1.0427931800705859\n",
      "MSE = 1.0271711618351094\n",
      "MSE = 1.0101479939948124\n",
      "MSE = 1.00763266577671\n",
      "MSE = 1.0052345693909768\n",
      "MSE = 1.00428742803341\n",
      "MSE = 1.0006000952539151\n",
      "MSE = 0.9942903915762423\n",
      "MSE = 0.991026676345947\n",
      "MSE = 0.9881819807305691\n",
      "MSE = 0.9910392497377389\n",
      "MSE = 0.9889245682159911\n",
      "MSE = 0.9873270399705456\n",
      "MSE = 0.9870687534518607\n",
      "MSE = 0.9864789936781436\n",
      "MSE = 0.9849970763628823\n",
      "MSE = 0.9859803265772715\n",
      "MSE = 0.9860993256264071\n",
      "MSE = 0.9867934269167316\n",
      "MSE = 0.9862240797848207\n",
      "MSE = 0.9864206368403752\n",
      "MSE = 0.9866673053616853\n",
      "MSE = 0.9874117000619355\n",
      "MSE = 0.987158419848364\n",
      "MSE = 1.0236651469692386\n",
      "MSE = 0.9868062075674499\n",
      "MSE = 0.9859764053694838\n",
      "MSE = 0.985288857587862\n",
      "MSE = 0.9850678118358515\n",
      "MSE = 0.985197287841846\n",
      "MSE = 0.985075564871056\n",
      "MSE = 0.9850684376132034\n",
      "MSE = 0.9850678634197452\n",
      "MSE = 0.9850678160953057\n",
      "MSE = 0.9850678121876636\n",
      "MSE = 0.9850678118649192\n",
      "MSE = 0.9850678118382994\n",
      "MSE = 0.9850678118360766\n",
      "MSE = 0.9850678118375361\n",
      "MSE = 0.9850678118362686\n",
      "MSE = 0.985067811837122\n",
      "MSE = 0.9850678118363425\n",
      "MSE = 0.9850678118362679\n",
      "MSE = 0.9850678118362679\n",
      "MSE = 0.9850678118363059\n",
      "MSE = 0.9850678118362679\n",
      "1.1325100917364426\n",
      "Lambda = 0.0001\n",
      "MSE = 1.477797106838764\n",
      "MSE = 2.3240544597224626\n",
      "MSE = 1.473055213778418\n",
      "MSE = 1.4728339238564936\n",
      "MSE = 1.472361705226112\n",
      "MSE = 1.4710230324585725\n",
      "MSE = 1.4676561521451856\n",
      "MSE = 1.4589487738594633\n",
      "MSE = 1.437648840277547\n",
      "MSE = 1.390380283285032\n",
      "MSE = 1.3103207270001491\n",
      "MSE = 1.1809964166882734\n",
      "MSE = 1.120923608404516\n",
      "MSE = 1.1223378519543272\n",
      "MSE = 1.1240610630144485\n",
      "MSE = 1.1223797734684757\n",
      "MSE = 1.1047050274951762\n",
      "MSE = 1.0944647809200128\n",
      "MSE = 1.1076093355213406\n",
      "MSE = 1.0990621834631544\n",
      "MSE = 1.0826704716479782\n",
      "MSE = 1.0841308760353574\n",
      "MSE = 1.0861270719566067\n",
      "MSE = 1.076800699570924\n",
      "MSE = 1.0669130875496828\n",
      "MSE = 1.0723268254359013\n",
      "MSE = 1.068491281905639\n",
      "MSE = 1.0680566366977697\n",
      "MSE = 1.0650792551121222\n",
      "MSE = 1.0633877857269805\n",
      "MSE = 1.063516091552632\n",
      "MSE = 1.0650123085897825\n",
      "MSE = 1.0648015643387394\n",
      "MSE = 1.0676540965104178\n",
      "MSE = 1.0655896086298153\n",
      "MSE = 1.0664286812463621\n",
      "MSE = 1.0661684314508604\n",
      "MSE = 1.063858998589921\n",
      "MSE = 1.0632241024405267\n",
      "MSE = 1.06353847699694\n",
      "MSE = 1.0636645572768024\n",
      "MSE = 1.063556748718016\n",
      "MSE = 1.0635415286665861\n",
      "MSE = 1.0635389982450678\n",
      "MSE = 1.063538566369394\n",
      "MSE = 1.063538492330706\n",
      "MSE = 1.0635384796282183\n",
      "MSE = 1.0635384774486836\n",
      "MSE = 1.063538477074519\n",
      "MSE = 1.0635384770104246\n",
      "MSE = 1.063538476999392\n",
      "MSE = 1.0635384770066605\n",
      "MSE = 1.063538477000725\n",
      "MSE = 1.0635384769995535\n",
      "MSE = 1.063538476999392\n",
      "1.182853662863315\n"
     ]
    }
   ],
   "source": [
    "lamb_values = [8e-6,1e-5,1.25e-5,1.5e-5,1.6e-5,1.7e-5,1.8e-5,2e-5,5e-5]\n",
    "MSE_valid =[]\n",
    "best_theta = []\n",
    "best_MSE = 0\n",
    "\n",
    "for lamb in lamb_values:\n",
    "    print(\"Lambda = {}\".format(lamb))\n",
    "  \n",
    "    theta_init = [alpha] + [0.0]*(nUsers+nItems)\n",
    "    unpack(theta_init)\n",
    "    \n",
    "    def cost(theta, labels, lamb):\n",
    "        unpack(theta)\n",
    "        predictions = [prediction(user, book) for user,book in Xtrain]\n",
    "        cost = MSE(predictions, labels)\n",
    "        print(\"MSE = \" + str(cost))\n",
    "        for u in userBiases:\n",
    "            cost += lamb*userBiases[u]**2\n",
    "        for i in itemBiases:\n",
    "            cost += lamb*itemBiases[i]**2\n",
    "        return cost\n",
    "\n",
    "    def derivative(theta, labels, lamb):\n",
    "        unpack(theta)\n",
    "        N = len(dataset)\n",
    "        dalpha = 0\n",
    "        dUserBiases = defaultdict(float)\n",
    "        dItemBiases = defaultdict(float)\n",
    "        for value in zip(Xtrain,ytrain):\n",
    "            x,rating = value\n",
    "            user = x[0]\n",
    "            book = x[1]\n",
    "            pred = prediction(user,book)\n",
    "            diff = pred - rating\n",
    "            dalpha += 2/N*diff\n",
    "            dUserBiases[user] += 2/N*diff\n",
    "            dItemBiases[book] += 2/N*diff\n",
    "        for u in userBiases:\n",
    "            dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "        for i in itemBiases:\n",
    "            dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "        dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "        return numpy.array(dtheta)\n",
    "    \n",
    "    theta,_,_ = scipy.optimize.fmin_l_bfgs_b(cost, theta_init, derivative, args = (ytrain, lamb))\n",
    "\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(user, book) for user,book in Xvalid]\n",
    "    cost = MSE(predictions, yvalid)\n",
    "    \n",
    "    if best_MSE is 0:\n",
    "        best_MSE = cost\n",
    "        best_theta = theta\n",
    "        print(\"Save best theta...\")\n",
    "    else:\n",
    "        if cost < best_MSE:\n",
    "            best_MSE = cost\n",
    "            best_theta = theta\n",
    "            print(\"Save best theta...\")\n",
    "    \n",
    "    MSE_valid.append(cost)\n",
    "    print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1108039952167454,\n",
       " 1.1090961226343612,\n",
       " 1.108149717595015,\n",
       " 1.1080827441140344,\n",
       " 1.107547823541491,\n",
       " 1.1070277944583267,\n",
       " 1.1068034364209327,\n",
       " 1.1325100917364426,\n",
       " 1.182853662863315]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (11) Kaggle Submission - Lambda=1.2e-5,  MSE=1.143, User_Name='Luke Liem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff2bf6d2b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.xlabel('Lambda')\n",
    "plt.title('Validation MSE')\n",
    "plt.xscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.plot(lamb_values, MSE_valid, label='Validation MSE')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.81232674,  0.65968155,  0.19523218, ..., -0.23033841,\n",
       "        0.0180167 , -0.04204099])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rating baseline: compute averages for each user, or return \n",
    "### the global average if we've never seen the user before\n",
    "\n",
    "predictions = open(\"assignment1/predictions_Rating.txt\", 'w')\n",
    "unpack(best_theta)\n",
    "\n",
    "for l in open(\"assignment1/pairs_Rating.txt\"):\n",
    "  # write header  \n",
    "  if l.startswith(\"userID\"):\n",
    "    predictions.write(l)\n",
    "    continue\n",
    "    \n",
    "  # write user-item-rating\n",
    "  u,b = l.strip().split('-')\n",
    "  predictions.write(u + '-' + b + ',' + str(prediction(u,b)) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ratingMean\n",
    "\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n",
    "\n",
    "userGamma = {}\n",
    "itemGamma = {}\n",
    "\n",
    "K = 2\n",
    "\n",
    "# Initialize user and item Gammas\n",
    "for u in ratingsPerUser:\n",
    "    userGamma[u] = [random.random() * 0.1 - 0.05 for k in range(K)]\n",
    "for i in ratingsPerItem:\n",
    "    itemGamma[i] = [random.random() * 0.1 - 0.05 for k in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    global userGamma\n",
    "    global itemGamma\n",
    "    index = 0\n",
    "    alpha = theta[index]\n",
    "    index += 1\n",
    "    userBiases = dict(zip(users, theta[index:index+nUsers]))\n",
    "    index += nUsers\n",
    "    itemBiases = dict(zip(items, theta[index:index+nItems]))\n",
    "    index += nItems\n",
    "    for u in users:\n",
    "        userGamma[u] = theta[index:index+K]\n",
    "        index += K\n",
    "    for i in items:\n",
    "        itemGamma[i] = theta[index:index+K]\n",
    "        index += K\n",
    "        \n",
    "def inner(x, y):\n",
    "    return sum([a*b for a,b in zip(x,y)])\n",
    "\n",
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item] + inner(userGamma[user], itemGamma[item])\n",
    "\n",
    "   \n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(user, book) for user,book in Xtrain]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in users:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "        for k in range(K):\n",
    "            cost += lamb*userGamma[u][k]**2\n",
    "    for i in items:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "        for k in range(K):\n",
    "            cost += lamb*itemGamma[i][k]**2\n",
    "    return cost\n",
    "\n",
    "\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(dataset)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    dUserGamma = {}\n",
    "    dItemGamma = {}\n",
    "    for u in ratingsPerUser:\n",
    "        dUserGamma[u] = [0.0 for k in range(K)]\n",
    "    for i in ratingsPerItem:\n",
    "        dItemGamma[i] = [0.0 for k in range(K)]\n",
    "    for value in zip(Xtrain,ytrain):\n",
    "        x,rating = value\n",
    "        u = x[0]\n",
    "        i = x[1]\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - rating\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[i] += 2/N*diff\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2/N*itemGamma[i][k]*diff\n",
    "            dItemGamma[i][k] += 2/N*userGamma[u][k]*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2*lamb*userGamma[u][k]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "        for k in range(K):\n",
    "            dItemGamma[i][k] += 2*lamb*itemGamma[i][k]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    for u in users:\n",
    "        dtheta += dUserGamma[u]\n",
    "    for i in items:\n",
    "        dtheta += dItemGamma[i]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.4731606725452635\n",
      "MSE = 1.4585579071198616\n",
      "MSE = 11.399615272662963\n",
      "MSE = 1.457355011553468\n",
      "MSE = 1.362105496200898\n",
      "MSE = 1.3617641855130933\n",
      "MSE = 1.3606392194120347\n",
      "MSE = 1.3607784745127438\n",
      "MSE = 1.361300203946284\n",
      "MSE = 1.3608815921069584\n",
      "MSE = 1.361508747882666\n",
      "MSE = 1.3609648599608084\n",
      "MSE = 1.3608945415567408\n",
      "MSE = 1.3608836532375432\n",
      "MSE = 1.3608819213762127\n",
      "MSE = 1.3608816447391514\n",
      "MSE = 1.360881600520776\n",
      "MSE = 1.3608815934519811\n",
      "MSE = 1.3608815923220703\n",
      "MSE = 1.3608815921413568\n",
      "MSE = 1.3608815921124637\n",
      "MSE = 1.3608815921078279\n",
      "MSE = 1.360881592107075\n",
      "MSE = 1.3608815921069697\n",
      "MSE = 1.3608815921070418\n",
      "MSE = 1.360881592106977\n",
      "MSE = 1.3608815921069697\n",
      "MSE = 1.3608815921069755\n",
      "MSE = 1.3608815921069697\n",
      "MSE = 1.3608815921069697\n",
      "MSE = 1.360882984959193\n",
      "MSE = 1.3608818481480898\n",
      "MSE = 1.3608816393247558\n",
      "MSE = 1.3608816008197253\n",
      "MSE = 1.3608815937150593\n",
      "MSE = 1.360881592403613\n",
      "MSE = 1.3608815921616397\n",
      "MSE = 1.3608815921170523\n",
      "MSE = 1.360881592108743\n",
      "MSE = 1.3608815921072497\n",
      "MSE = 1.3608815921082345\n",
      "MSE = 1.3608815921074262\n",
      "MSE = 1.3608815921072677\n",
      "MSE = 1.3608815921072497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3.87933610e+00,  4.75676895e-02,  1.92375278e-02, ...,\n",
       "        -8.90784138e-05, -3.42203736e-04, -6.12530994e-04]),\n",
       " 1.4097564101752147,\n",
       " {'funcalls': 44,\n",
       "  'grad': array([ 1.95098963e-05,  5.05694400e-08,  8.34533129e-07, ...,\n",
       "         -1.78423220e-07, -6.83913802e-07, -1.22000814e-06]),\n",
       "  'nit': 7,\n",
       "  'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH',\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamb = 0.001\n",
    "theta_init = [alpha] + [0.0]*(nUsers+nItems) +  \\\n",
    "            [random.random() * 0.1 - 0.05 for k in range(K*(nUsers+nItems))]\n",
    "scipy.optimize.fmin_l_bfgs_b(cost, theta_init, derivative, args = (ytrain, lamb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 1e-06\n",
      "MSE = 1.4734631005275007\n",
      "MSE = 2.2289941967653615\n",
      "MSE = 1.4729891463264104\n",
      "MSE = 1.4727831465346721\n",
      "MSE = 1.4721179345183184\n",
      "MSE = 1.468897925116665\n",
      "MSE = 1.4573683245509172\n",
      "MSE = 1.432848594298881\n",
      "MSE = 1.3790594454845557\n",
      "MSE = 1.297683793182218\n",
      "MSE = 1.2210357433556598\n",
      "MSE = 1.189803538492067\n",
      "MSE = 1.1857077773429157\n",
      "MSE = 1.1848446637231715\n",
      "MSE = 1.182957043043187\n",
      "MSE = 1.1770931354657612\n",
      "MSE = 1.1630902191748085\n",
      "MSE = 1.1339381809053934\n",
      "MSE = 1.0939925848696261\n",
      "MSE = 1.88019161600758\n",
      "MSE = 1.0918676037333273\n",
      "MSE = 1.0659253240525661\n",
      "MSE = 1.0583909826251494\n",
      "MSE = 1.057281807018967\n",
      "MSE = 1.0563425915850246\n",
      "MSE = 1.050347685981483\n",
      "MSE = 1.0391645448673952\n",
      "MSE = 1.0184654963128168\n",
      "MSE = 0.9988208285324273\n",
      "MSE = 0.9985765279133295\n",
      "MSE = 0.9938917682054514\n",
      "MSE = 0.9890699207519269\n",
      "MSE = 0.9884245407412874\n",
      "MSE = 0.988021042207404\n",
      "MSE = 0.9869461681761265\n",
      "MSE = 0.9835785207975164\n",
      "MSE = 1.0321413920928233\n",
      "MSE = 0.9823916923884309\n",
      "MSE = 0.9759190212249278\n",
      "MSE = 0.9620182528622783\n",
      "MSE = 1.6828403940591763\n",
      "MSE = 0.9522134264748967\n",
      "MSE = 0.9477631349796157\n",
      "MSE = 0.9459315398808328\n",
      "MSE = 0.9452067512689772\n",
      "MSE = 0.9426137004907199\n",
      "MSE = 0.9380865832447399\n",
      "MSE = 0.9323001002234848\n",
      "MSE = 0.9287974315454885\n",
      "MSE = 0.9312608933114269\n",
      "MSE = 0.9281788535447878\n",
      "MSE = 0.9262960288698778\n",
      "MSE = 0.9248013466749222\n",
      "MSE = 0.9219526696527268\n",
      "MSE = 0.921688003575675\n",
      "MSE = 0.9193899049263128\n",
      "MSE = 0.9190354124330486\n",
      "MSE = 0.919019474496531\n",
      "MSE = 0.9179785043457894\n",
      "MSE = 0.9167008085725461\n",
      "MSE = 0.9144110802357498\n",
      "MSE = 0.9131865991994402\n",
      "MSE = 0.9127351192551431\n",
      "MSE = 0.9125637945461049\n",
      "MSE = 0.9125387726703705\n",
      "MSE = 0.9124352567601903\n",
      "MSE = 0.9122090798711285\n",
      "MSE = 0.9118275116904063\n",
      "MSE = 0.9115208001799213\n",
      "MSE = 0.9121299312756812\n",
      "MSE = 0.9111717038316367\n",
      "MSE = 0.9110470561189664\n",
      "MSE = 0.9109281676141732\n",
      "MSE = 0.9106293379547657\n",
      "MSE = 0.9101189946908665\n",
      "MSE = 0.909507009148939\n",
      "MSE = 0.908745517505179\n",
      "MSE = 0.9079961468090676\n",
      "MSE = 0.9074150316758783\n",
      "MSE = 0.9065109320732376\n",
      "MSE = 0.9052809949507137\n",
      "MSE = 0.9048411928506801\n",
      "MSE = 0.9046452705102644\n",
      "MSE = 0.9045594843785156\n",
      "MSE = 0.9043204894104241\n",
      "MSE = 0.903097651132481\n",
      "MSE = 0.904009631570165\n",
      "MSE = 0.9029388684068749\n",
      "MSE = 0.9026352703519593\n",
      "MSE = 0.9021529212608957\n",
      "MSE = 0.9016027593578992\n",
      "MSE = 0.9013526986324654\n",
      "MSE = 0.9012554188591594\n",
      "MSE = 0.9012071529973037\n",
      "MSE = 0.9011508134554996\n",
      "MSE = 0.9010107843706864\n",
      "MSE = 0.9254329650211318\n",
      "MSE = 0.9009034553198132\n",
      "MSE = 0.9004445661222157\n",
      "MSE = 0.9007454940292572\n",
      "MSE = 0.900213130033258\n",
      "MSE = 0.8997040704433956\n",
      "MSE = 0.89930648491959\n",
      "MSE = 0.8989038451726405\n",
      "MSE = 0.8985711448243139\n",
      "MSE = 0.8981381570750157\n",
      "MSE = 1.6908919315659794\n",
      "MSE = 0.8980963769999316\n",
      "MSE = 0.897643721263419\n",
      "MSE = 0.8969252094614945\n",
      "MSE = 0.895490352373827\n",
      "MSE = 0.8943322383429958\n",
      "MSE = 0.892594840495292\n",
      "MSE = 0.8918576763976837\n",
      "MSE = 0.8898398061340017\n",
      "MSE = 0.8892954315846544\n",
      "MSE = 0.8889976755115724\n",
      "MSE = 0.8886354742446877\n",
      "MSE = 7.405912057723457\n",
      "MSE = 0.8885243518356785\n",
      "MSE = 0.8882392789872386\n",
      "MSE = 0.8872955487058974\n",
      "MSE = 0.8865570648231094\n",
      "MSE = 0.8858784127846214\n",
      "MSE = 0.8842861816785155\n",
      "MSE = 0.8815200398184093\n",
      "MSE = 0.8909076426612513\n",
      "MSE = 0.8805104809071043\n",
      "MSE = 0.8757093954213294\n",
      "MSE = 0.8717401441043718\n",
      "MSE = 0.8664335358643538\n",
      "MSE = 0.8634792578030468\n",
      "MSE = 0.86280249716493\n",
      "MSE = 0.859956222721873\n",
      "MSE = 0.8528756690186358\n",
      "MSE = 0.8447207308086373\n",
      "MSE = 0.840443828789051\n",
      "MSE = 0.8323701709746765\n",
      "MSE = 0.8916620737288743\n",
      "MSE = 0.8317273885736496\n",
      "MSE = 0.8264890909814407\n",
      "MSE = 0.8241296821967459\n",
      "MSE = 0.8227372215792382\n",
      "MSE = 0.8196973445717475\n",
      "MSE = 0.8145421906536631\n",
      "MSE = 0.8118953510744031\n",
      "MSE = 0.805195662188702\n",
      "MSE = 0.8028871102774034\n",
      "MSE = 0.8017487057059965\n",
      "MSE = 0.8014225409381366\n",
      "MSE = 0.7996339150518125\n",
      "MSE = 0.7972507725505953\n",
      "MSE = 0.7944400790180797\n",
      "MSE = 0.792221111221202\n",
      "MSE = 0.7909404485880858\n",
      "MSE = 0.790069792365242\n",
      "MSE = 0.7888259399395801\n",
      "MSE = 0.787422945137922\n",
      "MSE = 0.7856125631205373\n",
      "MSE = 0.7832857833099428\n",
      "MSE = 0.7816404280676198\n",
      "MSE = 0.7757520471599666\n",
      "MSE = 0.7688977071424026\n",
      "MSE = 0.7690960217738688\n",
      "MSE = 0.7653241660297097\n",
      "MSE = 0.7616417524783653\n",
      "MSE = 0.7606873998837943\n",
      "MSE = 0.7739322569646017\n",
      "MSE = 0.7601165525430578\n",
      "MSE = 0.7597346182822082\n",
      "MSE = 0.7586481135035896\n",
      "MSE = 0.7543736299156697\n",
      "MSE = 0.7473670911469548\n",
      "MSE = 1.1958121205199956\n",
      "MSE = 0.7464805976872076\n",
      "MSE = 0.7370183307162996\n",
      "MSE = 0.7312078790173188\n",
      "MSE = 0.7291300230621706\n",
      "MSE = 0.7282672804781981\n",
      "MSE = 0.7306863703269697\n",
      "MSE = 0.7272923377278409\n",
      "MSE = 0.7253400996458923\n",
      "MSE = 0.722485943720032\n",
      "MSE = 0.7186873128077909\n",
      "MSE = 0.7169106893429404\n",
      "MSE = 0.7151220819238\n",
      "MSE = 0.7155093267400099\n",
      "MSE = 0.7144750606914296\n",
      "MSE = 0.7136822499129432\n",
      "MSE = 0.7094127801806366\n",
      "MSE = 0.7053693873389183\n",
      "MSE = 0.6981707640724081\n",
      "MSE = 0.6958140499140093\n",
      "MSE = 1.066515843743267\n",
      "MSE = 0.6941897861262554\n",
      "MSE = 0.6938695064913141\n",
      "MSE = 0.693780353424145\n",
      "MSE = 0.6935619907288003\n",
      "MSE = 0.6927041953769854\n",
      "MSE = 0.6910026129650881\n",
      "MSE = 0.6935205185002279\n",
      "MSE = 0.6903181186632242\n",
      "MSE = 0.6876245631063195\n",
      "MSE = 0.6885392034399599\n",
      "MSE = 0.6837005237199238\n",
      "MSE = 0.6777556260347822\n",
      "MSE = 0.6685720551461959\n",
      "MSE = 195.25370164788177\n",
      "MSE = 0.8510260151697943\n",
      "MSE = 0.6683784873564144\n",
      "MSE = 0.6629128353645135\n",
      "MSE = 0.6619539998566608\n",
      "MSE = 0.6613979726793219\n",
      "MSE = 0.6611833500386084\n",
      "MSE = 0.6599260267047894\n",
      "MSE = 0.6582082981804268\n",
      "MSE = 0.6550108177311507\n",
      "MSE = 4.699669233266293\n",
      "MSE = 0.6548125149807285\n",
      "MSE = 0.6525277506756736\n",
      "MSE = 0.6515102438074573\n",
      "MSE = 0.6511265699606009\n",
      "MSE = 0.6508445793532145\n",
      "MSE = 0.652784023258821\n",
      "MSE = 0.6506451974645644\n",
      "MSE = 0.6501555509847019\n",
      "MSE = 0.649147706738272\n",
      "MSE = 0.6489327451450156\n",
      "MSE = 0.6486538114241326\n",
      "MSE = 0.6479616407000488\n",
      "MSE = 0.6466748176425569\n",
      "MSE = 1.426415336842259\n",
      "MSE = 0.6463441025508613\n",
      "MSE = 0.6447065682992369\n",
      "MSE = 0.6433320227302581\n",
      "MSE = 0.6429901153665915\n",
      "MSE = 0.753824129607488\n",
      "MSE = 0.6429484873136075\n",
      "MSE = 0.6428200105605907\n",
      "MSE = 0.6464707618472644\n",
      "MSE = 0.6426702371451107\n",
      "MSE = 0.642032350359345\n",
      "MSE = 0.6341600080114042\n",
      "MSE = 0.6286402718869462\n",
      "MSE = 0.620839731813641\n",
      "MSE = 0.6189823793214507\n",
      "MSE = 0.6274435242852487\n",
      "MSE = 0.6180716833014919\n",
      "MSE = 0.6171752850469228\n",
      "MSE = 0.6137962101111389\n",
      "MSE = 9.50959785060562\n",
      "MSE = 0.6137690056896404\n",
      "MSE = 0.6100060315703034\n",
      "MSE = 0.6077421114270991\n",
      "MSE = 0.6109125539118051\n",
      "MSE = 0.6068107213554195\n",
      "MSE = 0.6048692679565039\n",
      "MSE = 0.6040286875864166\n",
      "MSE = 0.5998786556543916\n",
      "MSE = 0.5954616393822022\n",
      "MSE = 0.6271798376634166\n",
      "MSE = 0.594643751240621\n",
      "MSE = 0.5932328664529679\n",
      "MSE = 0.5906185615310651\n",
      "MSE = 0.5849687749614761\n",
      "MSE = 0.6017553177799985\n",
      "MSE = 0.5838512721622604\n",
      "MSE = 0.58340120591487\n",
      "MSE = 0.5831170918173949\n",
      "MSE = 0.5822696705746659\n",
      "MSE = 0.5810670151008364\n",
      "MSE = 0.5802283138367421\n",
      "MSE = 0.5796409641910487\n",
      "MSE = 0.5786179951724336\n",
      "MSE = 0.6046685121469679\n",
      "MSE = 0.578019486456184\n",
      "MSE = 0.5769279282609927\n",
      "MSE = 0.57503187465503\n",
      "MSE = 0.5733699443220772\n",
      "MSE = 0.5756376519939974\n",
      "MSE = 0.5732462755685599\n",
      "MSE = 0.57282315524848\n",
      "MSE = 0.5725829705128143\n",
      "MSE = 0.5724592514362571\n",
      "MSE = 0.5722188755266738\n",
      "MSE = 0.572002085665376\n",
      "MSE = 0.5717632865248033\n",
      "MSE = 0.5714506929442658\n",
      "MSE = 0.5723008533710741\n",
      "MSE = 0.5715039931068109\n",
      "MSE = 0.5709250609880149\n",
      "MSE = 0.5699154863311942\n",
      "MSE = 0.5676961673151215\n",
      "MSE = 0.5703569784055306\n",
      "MSE = 0.5676143507490474\n",
      "MSE = 0.5673617171982998\n",
      "MSE = 0.5678739064130648\n",
      "MSE = 0.5672122559574548\n",
      "MSE = 0.5671195210977111\n",
      "MSE = 0.5669238022524644\n",
      "MSE = 0.5662543982581218\n",
      "MSE = 0.565330828335615\n",
      "MSE = 0.5649180678296819\n",
      "MSE = 0.5644513003081217\n",
      "MSE = 0.5781901024458962\n",
      "MSE = 0.563705463769119\n",
      "MSE = 0.5599399791479384\n",
      "MSE = 0.6872161583549725\n",
      "MSE = 0.5598996572629298\n",
      "MSE = 0.5596562708590089\n",
      "MSE = 0.5603274334650497\n",
      "MSE = 0.559411553517247\n",
      "MSE = 0.5593600083003794\n",
      "MSE = 0.5589065920039787\n",
      "MSE = 0.5587463832260872\n",
      "MSE = 0.55873239625331\n",
      "MSE = 0.5582869887063978\n",
      "MSE = 0.5577228006633296\n",
      "MSE = 0.5564930736258276\n",
      "MSE = 0.5562271099787174\n",
      "MSE = 0.8249182481956711\n",
      "MSE = 0.5562171615995162\n",
      "MSE = 0.5557533246451312\n",
      "MSE = 0.5552999730837089\n",
      "MSE = 0.5535009240335929\n",
      "MSE = 0.5520393415730906\n",
      "MSE = 0.5509857737203685\n",
      "MSE = 0.5652631760883824\n",
      "MSE = 0.5508696798392242\n",
      "MSE = 0.5505432201977875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.7311010329797539\n",
      "MSE = 0.5504934637265139\n",
      "MSE = 0.549839295772244\n",
      "MSE = 0.54935785150265\n",
      "MSE = 0.5845717104067994\n",
      "MSE = 0.5493408842918147\n",
      "MSE = 0.5492280077424355\n",
      "MSE = 0.5490482484171575\n",
      "MSE = 0.5488993171667669\n",
      "MSE = 0.5483887596699809\n",
      "MSE = 0.5475141979585454\n",
      "MSE = 0.5464105453714472\n",
      "MSE = 0.5461677036630895\n",
      "MSE = 0.5457550600401011\n",
      "MSE = 0.5455517690598511\n",
      "MSE = 0.5453705795414363\n",
      "MSE = 0.5452593536490007\n",
      "MSE = 0.5451347704471814\n",
      "MSE = 0.5453366124325064\n",
      "MSE = 0.5446735224911158\n",
      "MSE = 0.5437802501528999\n",
      "MSE = 0.5420751158591083\n",
      "MSE = 0.5411057588785745\n",
      "MSE = 0.5387978154488863\n",
      "MSE = 0.5383361781870385\n",
      "MSE = 0.5383374062421769\n",
      "MSE = 0.5383368625437881\n",
      "MSE = 0.5384893123942421\n",
      "MSE = 0.5382968080821143\n",
      "MSE = 0.5381532165472016\n",
      "MSE = 0.5373327812222216\n",
      "MSE = 0.5368708555857774\n",
      "MSE = 0.5366177636234042\n",
      "MSE = 0.5366928910506356\n",
      "MSE = 0.5365564896537538\n",
      "MSE = 0.5365316043887168\n",
      "MSE = 0.5361555598228129\n",
      "MSE = 0.5625988288014345\n",
      "MSE = 0.5360733579129161\n",
      "MSE = 0.5356786902287403\n",
      "MSE = 0.5329098066767912\n",
      "MSE = 0.6369715178566799\n",
      "MSE = 0.5327309064541811\n",
      "MSE = 0.5315782950446443\n",
      "MSE = 0.5310374122396387\n",
      "MSE = 0.5305541339052123\n",
      "MSE = 0.5972779192734825\n",
      "MSE = 0.5304582183203134\n",
      "MSE = 0.5302629215920854\n",
      "MSE = 0.5300762352932092\n",
      "MSE = 0.5298685658819716\n",
      "MSE = 0.5295480220880672\n",
      "MSE = 0.528585480517293\n",
      "MSE = 0.5280199159468207\n",
      "MSE = 0.5279215422290073\n",
      "MSE = 0.5277359998135984\n",
      "MSE = 0.5280398376625373\n",
      "MSE = 0.5276018948221346\n",
      "MSE = 0.5274064774425248\n",
      "MSE = 0.5270289439275412\n",
      "MSE = 0.5264107608900699\n",
      "MSE = 0.5312109382205545\n",
      "MSE = 0.5264722996584356\n",
      "MSE = 0.5262568394843166\n",
      "MSE = 0.5279423795440074\n",
      "MSE = 0.5263184461966526\n",
      "MSE = 0.5262544715592811\n",
      "MSE = 0.5265518041759466\n",
      "MSE = 0.5261525209601935\n",
      "MSE = 0.5259510514039003\n",
      "MSE = 0.5250058005380583\n",
      "MSE = 0.5324480896822587\n",
      "MSE = 0.5249315237396597\n",
      "MSE = 0.5248120761939529\n",
      "MSE = 0.524428296710543\n",
      "MSE = 0.5238291121360283\n",
      "MSE = 0.5299930809531539\n",
      "MSE = 0.5235889489628928\n",
      "MSE = 0.5228484372637554\n",
      "MSE = 0.5224286085420211\n",
      "MSE = 0.5225187477241939\n",
      "MSE = 0.5222831915448999\n",
      "MSE = 0.5218163768522548\n",
      "MSE = 0.5215329452659662\n",
      "MSE = 0.5257642208548652\n",
      "MSE = 0.5211994439211013\n",
      "MSE = 0.5208312627374634\n",
      "MSE = 0.5206152129770857\n",
      "MSE = 0.5204289626505675\n",
      "MSE = 0.5202682490897065\n",
      "MSE = 0.5205832607705843\n",
      "MSE = 0.5200941231197295\n",
      "MSE = 0.5196306177672099\n",
      "MSE = 0.5194531359562574\n",
      "MSE = 0.5192195193455854\n",
      "MSE = 0.5217838336361361\n",
      "MSE = 0.5193531793378899\n",
      "MSE = 0.6686214906444897\n",
      "MSE = 0.5193995568371351\n",
      "MSE = 0.5193623566139665\n",
      "MSE = 0.5205749057020647\n",
      "MSE = 0.5191925192523702\n",
      "MSE = 0.5190733106702203\n",
      "MSE = 0.5184867456527167\n",
      "MSE = 0.5181690497475744\n",
      "MSE = 0.5179871337369765\n",
      "MSE = 0.51794108760298\n",
      "MSE = 0.5179118751041097\n",
      "MSE = 0.5178546996538677\n",
      "MSE = 0.5189201535733822\n",
      "MSE = 0.5176186847937279\n",
      "MSE = 0.5173856251473385\n",
      "MSE = 0.5169296050533396\n",
      "MSE = 0.5412128425520712\n",
      "MSE = 0.5169101805562725\n",
      "MSE = 0.5166377184323503\n",
      "MSE = 0.5162841173416332\n",
      "MSE = 0.5161304317673856\n",
      "MSE = 0.5161117795784331\n",
      "MSE = 0.5160589253002247\n",
      "MSE = 0.5159351225828677\n",
      "MSE = 0.5157855941281081\n",
      "MSE = 0.5154308574251574\n",
      "MSE = 0.5150893268784782\n",
      "MSE = 0.5149500501942774\n",
      "MSE = 0.5207545313822094\n",
      "MSE = 0.5148952823383988\n",
      "MSE = 0.5146155322950936\n",
      "MSE = 0.5139862862524535\n",
      "MSE = 0.5137433549392254\n",
      "MSE = 0.5330501798678544\n",
      "MSE = 0.5136552987534996\n",
      "MSE = 0.5134170322437105\n",
      "MSE = 0.6073803507450091\n",
      "MSE = 0.512414785892693\n",
      "MSE = 0.5118894228327574\n",
      "MSE = 0.5116915914922233\n",
      "MSE = 0.5115129553912309\n",
      "MSE = 0.5127348398546806\n",
      "MSE = 0.5114688065432853\n",
      "MSE = 0.5113069609311149\n",
      "MSE = 0.5128716613093448\n",
      "MSE = 0.5112389251904526\n",
      "MSE = 0.5110940867992534\n",
      "MSE = 0.5108104163055708\n",
      "MSE = 0.5105211609976142\n",
      "MSE = 0.5104636222172074\n",
      "MSE = 0.5104812253238821\n",
      "MSE = 0.5105098204481515\n",
      "MSE = 0.5105238364696135\n",
      "MSE = 0.5103455146428509\n",
      "MSE = 0.5117420315481388\n",
      "MSE = 0.5102831595060984\n",
      "MSE = 0.5102055668571128\n",
      "MSE = 0.5101429338819186\n",
      "MSE = 0.5101709917283915\n",
      "MSE = 0.5101471257044782\n",
      "MSE = 0.5100323333007252\n",
      "MSE = 0.5099524166079532\n",
      "MSE = 0.5098304396959519\n",
      "MSE = 0.510084177025652\n",
      "MSE = 0.5097659847127275\n",
      "MSE = 0.5096524266537589\n",
      "MSE = 0.5095471231867585\n",
      "MSE = 0.5094130422429448\n",
      "MSE = 0.5091896833956986\n",
      "MSE = 0.5185805527837194\n",
      "MSE = 0.5091574673905147\n",
      "MSE = 0.508975935405005\n",
      "MSE = 0.5086174293384035\n",
      "MSE = 0.5086026566535329\n",
      "MSE = 0.508550110894854\n",
      "MSE = 0.5085932518708307\n",
      "MSE = 0.5076699221049651\n",
      "MSE = 0.5077712520203214\n",
      "MSE = 0.5074600552326531\n",
      "MSE = 0.507382457255246\n",
      "MSE = 0.5073359190055016\n",
      "MSE = 0.5072892064959877\n",
      "MSE = 0.5071964375729355\n",
      "MSE = 0.5070094254299692\n",
      "MSE = 0.5176446689085195\n",
      "MSE = 0.5070449881666342\n",
      "MSE = 0.5068503337649912\n",
      "MSE = 0.5066925096886059\n",
      "MSE = 0.506568746008991\n",
      "MSE = 0.5065626582691181\n",
      "MSE = 0.5064964386135596\n",
      "MSE = 0.5063102320069919\n",
      "MSE = 0.5061009701400789\n",
      "MSE = 0.5054251108615196\n",
      "MSE = 0.5086642230476625\n",
      "MSE = 0.5049264043567516\n",
      "MSE = 0.50484017035611\n",
      "MSE = 0.504871551591947\n",
      "MSE = 0.5049388441026963\n",
      "MSE = 0.5049561309847644\n",
      "MSE = 0.5090980009654097\n",
      "MSE = 0.5049455523626984\n",
      "MSE = 0.5049318942198896\n",
      "MSE = 0.5048275578108363\n",
      "MSE = 0.5044815795342072\n",
      "MSE = 0.5043119632403834\n",
      "MSE = 0.5041086344719931\n",
      "MSE = 0.5037062056908747\n",
      "MSE = 0.5035795753407182\n",
      "MSE = 0.5039299681553756\n",
      "MSE = 0.5036052177083103\n",
      "MSE = 0.5035911840876348\n",
      "MSE = 0.5034263568118501\n",
      "MSE = 0.5087671135888978\n",
      "MSE = 0.5034214373450149\n",
      "MSE = 0.5033406349153924\n",
      "MSE = 0.5031599428262834\n",
      "MSE = 0.5030606735688791\n",
      "MSE = 0.5026081131114274\n",
      "MSE = 0.5025161840483444\n",
      "MSE = 0.5025493606105721\n",
      "MSE = 0.5027822816384643\n",
      "MSE = 0.502555788011304\n",
      "MSE = 0.5025762448216877\n",
      "MSE = 0.5025720649814919\n",
      "MSE = 0.5025338679962653\n",
      "MSE = 0.502443059930835\n",
      "MSE = 0.5022704324342238\n",
      "MSE = 0.5021289395067409\n",
      "MSE = 0.5020387072915814\n",
      "MSE = 0.5017077091002171\n",
      "MSE = 0.50187492025846\n",
      "MSE = 0.5017853477683678\n",
      "MSE = 0.5017505129925063\n",
      "MSE = 0.5016331718016724\n",
      "MSE = 0.5015917196494976\n",
      "MSE = 0.5015086724666723\n",
      "MSE = 0.5012934165027937\n",
      "MSE = 0.5010783808070405\n",
      "MSE = 0.5009564605576539\n",
      "MSE = 0.5009361294475999\n",
      "MSE = 0.5009232161243572\n",
      "MSE = 0.5009851869708704\n",
      "MSE = 0.5009889461896938\n",
      "MSE = 0.5023804589756338\n",
      "MSE = 0.5010255072025342\n",
      "MSE = 0.5009999414816286\n",
      "MSE = 0.5009623469808958\n",
      "MSE = 0.5008460611042599\n",
      "MSE = 0.5008854113767216\n",
      "MSE = 0.5008298856172057\n",
      "MSE = 0.5007223437541447\n",
      "MSE = 0.5008223025731066\n",
      "MSE = 0.5005771736994139\n",
      "MSE = 0.5004391742189856\n",
      "MSE = 0.500185335891609\n",
      "MSE = 0.5001111813536229\n",
      "MSE = 0.502490664283913\n",
      "MSE = 0.5001058325213475\n",
      "MSE = 0.5000710321956446\n",
      "MSE = 0.5000101109838442\n",
      "MSE = 0.49991558057055374\n",
      "MSE = 0.4998908239511349\n",
      "MSE = 0.49980962694320097\n",
      "MSE = 0.4997136628312763\n",
      "MSE = 0.4995813875601561\n",
      "MSE = 0.49941399824348814\n",
      "MSE = 0.49936887871231106\n",
      "MSE = 0.49936858093891257\n",
      "MSE = 0.5030320421984257\n",
      "MSE = 0.49926057554527564\n",
      "MSE = 0.4991366427190753\n",
      "MSE = 0.49904268652122674\n",
      "MSE = 0.4988178207985644\n",
      "MSE = 0.5484719036254524\n",
      "MSE = 0.4988154820624876\n",
      "MSE = 0.4987040716501943\n",
      "MSE = 0.507926626534634\n",
      "MSE = 0.49866963983373536\n",
      "MSE = 0.49831355463478105\n",
      "MSE = 0.49880106904411664\n",
      "MSE = 0.49831376979014397\n",
      "MSE = 0.49813394245731624\n",
      "MSE = 0.49759278995757045\n",
      "MSE = 0.4973709248791463\n",
      "MSE = 0.49723161366967444\n",
      "MSE = 0.4969654304577126\n",
      "MSE = 0.49675569481238074\n",
      "MSE = 0.49653844137077063\n",
      "MSE = 0.49632458024846954\n",
      "MSE = 0.49622301360870075\n",
      "MSE = 0.4959165340117321\n",
      "MSE = 0.49649645521443064\n",
      "MSE = 0.49591438051220676\n",
      "MSE = 0.49588528950644917\n",
      "MSE = 0.49588062941306027\n",
      "MSE = 0.4958878877311083\n",
      "MSE = 0.4958997359628935\n",
      "MSE = 0.4956690472377041\n",
      "MSE = 0.4956544136424935\n",
      "MSE = 0.4955584940516243\n",
      "MSE = 0.49544092295792724\n",
      "MSE = 0.4958523447659851\n",
      "MSE = 0.49543591567600964\n",
      "MSE = 0.49534482785969575\n",
      "MSE = 0.4953115173334812\n",
      "MSE = 0.4951945059942629\n",
      "MSE = 0.4951869996328816\n",
      "MSE = 0.4951554706737388\n",
      "MSE = 0.49512224958673123\n",
      "MSE = 0.4952011124906578\n",
      "MSE = 0.49506023275292693\n",
      "MSE = 0.49499862620184865\n",
      "MSE = 0.4949755485956101\n",
      "MSE = 0.4948948203260354\n",
      "MSE = 0.49713183137629724\n",
      "MSE = 0.49488823237456936\n",
      "MSE = 0.49479729606382533\n",
      "MSE = 0.4947306242956377\n",
      "MSE = 0.49471218122197747\n",
      "MSE = 0.4946651976728702\n",
      "MSE = 0.49443460049991594\n",
      "MSE = 0.49421016266617906\n",
      "MSE = 0.4942220310568295\n",
      "MSE = 0.49420173668209405\n",
      "MSE = 0.49419929852055844\n",
      "MSE = 0.49419116771508836\n",
      "MSE = 0.49414944879701955\n",
      "MSE = 0.4940839758340349\n",
      "MSE = 0.49395363480662385\n",
      "MSE = 0.49359692799746974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.4935300972634934\n",
      "MSE = 0.4935209066363572\n",
      "MSE = 0.4934853200980747\n",
      "MSE = 0.4934378316089915\n",
      "MSE = 0.4953387413607877\n",
      "MSE = 0.4934399092120351\n",
      "MSE = 0.4933988595688332\n",
      "MSE = 0.4933590545961987\n",
      "MSE = 0.4933315575538054\n",
      "MSE = 0.49329188444275257\n",
      "MSE = 0.49332245708489336\n",
      "MSE = 0.49327955649781385\n",
      "MSE = 0.4932368227489376\n",
      "MSE = 0.5078418018759662\n",
      "MSE = 0.4932402343223328\n",
      "MSE = 0.49321015270066\n",
      "MSE = 0.49319311115587705\n",
      "MSE = 0.49311119551679505\n",
      "MSE = 0.4936403122362767\n",
      "MSE = 0.4931072005477547\n",
      "MSE = 0.4929852536550268\n",
      "MSE = 0.4927679223370117\n",
      "MSE = 0.49282299426038473\n",
      "MSE = 0.4926376395482543\n",
      "MSE = 0.49240992179733345\n",
      "MSE = 0.49242797500384555\n",
      "MSE = 0.49238235820900833\n",
      "MSE = 0.4923051823783196\n",
      "MSE = 0.49225462974718537\n",
      "MSE = 0.4922749729864694\n",
      "MSE = 0.4922455118587826\n",
      "MSE = 0.4921877560558174\n",
      "MSE = 0.4921607763911802\n",
      "MSE = 0.4921375845654485\n",
      "MSE = 0.49215343997939953\n",
      "MSE = 0.49205352260839297\n",
      "MSE = 0.49204335816846595\n",
      "MSE = 0.49199978479138884\n",
      "MSE = 0.49196132420721017\n",
      "MSE = 0.49192489718020954\n",
      "MSE = 0.49192862552474004\n",
      "MSE = 0.4918430793587705\n",
      "MSE = 0.49187783128017454\n",
      "MSE = 0.49187670290336927\n",
      "MSE = 0.49187992517185886\n",
      "MSE = 0.4918817850739226\n",
      "MSE = 0.4918978736260125\n",
      "MSE = 0.4919126151627656\n",
      "MSE = 0.49232815774295574\n",
      "MSE = 0.4918845237547673\n",
      "MSE = 0.49192316568849076\n",
      "MSE = 0.4918914095444163\n",
      "MSE = 0.491824119569005\n",
      "MSE = 0.49182092498377583\n",
      "MSE = 0.49183347042927383\n",
      "MSE = 0.4918155886607319\n",
      "MSE = 0.49180026524406345\n",
      "MSE = 0.4917872510350992\n",
      "MSE = 0.4917734356351344\n",
      "MSE = 0.4917243441045407\n",
      "MSE = 0.5016425743455251\n",
      "MSE = 0.49174103694397764\n",
      "MSE = 0.4916768821790475\n",
      "MSE = 0.49160957041039677\n",
      "MSE = 0.49157016834569417\n",
      "MSE = 0.49189897100212954\n",
      "MSE = 0.49159328905336935\n",
      "MSE = 0.4915318084054001\n",
      "MSE = 0.4914742182144646\n",
      "MSE = 0.49141086769271686\n",
      "MSE = 0.49137363084549274\n",
      "MSE = 0.4920574577213989\n",
      "MSE = 0.49134064466763444\n",
      "MSE = 0.49121947453351217\n",
      "MSE = 0.4911327964801979\n",
      "MSE = 0.4911300361461705\n",
      "MSE = 0.4911409487884292\n",
      "MSE = 0.4911476564221998\n",
      "MSE = 0.4911555932829187\n",
      "MSE = 0.5068129073029176\n",
      "MSE = 0.4911565595948074\n",
      "MSE = 0.4911539966525615\n",
      "MSE = 0.49566504975469405\n",
      "MSE = 0.49113769089398757\n",
      "MSE = 0.49105985027749754\n",
      "MSE = 0.4960367973379469\n",
      "MSE = 0.4910607301659162\n",
      "MSE = 0.4909782184062984\n",
      "MSE = 0.4909173369592933\n",
      "MSE = 0.490801128633401\n",
      "MSE = 0.49065326353486316\n",
      "MSE = 0.49059325061463493\n",
      "MSE = 0.490667617332056\n",
      "MSE = 0.49403990249632523\n",
      "MSE = 0.4906402435087521\n",
      "MSE = 0.4904728316960531\n",
      "MSE = 0.49045561170045654\n",
      "MSE = 0.49044430122299043\n",
      "MSE = 0.4904275419608224\n",
      "MSE = 0.49041685693021214\n",
      "MSE = 0.49041527104176186\n",
      "MSE = 0.49039039065567913\n",
      "MSE = 0.49036778546978377\n",
      "MSE = 0.49362002896088114\n",
      "MSE = 0.49038188205886674\n",
      "MSE = 0.4903346879575358\n",
      "MSE = 0.4902159473807967\n",
      "MSE = 0.49016512878589996\n",
      "MSE = 0.4900930385391077\n",
      "MSE = 0.4900454996963609\n",
      "MSE = 0.49000149556981676\n",
      "MSE = 0.48995813970315993\n",
      "MSE = 0.4899350089282321\n",
      "MSE = 0.508471525038129\n",
      "MSE = 0.4899352683652239\n",
      "MSE = 0.4898999406899894\n",
      "MSE = 0.4898803024514401\n",
      "MSE = 0.49000465417548694\n",
      "MSE = 0.48981470637023383\n",
      "MSE = 0.4898004284347635\n",
      "MSE = 0.489781481175504\n",
      "MSE = 0.48974475981666826\n",
      "MSE = 0.4897478730723138\n",
      "MSE = 0.48972411496947577\n",
      "MSE = 0.4896830818420627\n",
      "MSE = 0.489572983293108\n",
      "MSE = 0.48955420642118114\n",
      "MSE = 0.489580276851935\n",
      "MSE = 0.4895417248698218\n",
      "MSE = 0.489533896428091\n",
      "MSE = 0.4942182237784154\n",
      "MSE = 0.48939191976652124\n",
      "MSE = 0.48939649582472616\n",
      "MSE = 0.48940286159050045\n",
      "MSE = 0.48939991976176905\n",
      "MSE = 0.48938101707366644\n",
      "MSE = 0.4893323568246117\n",
      "MSE = 0.4892512617115488\n",
      "MSE = 0.4891786714575696\n",
      "MSE = 0.48911720725344715\n",
      "MSE = 0.4890866344285991\n",
      "MSE = 0.4891090982695145\n",
      "MSE = 0.48905748390721304\n",
      "MSE = 0.48905430464335814\n",
      "MSE = 0.48908792711283533\n",
      "MSE = 0.48905985371019683\n",
      "MSE = 0.4889841225162158\n",
      "MSE = 0.48892648201753236\n",
      "MSE = 0.48887085273419134\n",
      "MSE = 0.48897070756255623\n",
      "MSE = 0.48887571497441984\n",
      "MSE = 0.48887009196158854\n",
      "MSE = 0.4888651126647079\n",
      "MSE = 0.48884194042137374\n",
      "MSE = 0.4888230738908739\n",
      "MSE = 0.49427834551400984\n",
      "MSE = 0.48882032338070797\n",
      "MSE = 0.4887902321138364\n",
      "MSE = 0.4887653604390899\n",
      "MSE = 0.48874222017774177\n",
      "MSE = 0.4887323492530015\n",
      "MSE = 0.48873022782833886\n",
      "MSE = 0.4888389767034006\n",
      "MSE = 0.4887341234524336\n",
      "MSE = 0.4887381407546056\n",
      "MSE = 0.4887376381496787\n",
      "MSE = 0.48872796280134645\n",
      "MSE = 0.4887025546570455\n",
      "MSE = 0.4886622924311549\n",
      "MSE = 0.4886249156717958\n",
      "MSE = 0.4886086131869865\n",
      "MSE = 0.48859948832017935\n",
      "MSE = 0.4885958337278637\n",
      "MSE = 0.48858887427385933\n",
      "MSE = 0.48857006927411656\n",
      "MSE = 0.4885579884960817\n",
      "MSE = 0.4885408970008527\n",
      "MSE = 0.48866801747003435\n",
      "MSE = 0.48849687591205193\n",
      "MSE = 0.48845255925408193\n",
      "MSE = 0.4884105789969622\n",
      "MSE = 0.4883900937915871\n",
      "MSE = 0.4883834369219112\n",
      "MSE = 0.5228425468674694\n",
      "MSE = 0.4883847144638297\n",
      "MSE = 0.48836791142917096\n",
      "MSE = 0.4883551471320854\n",
      "MSE = 0.48832589897140677\n",
      "MSE = 0.48832496712263856\n",
      "MSE = 0.48829557042013355\n",
      "MSE = 0.4882850694686716\n",
      "MSE = 0.4882785861312093\n",
      "MSE = 0.4882748559074186\n",
      "MSE = 0.48827021679893634\n",
      "MSE = 0.48824297931341\n",
      "MSE = 0.5111598180965913\n",
      "MSE = 0.48824012003581707\n",
      "MSE = 0.48822020656298704\n",
      "MSE = 0.4881639445289662\n",
      "MSE = 0.48813285676956897\n",
      "MSE = 0.4880788050948735\n",
      "MSE = 0.4889886474985991\n",
      "MSE = 0.4880665631147989\n",
      "MSE = 0.4880617141545999\n",
      "MSE = 0.4880615252533829\n",
      "MSE = 0.48806052803281424\n",
      "MSE = 0.4880449372292785\n",
      "MSE = 0.488024946436586\n",
      "MSE = 0.4879663810894592\n",
      "MSE = 0.4879306104968355\n",
      "MSE = 0.48790047547478976\n",
      "MSE = 0.4879010330013434\n",
      "MSE = 0.4879000388183473\n",
      "MSE = 0.48789659939658725\n",
      "MSE = 0.48789175845243044\n",
      "MSE = 0.48787645296688054\n",
      "MSE = 0.4878552904894026\n",
      "MSE = 0.4878272972302826\n",
      "MSE = 0.4877922307784411\n",
      "MSE = 0.487782130527399\n",
      "MSE = 0.48777486517398644\n",
      "MSE = 0.48784123393114176\n",
      "MSE = 0.4877781486183426\n",
      "MSE = 0.487762471698355\n",
      "MSE = 0.487733126580165\n",
      "MSE = 0.487794185980107\n",
      "MSE = 0.4877278034307878\n",
      "MSE = 0.48770016175451114\n",
      "MSE = 0.48759835908579235\n",
      "MSE = 0.48752459485286703\n",
      "MSE = 0.487724796305081\n",
      "MSE = 0.48749652908817953\n",
      "MSE = 0.4874986143621317\n",
      "MSE = 0.487496648277911\n",
      "MSE = 0.4874908496549471\n",
      "MSE = 0.49418752341998845\n",
      "MSE = 0.487496493802452\n",
      "MSE = 0.4874691325162814\n",
      "MSE = 0.4874728235310946\n",
      "MSE = 0.4874285764856057\n",
      "MSE = 0.4873741567785806\n",
      "MSE = 0.4873449515611601\n",
      "MSE = 0.4873937123627903\n",
      "MSE = 0.48734033784156844\n",
      "MSE = 0.4873295634730671\n",
      "MSE = 0.4873135071893847\n",
      "MSE = 0.48768932727559294\n",
      "MSE = 0.48730170348431406\n",
      "MSE = 0.4872759812221906\n",
      "MSE = 0.48750630142699725\n",
      "MSE = 0.4871956044433526\n",
      "MSE = 0.48716203907716354\n",
      "MSE = 0.5017366559351382\n",
      "MSE = 0.4871072035568226\n",
      "MSE = 0.48710066659043355\n",
      "MSE = 0.48700969761522866\n",
      "MSE = 0.48690896418431406\n",
      "MSE = 0.4869048249397482\n",
      "MSE = 0.4868974075779151\n",
      "MSE = 0.48692728163214494\n",
      "MSE = 0.4868948316829954\n",
      "MSE = 0.4868893745496438\n",
      "MSE = 0.48688177368493357\n",
      "MSE = 0.4868872469749429\n",
      "MSE = 0.4868791519799545\n",
      "MSE = 0.4868533331355304\n",
      "MSE = 0.4868224264965512\n",
      "MSE = 0.5083401875943987\n",
      "MSE = 0.4868222366409671\n",
      "MSE = 0.48679365094734767\n",
      "MSE = 0.4867817247411086\n",
      "MSE = 0.48677924752293805\n",
      "MSE = 0.48677775377278687\n",
      "MSE = 0.4867556241252697\n",
      "MSE = 0.48673684834438535\n",
      "MSE = 0.48668024583944347\n",
      "MSE = 0.4866971701165663\n",
      "MSE = 0.48665927513396484\n",
      "MSE = 0.4866509335712306\n",
      "MSE = 0.48664759048392736\n",
      "MSE = 0.4866458899403491\n",
      "MSE = 0.4866444620577286\n",
      "MSE = 0.48663828246892465\n",
      "MSE = 0.48665330494697395\n",
      "MSE = 0.4866413368660772\n",
      "MSE = 0.48663805634674073\n",
      "MSE = 0.4866375991336598\n",
      "MSE = 0.48663246298114093\n",
      "MSE = 0.48662100862945123\n",
      "MSE = 0.4866191775424683\n",
      "MSE = 0.4866100667334881\n",
      "MSE = 0.48659369403459063\n",
      "MSE = 0.4866659510993399\n",
      "MSE = 0.48660035983632466\n",
      "MSE = 0.48657070268733993\n",
      "MSE = 0.48656043393866866\n",
      "MSE = 0.48736162882634587\n",
      "MSE = 0.4865561637377015\n",
      "MSE = 0.4865473762107855\n",
      "MSE = 0.4874903303192991\n",
      "MSE = 0.4865425105790287\n",
      "MSE = 0.4865361117070019\n",
      "MSE = 0.48649714446233205\n",
      "MSE = 0.4866252103264569\n",
      "MSE = 0.4864943300673871\n",
      "MSE = 0.48644165877596235\n",
      "MSE = 0.4864317791759709\n",
      "MSE = 0.48649628713104487\n",
      "MSE = 0.4864334792042215\n",
      "MSE = 0.48641519715694703\n",
      "MSE = 0.4864075484142294\n",
      "MSE = 0.4863834011528589\n",
      "MSE = 0.4865053255845565\n",
      "MSE = 0.48636989112431334\n",
      "MSE = 0.4863536719499695\n",
      "MSE = 0.4863218303327959\n",
      "MSE = 0.4870370852723602\n",
      "MSE = 0.4863178948169368\n",
      "MSE = 0.4863198010687987\n",
      "MSE = 0.48631199127647773\n",
      "MSE = 0.48628330788100566\n",
      "MSE = 0.48619644448709026\n",
      "MSE = 0.48619054335955614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.4861635125738204\n",
      "MSE = 0.48616104487027434\n",
      "MSE = 0.4861306773880942\n",
      "MSE = 0.4860482545078755\n",
      "MSE = 0.48605868276970937\n",
      "MSE = 0.48603988844919266\n",
      "MSE = 0.48601774731602626\n",
      "MSE = 0.48597950551428876\n",
      "MSE = 0.48599066385574613\n",
      "MSE = 0.48595574105758527\n",
      "MSE = 0.485923036312163\n",
      "MSE = 0.48586620221188553\n",
      "MSE = 0.4858610002169216\n",
      "MSE = 0.4858670151844048\n",
      "MSE = 0.4858647824709476\n",
      "MSE = 0.48585101033423955\n",
      "MSE = 0.4858225854315414\n",
      "MSE = 0.48578469585362805\n",
      "MSE = 0.4857804810710911\n",
      "MSE = 0.48576698853806805\n",
      "MSE = 0.48575046948615425\n",
      "MSE = 0.4857309193122138\n",
      "MSE = 0.48571945099215713\n",
      "MSE = 0.48571903619922535\n",
      "MSE = 0.4857135532605047\n",
      "MSE = 0.485702862709649\n",
      "MSE = 0.48568246685156174\n",
      "MSE = 0.4856409819280203\n",
      "MSE = 0.48582233473115904\n",
      "MSE = 0.4856522076332845\n",
      "MSE = 0.48700371198544323\n",
      "MSE = 0.4856538899431441\n",
      "MSE = 0.4856459997585467\n",
      "MSE = 0.4856377981714964\n",
      "MSE = 0.48563315473221125\n",
      "MSE = 0.4856225006756062\n",
      "MSE = 0.4855400525723803\n",
      "MSE = 0.48550089796656265\n",
      "MSE = 0.48549090069398043\n",
      "MSE = 0.48549271098893565\n",
      "MSE = 0.48549107226773874\n",
      "MSE = 0.4854767973510491\n",
      "MSE = 0.48545197411033747\n",
      "MSE = 0.4857266614244859\n",
      "MSE = 0.48544845407715714\n",
      "MSE = 0.4854361479380525\n",
      "MSE = 0.4854085894626833\n",
      "MSE = 0.48536909227267017\n",
      "MSE = 0.485454056227298\n",
      "MSE = 0.48536342685381606\n",
      "MSE = 0.4853463840429721\n",
      "MSE = 0.48570610089755994\n",
      "MSE = 0.48535370773675174\n",
      "MSE = 0.48533644657471586\n",
      "MSE = 0.48532334842887465\n",
      "MSE = 0.485326588228614\n",
      "MSE = 0.4853284078612238\n",
      "MSE = 0.4853512324280614\n",
      "MSE = 0.4853233497587871\n",
      "MSE = 0.4853143508083054\n",
      "MSE = 0.48528567657450267\n",
      "MSE = 0.48525284958294906\n",
      "MSE = 0.4852222023688993\n",
      "MSE = 0.4851814016061022\n",
      "MSE = 0.4893911003446747\n",
      "MSE = 0.4851879741131343\n",
      "MSE = 0.4851565327948125\n",
      "MSE = 0.48514350339328316\n",
      "MSE = 0.4851266422617674\n",
      "MSE = 0.4850591477711495\n",
      "MSE = 0.48509296275711233\n",
      "MSE = 0.4850576852015331\n",
      "MSE = 0.48503704268089376\n",
      "MSE = 0.4850257142097647\n",
      "MSE = 0.48530413999303645\n",
      "MSE = 0.4850348784549859\n",
      "MSE = 0.48500597461686024\n",
      "MSE = 0.48493574710892307\n",
      "MSE = 0.4849581526100946\n",
      "MSE = 0.4849207940442807\n",
      "MSE = 0.4849055179316883\n",
      "MSE = 0.48489449443876964\n",
      "MSE = 0.4848879864689203\n",
      "MSE = 0.48489872109722615\n",
      "MSE = 0.48488448312274424\n",
      "MSE = 0.48486863694109955\n",
      "MSE = 0.48486296809874535\n",
      "MSE = 0.4848660137195929\n",
      "MSE = 0.4848623624373422\n",
      "MSE = 0.4848645199830407\n",
      "MSE = 0.4852457853011997\n",
      "MSE = 0.4848711420217867\n",
      "MSE = 0.4848768065036449\n",
      "MSE = 0.48924693215174214\n",
      "MSE = 0.4848634457588917\n",
      "MSE = 0.484857909611386\n",
      "MSE = 0.4848258594904212\n",
      "MSE = 0.4848030641932114\n",
      "MSE = 0.48477910329295665\n",
      "MSE = 0.484761036709668\n",
      "MSE = 0.48475727227117943\n",
      "MSE = 0.48475021130830503\n",
      "MSE = 0.4847058401700718\n",
      "MSE = 0.4847222449644745\n",
      "MSE = 0.4846995284197524\n",
      "MSE = 0.4846795602331752\n",
      "MSE = 0.4846500563324171\n",
      "MSE = 0.48611315641530584\n",
      "MSE = 0.4846542369117447\n",
      "MSE = 0.48464988316390195\n",
      "MSE = 0.48464555945872434\n",
      "MSE = 0.4846422592714859\n",
      "MSE = 0.4848222813785704\n",
      "MSE = 0.48464016724672515\n",
      "MSE = 0.4846385569045252\n",
      "MSE = 0.48463685869790746\n",
      "MSE = 0.48463493115083633\n",
      "MSE = 0.4846310876821544\n",
      "MSE = 0.48461715385154536\n",
      "MSE = 0.48460066164533844\n",
      "MSE = 0.5152731781255493\n",
      "MSE = 0.4846019523376032\n",
      "MSE = 0.4845913815883129\n",
      "MSE = 0.48458480063942005\n",
      "MSE = 0.4845750553341447\n",
      "MSE = 0.484565165712691\n",
      "MSE = 0.48457911585295693\n",
      "MSE = 0.4845553616749642\n",
      "MSE = 0.4845447850136578\n",
      "MSE = 0.4845346520979097\n",
      "MSE = 0.48453209009605414\n",
      "MSE = 0.48452914594393204\n",
      "MSE = 0.4847175552284465\n",
      "MSE = 0.484522716514856\n",
      "MSE = 0.48451402110113256\n",
      "MSE = 0.48451271843925175\n",
      "MSE = 0.48450254012683314\n",
      "MSE = 0.4844829172592396\n",
      "MSE = 0.48447499476675754\n",
      "MSE = 0.48443085019210835\n",
      "MSE = 0.4845086069007131\n",
      "MSE = 0.48443239459875354\n",
      "MSE = 0.48441273045605093\n",
      "MSE = 0.48437594528244576\n",
      "MSE = 0.4843761026991189\n",
      "MSE = 0.48430235045733006\n",
      "MSE = 0.48422526107501107\n",
      "MSE = 0.48806911556396015\n",
      "MSE = 0.48422550891567234\n",
      "MSE = 0.48417803227823875\n",
      "MSE = 0.48413718121350163\n",
      "MSE = 0.49086801288280046\n",
      "MSE = 0.48410521420375613\n",
      "MSE = 0.48408093439841976\n",
      "MSE = 0.4863414600880655\n",
      "MSE = 0.48408003536204064\n",
      "MSE = 0.4840774459552521\n",
      "MSE = 0.486287086530929\n",
      "MSE = 0.4840698880836581\n",
      "MSE = 0.48407124315037164\n",
      "MSE = 0.48407093248369076\n",
      "MSE = 0.484063794275079\n",
      "MSE = 0.4840561761216111\n",
      "MSE = 0.4840544349404935\n",
      "MSE = 0.48404021012682474\n",
      "MSE = 0.4840235626605615\n",
      "MSE = 0.484011224686247\n",
      "MSE = 0.48395612430585605\n",
      "MSE = 0.4839549815904196\n",
      "MSE = 0.4839406200541165\n",
      "MSE = 0.4839200371409917\n",
      "MSE = 0.4839124146872738\n",
      "MSE = 0.4839793543447256\n",
      "MSE = 0.48391728157531366\n",
      "MSE = 0.4839045754184771\n",
      "MSE = 0.4838562876923514\n",
      "MSE = 0.4838187961572406\n",
      "MSE = 0.48379380200116273\n",
      "MSE = 0.4997829428534622\n",
      "MSE = 0.4837872344036835\n",
      "MSE = 0.483786557619359\n",
      "MSE = 0.4837753208025541\n",
      "MSE = 0.48371833659761826\n",
      "MSE = 0.48558260004427906\n",
      "MSE = 0.48371717460968056\n",
      "MSE = 0.4837186987892361\n",
      "MSE = 0.48596274308260357\n",
      "MSE = 0.4837079729404428\n",
      "MSE = 0.48371646860434475\n",
      "MSE = 0.4837141723181038\n",
      "MSE = 0.4837045136401063\n",
      "MSE = 0.4836904904489524\n",
      "MSE = 0.48367724264477907\n",
      "MSE = 0.48363397138360364\n",
      "MSE = 0.4836464975091798\n",
      "MSE = 0.4836395223838506\n",
      "MSE = 0.48363201679578516\n",
      "MSE = 0.48362847051632474\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-31391de0c717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmin_l_bfgs_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderivative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 193\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    194\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    195\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-31391de0c717>\u001b[0m in \u001b[0;36mderivative\u001b[0;34m(theta, labels, lamb)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mdalpha\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mdUserBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mdItemBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lamb_values = [1e-6,1e-5,1e-4,1e-3,0.01]\n",
    "MSE_valid =[]\n",
    "best_theta = []\n",
    "best_MSE = 0\n",
    "\n",
    "for lamb in lamb_values:\n",
    "    print(\"Lambda = {}\".format(lamb))\n",
    "  \n",
    "    theta_init = [alpha] + [0.0]*(nUsers+nItems) +  \\\n",
    "            [random.random() * 0.1 - 0.05 for k in range(K*(nUsers+nItems))]\n",
    "    unpack(theta_init)\n",
    "    \n",
    "    def cost(theta, labels, lamb):\n",
    "        unpack(theta)\n",
    "        predictions = [prediction(user, book) for user,book in Xtrain]\n",
    "        cost = MSE(predictions, labels)\n",
    "        print(\"MSE = \" + str(cost))\n",
    "        for u in users:\n",
    "            cost += lamb*userBiases[u]**2\n",
    "            for k in range(K):\n",
    "                cost += lamb*userGamma[u][k]**2\n",
    "        for i in items:\n",
    "            cost += lamb*itemBiases[i]**2\n",
    "            for k in range(K):\n",
    "                cost += lamb*itemGamma[i][k]**2\n",
    "        return cost\n",
    "\n",
    "    def derivative(theta, labels, lamb):\n",
    "        unpack(theta)\n",
    "        N = len(dataset)\n",
    "        dalpha = 0\n",
    "        dUserBiases = defaultdict(float)\n",
    "        dItemBiases = defaultdict(float)\n",
    "        dUserGamma = {}\n",
    "        dItemGamma = {}\n",
    "        for u in ratingsPerUser:\n",
    "            dUserGamma[u] = [0.0 for k in range(K)]\n",
    "        for i in ratingsPerItem:\n",
    "            dItemGamma[i] = [0.0 for k in range(K)]\n",
    "        for value in zip(Xtrain,ytrain):\n",
    "            x,rating = value\n",
    "            u = x[0]\n",
    "            i = x[1]\n",
    "            pred = prediction(u, i)\n",
    "            diff = pred - rating\n",
    "            dalpha += 2/N*diff\n",
    "            dUserBiases[u] += 2/N*diff\n",
    "            dItemBiases[i] += 2/N*diff\n",
    "            for k in range(K):\n",
    "                dUserGamma[u][k] += 2/N*itemGamma[i][k]*diff\n",
    "                dItemGamma[i][k] += 2/N*userGamma[u][k]*diff\n",
    "        for u in userBiases:\n",
    "            dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "            for k in range(K):\n",
    "                dUserGamma[u][k] += 2*lamb*userGamma[u][k]\n",
    "        for i in itemBiases:\n",
    "            dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "            for k in range(K):\n",
    "                dItemGamma[i][k] += 2*lamb*itemGamma[i][k]\n",
    "        dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "        for u in users:\n",
    "            dtheta += dUserGamma[u]\n",
    "        for i in items:\n",
    "            dtheta += dItemGamma[i]\n",
    "        return numpy.array(dtheta)\n",
    "    \n",
    "    theta,_,_ = scipy.optimize.fmin_l_bfgs_b(cost, theta_init, derivative, args = (ytrain, lamb))\n",
    "\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(user, book) for user,book in Xvalid]\n",
    "    cost = MSE(predictions, yvalid)\n",
    "    \n",
    "    if best_MSE is 0:\n",
    "        best_MSE = cost\n",
    "        best_theta = theta\n",
    "        print(\"Save best theta...\")\n",
    "    else:\n",
    "        if cost < best_MSE:\n",
    "            best_MSE = cost\n",
    "            best_theta = theta\n",
    "            print(\"Save best theta...\")\n",
    "    \n",
    "    MSE_valid.append(cost)\n",
    "    print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
