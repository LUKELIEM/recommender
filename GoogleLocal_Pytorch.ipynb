{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd \n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bias_Only(nn.Module):\n",
    "    # itr = 0\n",
    "    \n",
    "    def __init__(self, n_user, n_item, mean=0):\n",
    "        super(Bias_Only, self).__init__()\n",
    "\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        \n",
    "        # alpha and betas (users and items)\n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "    \n",
    "    def forward(self, train_x):\n",
    "        item_id = train_x[:, 0]\n",
    "        user_id = train_x[:, 1]\n",
    "        \n",
    "        # Pull out biases\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        prediction = (self.bias + bias_user + bias_item)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "        \n",
    "        return loss_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    itr = 0\n",
    "    \n",
    "    def __init__(self, n_user, n_item, k=1, c_vector=1.0, c_bias=1.0, writer=None):\n",
    "        super(MF, self).__init__()\n",
    "        self.writer = writer\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.c_bias = c_bias\n",
    "        self.c_vector = c_vector\n",
    "        \n",
    "        # gammas (users and items)\n",
    "        self.user = nn.Embedding(n_user, k)\n",
    "        self.item = nn.Embedding(n_item, k)\n",
    "        \n",
    "        # alpha and betas (users and items)\n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, train_x):\n",
    "        item_id = train_x[:, 0]\n",
    "        user_id = train_x[:, 1]\n",
    "        vector_user = self.user(user_id)\n",
    "        vector_item = self.item(item_id)\n",
    "        \n",
    "        # Pull out biases\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        biases = (self.bias + bias_user + bias_item)\n",
    "        \n",
    "        ui_interaction = torch.sum(vector_user * vector_item, dim=1)\n",
    "        \n",
    "        # Add bias prediction to the interaction prediction\n",
    "        prediction = ui_interaction + biases\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "        \n",
    "        # BUG - PyTorch optimizer already takes care of regularization!!!\n",
    "        \n",
    "        # prior_bias_user =  l2_regularize(self.bias_user.weight) * self.c_bias\n",
    "        # prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias\n",
    "        # prior_user =  l2_regularize(self.user.weight) * self.c_vector\n",
    "        # prior_item = l2_regularize(self.item.weight) * self.c_vector\n",
    "        # total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item\n",
    "        \n",
    "        total = loss_mse\n",
    "        \n",
    "        for name, var in locals().items():\n",
    "            if type(var) is torch.Tensor and var.nelement() == 1 and self.writer is not None:\n",
    "                self.writer.add_scalar(name, var, self.itr)\n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user_id and place_id --> user_idx, place_idx\n",
    "\n",
    "To perform matrix factorization, we need to convert user_id and place_id into their index in the interaction matrix. This has already been done in the Notebook GoogleLocal_reformat.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews based on (user_id, place_id, rating, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11453845 entries, 0 to 11453844\n",
      "Data columns (total 4 columns):\n",
      "gPlusPlaceId      object\n",
      "gPlusUserId       object\n",
      "rating            float64\n",
      "unixReviewTime    object\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 349.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gPlusPlaceId</th>\n",
       "      <th>gPlusUserId</th>\n",
       "      <th>rating</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108103314380004200232</td>\n",
       "      <td>100000010817154263736</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1372686659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102194128241608748649</td>\n",
       "      <td>100000013500285534661</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1342870724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101409858828175402384</td>\n",
       "      <td>100000021336848867366</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1390653513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101477177500158511502</td>\n",
       "      <td>100000021336848867366</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1389187706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106994170641063333085</td>\n",
       "      <td>100000021336848867366</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1390486279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            gPlusPlaceId            gPlusUserId  rating unixReviewTime\n",
       "0  108103314380004200232  100000010817154263736     3.0     1372686659\n",
       "1  102194128241608748649  100000013500285534661     5.0     1342870724\n",
       "2  101409858828175402384  100000021336848867366     5.0     1390653513\n",
       "3  101477177500158511502  100000021336848867366     5.0     1389187706\n",
       "4  106994170641063333085  100000021336848867366     4.0     1390486279"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5054567 3116785\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../datasets/google_local/reviews.csv\")\n",
    "display(data.info())\n",
    "display(data.head())\n",
    "\n",
    "n_user = len(data['gPlusUserId'].unique())\n",
    "n_place = len(data['gPlusPlaceId'].unique())\n",
    "\n",
    "print(n_user,n_place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews based on (user_idx, place_idx, rating, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11453845 entries, 0 to 11453844\n",
      "Data columns (total 4 columns):\n",
      "gPlusPlaceId      int64\n",
      "gPlusUserId       int64\n",
      "rating            float64\n",
      "unixReviewTime    object\n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 349.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gPlusPlaceId</th>\n",
       "      <th>gPlusUserId</th>\n",
       "      <th>rating</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1368311</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1372686659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>370282</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1342870724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>237940</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1390653513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>249417</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1389187706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1181533</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1390486279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gPlusPlaceId  gPlusUserId  rating unixReviewTime\n",
       "0       1368311            0     3.0     1372686659\n",
       "1        370282            1     5.0     1342870724\n",
       "2        237940            2     5.0     1390653513\n",
       "3        249417            2     5.0     1389187706\n",
       "4       1181533            2     4.0     1390486279"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5054567 3116785\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../datasets/google_local/reviews_reformatted.csv\")\n",
    "display(data.info())\n",
    "display(data.head())\n",
    "\n",
    "n_user = len(data['gPlusUserId'].unique())\n",
    "n_item = len(data['gPlusPlaceId'].unique())\n",
    "\n",
    "print(n_user,n_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularize(array):\n",
    "    loss = torch.sum(array ** 2.0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gPlusPlaceId</th>\n",
       "      <th>gPlusUserId</th>\n",
       "      <th>rating</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1790517</td>\n",
       "      <td>2755302</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1360610950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2125285</td>\n",
       "      <td>74522</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1389741387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1323596</td>\n",
       "      <td>3431767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1327291970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019579</td>\n",
       "      <td>3822788</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1364904582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>912466</td>\n",
       "      <td>1995657</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1319107160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gPlusPlaceId  gPlusUserId  rating unixReviewTime\n",
       "0       1790517      2755302     4.0     1360610950\n",
       "1       2125285        74522     5.0     1389741387\n",
       "2       1323596      3431767     1.0     1327291970\n",
       "3       2019579      3822788     5.0     1364904582\n",
       "4        912466      1995657     4.0     1319107160"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the data\n",
    "shuffled_data = data.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training, Validation and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gPlusPlaceId</th>\n",
       "      <th>gPlusUserId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.017692e+06</td>\n",
       "      <td>8.017692e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.558347e+06</td>\n",
       "      <td>2.514258e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.996626e+05</td>\n",
       "      <td>1.457303e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.795788e+05</td>\n",
       "      <td>1.250433e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.558165e+06</td>\n",
       "      <td>2.509377e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.336931e+06</td>\n",
       "      <td>3.773621e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.116784e+06</td>\n",
       "      <td>5.054566e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       gPlusPlaceId   gPlusUserId\n",
       "count  8.017692e+06  8.017692e+06\n",
       "mean   1.558347e+06  2.514258e+06\n",
       "std    8.996626e+05  1.457303e+06\n",
       "min    0.000000e+00  0.000000e+00\n",
       "25%    7.795788e+05  1.250433e+06\n",
       "50%    1.558165e+06  2.509377e+06\n",
       "75%    2.336931e+06  3.773621e+06\n",
       "max    3.116784e+06  5.054566e+06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.017692e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.047441e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.195633e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rating\n",
       "count  8.017692e+06\n",
       "mean   4.047441e+00\n",
       "std    1.195633e+00\n",
       "min    0.000000e+00\n",
       "25%    3.000000e+00\n",
       "50%    4.000000e+00\n",
       "75%    5.000000e+00\n",
       "max    5.000000e+00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gPlusPlaceId</th>\n",
       "      <th>gPlusUserId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.718077e+06</td>\n",
       "      <td>1.718077e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.558657e+06</td>\n",
       "      <td>2.512333e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.002781e+05</td>\n",
       "      <td>1.456864e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.795530e+05</td>\n",
       "      <td>1.249378e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.558435e+06</td>\n",
       "      <td>2.506407e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.338080e+06</td>\n",
       "      <td>3.772192e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.116779e+06</td>\n",
       "      <td>5.054565e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       gPlusPlaceId   gPlusUserId\n",
       "count  1.718077e+06  1.718077e+06\n",
       "mean   1.558657e+06  2.512333e+06\n",
       "std    9.002781e+05  1.456864e+06\n",
       "min    0.000000e+00  2.000000e+00\n",
       "25%    7.795530e+05  1.249378e+06\n",
       "50%    1.558435e+06  2.506407e+06\n",
       "75%    2.338080e+06  3.772192e+06\n",
       "max    3.116779e+06  5.054565e+06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.718077e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.046968e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.195618e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rating\n",
       "count  1.718077e+06\n",
       "mean   4.046968e+00\n",
       "std    1.195618e+00\n",
       "min    0.000000e+00\n",
       "25%    3.000000e+00\n",
       "50%    4.000000e+00\n",
       "75%    5.000000e+00\n",
       "max    5.000000e+00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gPlusPlaceId</th>\n",
       "      <th>gPlusUserId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.718076e+06</td>\n",
       "      <td>1.718076e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.558454e+06</td>\n",
       "      <td>2.513663e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.998956e+05</td>\n",
       "      <td>1.457995e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.797642e+05</td>\n",
       "      <td>1.248296e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.557790e+06</td>\n",
       "      <td>2.507972e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.338689e+06</td>\n",
       "      <td>3.774438e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.116784e+06</td>\n",
       "      <td>5.054566e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       gPlusPlaceId   gPlusUserId\n",
       "count  1.718076e+06  1.718076e+06\n",
       "mean   1.558454e+06  2.513663e+06\n",
       "std    8.998956e+05  1.457995e+06\n",
       "min    1.000000e+00  1.000000e+00\n",
       "25%    7.797642e+05  1.248296e+06\n",
       "50%    1.557790e+06  2.507972e+06\n",
       "75%    2.338689e+06  3.774438e+06\n",
       "max    3.116784e+06  5.054566e+06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.718076e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.047148e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.196265e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rating\n",
       "count  1.718076e+06\n",
       "mean   4.047148e+00\n",
       "std    1.196265e+00\n",
       "min    0.000000e+00\n",
       "25%    3.000000e+00\n",
       "50%    4.000000e+00\n",
       "75%    5.000000e+00\n",
       "max    5.000000e+00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11453845 8017692 1718077 1718076\n"
     ]
    }
   ],
   "source": [
    "N = shuffled_data.index.size\n",
    "\n",
    "train_split = int(N * 0.70)\n",
    "valid_split =  int(N * 0.85)\n",
    "\n",
    "train_x = shuffled_data.loc[:train_split, 'gPlusPlaceId':'gPlusUserId']\n",
    "train_y = shuffled_data.loc[:train_split, 'rating':'rating']\n",
    "valid_x = shuffled_data.loc[train_split+1:valid_split, 'gPlusPlaceId':'gPlusUserId']\n",
    "valid_y = shuffled_data.loc[train_split+1:valid_split, 'rating':'rating']\n",
    "test_x = shuffled_data.loc[valid_split+1:, 'gPlusPlaceId':'gPlusUserId']\n",
    "test_y = shuffled_data.loc[valid_split+1:, 'rating':'rating']\n",
    "\n",
    "display(train_x.describe())\n",
    "display(train_y.describe())\n",
    "display(valid_x.describe())\n",
    "display(valid_y.describe())\n",
    "display(test_x.describe())\n",
    "display(test_y.describe())\n",
    "\n",
    "print(N, train_x.index.size, valid_x.index.size,test_x.index.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lr = 1e-2\n",
    "lamb = 1e-6\n",
    "batch_size = 1024\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "print (cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Iteration[0] Training Loss: 12.80\n",
      "Epoch[0] Validation Loss: 12.643 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.60\n",
      "Epoch[0] Validation Loss: 1.449 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.35\n",
      "Epoch[0] Validation Loss: 1.360 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.26\n",
      "Epoch[0] Validation Loss: 1.310 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[4000] Training Loss: 1.24\n",
      "Epoch[0] Validation Loss: 1.280 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[5000] Training Loss: 1.23\n",
      "Epoch[0] Validation Loss: 1.262 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[6000] Training Loss: 1.19\n",
      "Epoch[0] Validation Loss: 1.252 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[7000] Training Loss: 1.35\n",
      "Epoch[0] Validation Loss: 1.246 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.54\n",
      "Epoch[1] Validation Loss: 1.243 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 1.07\n",
      "Epoch[1] Validation Loss: 1.252 \n",
      "Epoch[1] Iteration[2000] Training Loss: 1.26\n",
      "Epoch[1] Validation Loss: 1.253 \n",
      "Epoch[1] Iteration[3000] Training Loss: 1.25\n",
      "Epoch[1] Validation Loss: 1.250 \n",
      "Epoch[1] Iteration[4000] Training Loss: 1.07\n",
      "Epoch[1] Validation Loss: 1.246 \n",
      "Epoch[1] Iteration[5000] Training Loss: 1.33\n",
      "Epoch[1] Validation Loss: 1.243 \n",
      "Epoch[1] Iteration[6000] Training Loss: 1.29\n",
      "Epoch[1] Validation Loss: 1.241 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[7000] Training Loss: 1.23\n",
      "Epoch[1] Validation Loss: 1.240 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 0.93\n",
      "Epoch[2] Validation Loss: 1.238 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 1.19\n",
      "Epoch[2] Validation Loss: 1.249 \n",
      "Epoch[2] Iteration[2000] Training Loss: 1.00\n",
      "Epoch[2] Validation Loss: 1.252 \n",
      "Epoch[2] Iteration[3000] Training Loss: 1.09\n",
      "Epoch[2] Validation Loss: 1.249 \n",
      "Epoch[2] Iteration[4000] Training Loss: 1.31\n",
      "Epoch[2] Validation Loss: 1.246 \n",
      "Epoch[2] Iteration[5000] Training Loss: 1.20\n",
      "Epoch[2] Validation Loss: 1.243 \n",
      "Epoch[2] Iteration[6000] Training Loss: 1.25\n",
      "Epoch[2] Validation Loss: 1.241 \n",
      "Epoch[2] Iteration[7000] Training Loss: 1.15\n",
      "Epoch[2] Validation Loss: 1.240 \n",
      "Epoch[3] Iteration[0] Training Loss: 1.04\n",
      "Epoch[3] Validation Loss: 1.238 \n",
      "Save best theta...\n",
      "Epoch[3] Iteration[1000] Training Loss: 1.15\n",
      "Epoch[3] Validation Loss: 1.249 \n",
      "Epoch[3] Iteration[2000] Training Loss: 1.21\n",
      "Epoch[3] Validation Loss: 1.251 \n",
      "Epoch[3] Iteration[3000] Training Loss: 1.23\n",
      "Epoch[3] Validation Loss: 1.248 \n",
      "Epoch[3] Iteration[4000] Training Loss: 1.18\n",
      "Epoch[3] Validation Loss: 1.245 \n",
      "Epoch[3] Iteration[5000] Training Loss: 1.21\n",
      "Epoch[3] Validation Loss: 1.243 \n",
      "Epoch[3] Iteration[6000] Training Loss: 1.28\n",
      "Epoch[3] Validation Loss: 1.241 \n",
      "Epoch[3] Iteration[7000] Training Loss: 1.20\n",
      "Epoch[3] Validation Loss: 1.239 \n",
      "Epoch[4] Iteration[0] Training Loss: 1.05\n",
      "Epoch[4] Validation Loss: 1.238 \n",
      "Epoch[4] Iteration[1000] Training Loss: 0.96\n",
      "Epoch[4] Validation Loss: 1.249 \n",
      "Epoch[4] Iteration[2000] Training Loss: 1.26\n",
      "Epoch[4] Validation Loss: 1.252 \n",
      "Epoch[4] Iteration[3000] Training Loss: 1.07\n",
      "Epoch[4] Validation Loss: 1.250 \n",
      "Epoch[4] Iteration[4000] Training Loss: 1.30\n",
      "Epoch[4] Validation Loss: 1.246 \n",
      "Epoch[4] Iteration[5000] Training Loss: 1.16\n",
      "Epoch[4] Validation Loss: 1.242 \n",
      "Epoch[4] Iteration[6000] Training Loss: 1.15\n",
      "Epoch[4] Validation Loss: 1.240 \n",
      "Epoch[4] Iteration[7000] Training Loss: 1.31\n",
      "Epoch[4] Validation Loss: 1.239 \n",
      "Epoch[5] Iteration[0] Training Loss: 1.21\n",
      "Epoch[5] Validation Loss: 1.239 \n",
      "Epoch[5] Iteration[1000] Training Loss: 1.27\n",
      "Epoch[5] Validation Loss: 1.249 \n",
      "Epoch[5] Iteration[2000] Training Loss: 0.99\n",
      "Epoch[5] Validation Loss: 1.252 \n",
      "Epoch[5] Iteration[3000] Training Loss: 1.00\n",
      "Epoch[5] Validation Loss: 1.249 \n",
      "Epoch[5] Iteration[4000] Training Loss: 1.21\n",
      "Epoch[5] Validation Loss: 1.245 \n",
      "Epoch[5] Iteration[5000] Training Loss: 1.16\n",
      "Epoch[5] Validation Loss: 1.243 \n",
      "Epoch[5] Iteration[6000] Training Loss: 1.37\n",
      "Epoch[5] Validation Loss: 1.241 \n",
      "Epoch[5] Iteration[7000] Training Loss: 1.30\n",
      "Epoch[5] Validation Loss: 1.240 \n",
      "Epoch[6] Iteration[0] Training Loss: 0.78\n",
      "Epoch[6] Validation Loss: 1.238 \n",
      "Save best theta...\n",
      "Epoch[6] Iteration[1000] Training Loss: 1.20\n",
      "Epoch[6] Validation Loss: 1.250 \n",
      "Epoch[6] Iteration[2000] Training Loss: 1.21\n",
      "Epoch[6] Validation Loss: 1.251 \n",
      "Epoch[6] Iteration[3000] Training Loss: 1.16\n",
      "Epoch[6] Validation Loss: 1.250 \n",
      "Epoch[6] Iteration[4000] Training Loss: 1.22\n",
      "Epoch[6] Validation Loss: 1.245 \n",
      "Epoch[6] Iteration[5000] Training Loss: 1.25\n",
      "Epoch[6] Validation Loss: 1.242 \n",
      "Epoch[6] Iteration[6000] Training Loss: 1.23\n",
      "Epoch[6] Validation Loss: 1.241 \n",
      "Epoch[6] Iteration[7000] Training Loss: 1.23\n",
      "Epoch[6] Validation Loss: 1.239 \n",
      "Epoch[7] Iteration[0] Training Loss: 1.24\n",
      "Epoch[7] Validation Loss: 1.238 \n",
      "Epoch[7] Iteration[1000] Training Loss: 0.82\n",
      "Epoch[7] Validation Loss: 1.250 \n",
      "Epoch[7] Iteration[2000] Training Loss: 0.93\n",
      "Epoch[7] Validation Loss: 1.251 \n",
      "Epoch[7] Iteration[3000] Training Loss: 1.26\n",
      "Epoch[7] Validation Loss: 1.249 \n",
      "Epoch[7] Iteration[4000] Training Loss: 1.13\n",
      "Epoch[7] Validation Loss: 1.245 \n",
      "Epoch[7] Iteration[5000] Training Loss: 1.34\n",
      "Epoch[7] Validation Loss: 1.242 \n",
      "Epoch[7] Iteration[6000] Training Loss: 1.30\n",
      "Epoch[7] Validation Loss: 1.240 \n",
      "Epoch[7] Iteration[7000] Training Loss: 1.27\n",
      "Epoch[7] Validation Loss: 1.239 \n",
      "Epoch[8] Iteration[0] Training Loss: 1.12\n",
      "Epoch[8] Validation Loss: 1.239 \n",
      "Epoch[8] Iteration[1000] Training Loss: 0.71\n",
      "Epoch[8] Validation Loss: 1.248 \n",
      "Epoch[8] Iteration[2000] Training Loss: 1.26\n",
      "Epoch[8] Validation Loss: 1.252 \n",
      "Epoch[8] Iteration[3000] Training Loss: 1.16\n",
      "Epoch[8] Validation Loss: 1.251 \n",
      "Epoch[8] Iteration[4000] Training Loss: 1.28\n",
      "Epoch[8] Validation Loss: 1.246 \n",
      "Epoch[8] Iteration[5000] Training Loss: 1.18\n",
      "Epoch[8] Validation Loss: 1.243 \n",
      "Epoch[8] Iteration[6000] Training Loss: 1.13\n",
      "Epoch[8] Validation Loss: 1.241 \n",
      "Epoch[8] Iteration[7000] Training Loss: 1.26\n",
      "Epoch[8] Validation Loss: 1.238 \n",
      "Epoch[9] Iteration[0] Training Loss: 1.19\n",
      "Epoch[9] Validation Loss: 1.237 \n",
      "Save best theta...\n",
      "Epoch[9] Iteration[1000] Training Loss: 0.92\n",
      "Epoch[9] Validation Loss: 1.248 \n",
      "Epoch[9] Iteration[2000] Training Loss: 1.05\n",
      "Epoch[9] Validation Loss: 1.251 \n",
      "Epoch[9] Iteration[3000] Training Loss: 1.04\n",
      "Epoch[9] Validation Loss: 1.249 \n",
      "Epoch[9] Iteration[4000] Training Loss: 1.30\n",
      "Epoch[9] Validation Loss: 1.245 \n",
      "Epoch[9] Iteration[5000] Training Loss: 1.25\n",
      "Epoch[9] Validation Loss: 1.242 \n",
      "Epoch[9] Iteration[6000] Training Loss: 1.24\n",
      "Epoch[9] Validation Loss: 1.240 \n",
      "Epoch[9] Iteration[7000] Training Loss: 1.29\n",
      "Epoch[9] Validation Loss: 1.238 \n",
      "Epoch[10] Iteration[0] Training Loss: 0.45\n",
      "Epoch[10] Validation Loss: 1.239 \n",
      "Epoch[10] Iteration[1000] Training Loss: 1.17\n",
      "Epoch[10] Validation Loss: 1.249 \n",
      "Epoch[10] Iteration[2000] Training Loss: 1.16\n",
      "Epoch[10] Validation Loss: 1.252 \n",
      "Epoch[10] Iteration[3000] Training Loss: 1.25\n",
      "Epoch[10] Validation Loss: 1.250 \n",
      "Epoch[10] Iteration[4000] Training Loss: 1.14\n",
      "Epoch[10] Validation Loss: 1.246 \n",
      "Epoch[10] Iteration[5000] Training Loss: 1.25\n",
      "Epoch[10] Validation Loss: 1.243 \n",
      "Epoch[10] Iteration[6000] Training Loss: 1.20\n",
      "Epoch[10] Validation Loss: 1.240 \n",
      "Epoch[10] Iteration[7000] Training Loss: 1.37\n",
      "Epoch[10] Validation Loss: 1.239 \n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "\n",
    "# This code utilizes ignite engine's create_supervised_trainer()\n",
    "# But we need something more basic\n",
    "\n",
    "# model = MF(n_user, n_item, k=k)\n",
    "\n",
    "model = Bias_Only(n_user, n_item)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lamb)\n",
    "\n",
    "def chunks(X, Y, size):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    starts = list(range(0, len(X), size))\n",
    "    shuffle(starts)\n",
    "    for i in starts:\n",
    "        yield (X[i:i + size], Y[i:i + size])\n",
    "        \n",
    "# To keep track to best hyperparameters and results\n",
    "best_loss = 0\n",
    "best = []\n",
    "\n",
    "losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(10+1):\n",
    "    \n",
    "    i = 0\n",
    "    for feature, target in chunks(np.array(train_x), np.array(train_y), batch_size):\n",
    "        # This zeros the gradients on every parameter. \n",
    "        # This is easy to miss and hard to troubleshoot.\n",
    "        optimizer.zero_grad()\n",
    "        # Convert \n",
    "        feature = Variable(torch.from_numpy(feature))\n",
    "        target = Variable(torch.from_numpy(target).type(torch.FloatTensor))\n",
    "        \n",
    "        if cuda:\n",
    "            feature = feature.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "        # model in training mode    \n",
    "        model.train()\n",
    "            \n",
    "        # Compute a prediction for these features\n",
    "        prediction = model.forward(feature)\n",
    "        # Compute a loss given what the true target outcome was\n",
    "        loss = model.loss(prediction, target)\n",
    "        # break\n",
    "        # Backpropagate: compute the direction / gradient every model parameter\n",
    "        # defined in your __init__ should move in in order to minimize this loss\n",
    "        # However, we're not actually changing these parameters, we're just storing\n",
    "        # how they should change.\n",
    "\n",
    "        loss.backward()\n",
    "        # Now take a step & update the model parameters. The optimizer uses the gradient at \n",
    "        # defined on every parameter in our model and nudges it in that direction.\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%1000 == 0 and epoch%1 == 0:\n",
    "            print(\"Epoch[{}] Iteration[{}] Training Loss: {:.2f}\".format(epoch, i, loss.data))\n",
    "\n",
    "        # Record the loss per example\n",
    "        losses.append(loss.cpu().data.numpy() / len(feature))\n",
    "        \n",
    "        if i%1000 == 0 and epoch%1 == 0:\n",
    "            \n",
    "            val_feature = torch.from_numpy(np.array(valid_x))\n",
    "            val_target = torch.from_numpy(np.array(valid_y)).type(torch.FloatTensor)\n",
    "            \n",
    "            if cuda:\n",
    "                val_feature = val_feature.cuda()\n",
    "                val_target = val_target.cuda()\n",
    "                \n",
    "            # model in test mode    \n",
    "            model.eval()\n",
    "\n",
    "            val_pred = model.forward(val_feature)\n",
    "            vloss = model.loss(val_pred, val_target)\n",
    "            print(\"Epoch[{}] Validation Loss: {:.3f} \".format(epoch, vloss.data))\n",
    "            \n",
    "            # Record the validation loss per example\n",
    "            valid_losses.append(val_loss.cpu().data.numpy()/len(val_feature))\n",
    "            \n",
    "            if best_loss is 0:\n",
    "                best_loss = vloss\n",
    "                best = [vloss,lr,lamb]\n",
    "                print(\"Save best theta...\")\n",
    "            else:\n",
    "                if vloss < best_loss:\n",
    "                    best_loss = vloss\n",
    "                    best = [vloss,lr,lamb]\n",
    "                    print(\"Save best theta...\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.2374, device='cuda:0', grad_fn=<MseLossBackward>), 0.01, 1e-06]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bias-Only works quite well.\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Epoch[0] Iteration[0] Training Loss: 14.70\n",
      "Epoch[0] Validation Loss: 13.599 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[1000] Training Loss: 1.41\n",
      "Epoch[0] Validation Loss: 1.451 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[2000] Training Loss: 1.34\n",
      "Epoch[0] Validation Loss: 1.361 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[3000] Training Loss: 1.34\n",
      "Epoch[0] Validation Loss: 1.310 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[4000] Training Loss: 1.28\n",
      "Epoch[0] Validation Loss: 1.281 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[5000] Training Loss: 1.18\n",
      "Epoch[0] Validation Loss: 1.264 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[6000] Training Loss: 1.35\n",
      "Epoch[0] Validation Loss: 1.252 \n",
      "Save best theta...\n",
      "Epoch[0] Iteration[7000] Training Loss: 1.23\n",
      "Epoch[0] Validation Loss: 1.246 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[0] Training Loss: 0.55\n",
      "Epoch[1] Validation Loss: 1.242 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[1000] Training Loss: 1.07\n",
      "Epoch[1] Validation Loss: 1.252 \n",
      "Epoch[1] Iteration[2000] Training Loss: 1.26\n",
      "Epoch[1] Validation Loss: 1.253 \n",
      "Epoch[1] Iteration[3000] Training Loss: 1.13\n",
      "Epoch[1] Validation Loss: 1.251 \n",
      "Epoch[1] Iteration[4000] Training Loss: 1.29\n",
      "Epoch[1] Validation Loss: 1.247 \n",
      "Epoch[1] Iteration[5000] Training Loss: 1.14\n",
      "Epoch[1] Validation Loss: 1.244 \n",
      "Epoch[1] Iteration[6000] Training Loss: 1.14\n",
      "Epoch[1] Validation Loss: 1.241 \n",
      "Save best theta...\n",
      "Epoch[1] Iteration[7000] Training Loss: 1.28\n",
      "Epoch[1] Validation Loss: 1.239 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[0] Training Loss: 1.09\n",
      "Epoch[2] Validation Loss: 1.238 \n",
      "Save best theta...\n",
      "Epoch[2] Iteration[1000] Training Loss: 1.15\n",
      "Epoch[2] Validation Loss: 1.250 \n",
      "Epoch[2] Iteration[2000] Training Loss: 1.13\n",
      "Epoch[2] Validation Loss: 1.251 \n",
      "Epoch[2] Iteration[3000] Training Loss: 1.14\n",
      "Epoch[2] Validation Loss: 1.249 \n",
      "Epoch[2] Iteration[4000] Training Loss: 1.01\n",
      "Epoch[2] Validation Loss: 1.245 \n",
      "Epoch[2] Iteration[5000] Training Loss: 1.21\n",
      "Epoch[2] Validation Loss: 1.243 \n",
      "Epoch[2] Iteration[6000] Training Loss: 1.18\n",
      "Epoch[2] Validation Loss: 1.240 \n",
      "Epoch[2] Iteration[7000] Training Loss: 1.22\n",
      "Epoch[2] Validation Loss: 1.239 \n",
      "Epoch[3] Iteration[0] Training Loss: 1.07\n",
      "Epoch[3] Validation Loss: 1.238 \n",
      "Epoch[3] Iteration[1000] Training Loss: 1.07\n",
      "Epoch[3] Validation Loss: 1.249 \n",
      "Epoch[3] Iteration[2000] Training Loss: 0.97\n",
      "Epoch[3] Validation Loss: 1.251 \n",
      "Epoch[3] Iteration[3000] Training Loss: 0.97\n",
      "Epoch[3] Validation Loss: 1.249 \n",
      "Epoch[3] Iteration[4000] Training Loss: 1.12\n",
      "Epoch[3] Validation Loss: 1.246 \n",
      "Epoch[3] Iteration[5000] Training Loss: 1.09\n",
      "Epoch[3] Validation Loss: 1.242 \n",
      "Epoch[3] Iteration[6000] Training Loss: 1.06\n",
      "Epoch[3] Validation Loss: 1.241 \n",
      "Epoch[3] Iteration[7000] Training Loss: 1.14\n",
      "Epoch[3] Validation Loss: 1.239 \n",
      "Epoch[4] Iteration[0] Training Loss: 0.81\n",
      "Epoch[4] Validation Loss: 1.238 \n",
      "Epoch[4] Iteration[1000] Training Loss: 1.19\n",
      "Epoch[4] Validation Loss: 1.250 \n",
      "Epoch[4] Iteration[2000] Training Loss: 1.21\n",
      "Epoch[4] Validation Loss: 1.251 \n",
      "Epoch[4] Iteration[3000] Training Loss: 1.14\n",
      "Epoch[4] Validation Loss: 1.249 \n",
      "Epoch[4] Iteration[4000] Training Loss: 1.03\n",
      "Epoch[4] Validation Loss: 1.246 \n",
      "Epoch[4] Iteration[5000] Training Loss: 1.29\n",
      "Epoch[4] Validation Loss: 1.243 \n",
      "Epoch[4] Iteration[6000] Training Loss: 1.23\n",
      "Epoch[4] Validation Loss: 1.240 \n",
      "Epoch[4] Iteration[7000] Training Loss: 1.35\n",
      "Epoch[4] Validation Loss: 1.239 \n",
      "Epoch[5] Iteration[0] Training Loss: 1.11\n",
      "Epoch[5] Validation Loss: 1.238 \n",
      "Save best theta...\n",
      "Epoch[5] Iteration[1000] Training Loss: 0.89\n",
      "Epoch[5] Validation Loss: 1.250 \n",
      "Epoch[5] Iteration[2000] Training Loss: 1.19\n",
      "Epoch[5] Validation Loss: 1.252 \n",
      "Epoch[5] Iteration[3000] Training Loss: 1.20\n",
      "Epoch[5] Validation Loss: 1.250 \n",
      "Epoch[5] Iteration[4000] Training Loss: 1.20\n",
      "Epoch[5] Validation Loss: 1.246 \n",
      "Epoch[5] Iteration[5000] Training Loss: 1.16\n",
      "Epoch[5] Validation Loss: 1.242 \n",
      "Epoch[5] Iteration[6000] Training Loss: 1.22\n",
      "Epoch[5] Validation Loss: 1.240 \n",
      "Epoch[5] Iteration[7000] Training Loss: 1.29\n",
      "Epoch[5] Validation Loss: 1.239 \n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 1e-2\n",
    "lamb = 1e-6\n",
    "batch_size = 1024\n",
    "\n",
    "# To keep track to best hyperparameters and results\n",
    "best_loss = 0\n",
    "best = []\n",
    "\n",
    "k_values = [1]\n",
    "\n",
    "for k in k_values:\n",
    "    \n",
    "    print(\"k = {}\".format(k))\n",
    "\n",
    "    model = MF(n_user, n_item, k=k)\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lamb)\n",
    "\n",
    "    def chunks(X, Y, size):\n",
    "        \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "        starts = list(range(0, len(X), size))\n",
    "        shuffle(starts)\n",
    "        for i in starts:\n",
    "            yield (X[i:i + size], Y[i:i + size])\n",
    "\n",
    "\n",
    "    losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(5+1):\n",
    "\n",
    "        i = 0\n",
    "        for feature, target in chunks(np.array(train_x), np.array(train_y), batch_size):\n",
    "            # This zeros the gradients on every parameter. \n",
    "            # This is easy to miss and hard to troubleshoot.\n",
    "            optimizer.zero_grad()\n",
    "            # Convert \n",
    "            feature = Variable(torch.from_numpy(feature))\n",
    "            target = Variable(torch.from_numpy(target).type(torch.FloatTensor))\n",
    "\n",
    "            if cuda:\n",
    "                feature = feature.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "            # model in training mode    \n",
    "            model.train()\n",
    "\n",
    "            # Compute a prediction for these features\n",
    "            prediction = model.forward(feature)\n",
    "            # Compute a loss given what the true target outcome was\n",
    "            loss = model.loss(prediction, target)\n",
    "            # break\n",
    "            # Backpropagate: compute the direction / gradient every model parameter\n",
    "            # defined in your __init__ should move in in order to minimize this loss\n",
    "            # However, we're not actually changing these parameters, we're just storing\n",
    "            # how they should change.\n",
    "\n",
    "            loss.backward()\n",
    "            # Now take a step & update the model parameters. The optimizer uses the gradient at \n",
    "            # defined on every parameter in our model and nudges it in that direction.\n",
    "            optimizer.step()\n",
    "\n",
    "            if i%1000 == 0 and epoch%1 == 0:\n",
    "                print(\"Epoch[{}] Iteration[{}] Training Loss: {:.2f}\".format(epoch, i, loss.data))\n",
    "\n",
    "            # Record the loss per example\n",
    "            losses.append(loss.cpu().data.numpy() / len(feature))\n",
    "\n",
    "            if i%1000 == 0 and epoch%1 == 0:\n",
    "\n",
    "                val_feature = torch.from_numpy(np.array(valid_x))\n",
    "                val_target = torch.from_numpy(np.array(valid_y)).type(torch.FloatTensor)\n",
    "\n",
    "                if cuda:\n",
    "                    val_feature = val_feature.cuda()\n",
    "                    val_target = val_target.cuda()\n",
    "\n",
    "                # model in test mode    \n",
    "                model.eval()\n",
    "\n",
    "                val_pred = model.forward(val_feature)\n",
    "                val_loss = model.loss(val_pred, val_target)\n",
    "                print(\"Epoch[{}] Validation Loss: {:.3f} \".format(epoch, val_loss.data))\n",
    "                \n",
    "                vloss = float(val_loss.cpu().data.numpy())\n",
    "                \n",
    "                # Record the validation loss per example\n",
    "                valid_losses.append(vloss/len(val_feature))\n",
    "                \n",
    "                if best_loss is 0:\n",
    "                    best_loss = vloss\n",
    "                    best = [vloss,lr,lamb,k]\n",
    "                    print(\"Save best theta...\")\n",
    "                else:\n",
    "                    if vloss < best_loss:\n",
    "                        best_loss = vloss\n",
    "                        best = [vloss,lr,lamb,k]\n",
    "                        print(\"Save best theta...\")\n",
    "\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "learning rate = 0.01  \n",
    "Lambda (regularaization) = 1e-6  \n",
    "K = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.331364393234253, 0.01, 1e-05]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing over lr - 0.01 is good enough; k=3\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2373532056808472, 0.01, 1e-06]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing over lambda - 1e-6 is good enough; k=3\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.23684823513031, 0.01, 1e-06, 1]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing over K - 1 is good enough\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
