{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd \n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(path):\n",
    "  f = open(path, 'rt')\n",
    "  f.readline()\n",
    "  for l in f:\n",
    "    yield l.strip().split(',')\n",
    "    \n",
    "def calc_model_stats(pred,label):\n",
    "    \n",
    "    TP,FP,TN,FN = calc_metrics(pred,label)\n",
    "\n",
    "    # print(\"Stats\")\n",
    "    # print(TP,FP,TN,FN)\n",
    "\n",
    "    # print(\"Predict N: {} ({}%)\".format(TN+FN,(TN+FN)/(TP+TN+FP+FN)))\n",
    "    # print(\"Predict P: {} ({}%)\".format(TP+FP,(TP+FP)/(TP+TN+FP+FN)))\n",
    "\n",
    "    accuracy, TPR, TNR, BER = calc_error_rates(TP, FP, TN, FN)\n",
    "\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    # print(\"TPR: {}\".format(TPR))\n",
    "    # print(\"TNR: {}\".format(TNR))\n",
    "    # print(\"BER: {}\".format(BER))\n",
    "    \n",
    "    return\n",
    " \n",
    "def calc_metrics(predictions, labels):\n",
    "    # Calculate True positives, false positives, etc.\n",
    "\n",
    "    TP_ = numpy.logical_and(predictions, labels)\n",
    "    FP_ = numpy.logical_and(predictions, numpy.logical_not(labels))\n",
    "    TN_ = numpy.logical_and(numpy.logical_not(predictions), numpy.logical_not(labels))\n",
    "    FN_ = numpy.logical_and(numpy.logical_not(predictions), labels)\n",
    "\n",
    "    TP=sum(TP_)\n",
    "    FP=sum(FP_)\n",
    "    TN=sum(TN_)\n",
    "    FN=sum(FN_)\n",
    "    \n",
    "    return TP,FP,TN,FN\n",
    "\n",
    "def calc_error_rates(TP, FP, TN, FN):\n",
    "    # Calculate accuracy, TPR, TNR and BER\n",
    "    accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    TPR = TP/(TP+FN)\n",
    "    TNR = TN/(TN+FP)\n",
    "    BER = 1.0 - (TPR+TNR)/2\n",
    "    \n",
    "    return accuracy, TPR, TNR, BER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training Dataset into Train and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 190001 9999\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for user,book,rating in readCSV(\"../datasets/cse258/assignment1/train_Interactions.csv\"):\n",
    "  dataset.append([user,book,rating])\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "X = [values[0:2] for values in dataset]\n",
    "y = [int(values[-1]) for values in dataset]\n",
    "\n",
    "N = len(dataset)\n",
    "Ntrain = 190001\n",
    "\n",
    "Xtrain = X[:Ntrain]\n",
    "Xvalid = X[Ntrain:]\n",
    "\n",
    "ytrain = y[:Ntrain]\n",
    "yvalid = y[Ntrain:]\n",
    "\n",
    "print(N, len(ytrain),len(yvalid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adopt code from Workbook 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11357 7170\n",
      "['u63264319', 'u26021107', 'u88966754']\n",
      "['b41598050', 'b98870833', 'b30696407']\n"
     ]
    }
   ],
   "source": [
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "\n",
    "for user,book,rating in dataset:\n",
    "    ratingsPerUser[user].append(rating)\n",
    "    ratingsPerItem[book].append(rating)\n",
    "\n",
    "N = len(Xtrain)\n",
    "nUsers = len(ratingsPerUser)\n",
    "nItems = len(ratingsPerItem)\n",
    "users = list(ratingsPerUser.keys())\n",
    "items = list(ratingsPerItem.keys())\n",
    "\n",
    "print (nUsers, nItems)\n",
    "print (users[:3])\n",
    "print (items[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of ratings in training set: 3.897005805232604\n"
     ]
    }
   ],
   "source": [
    "ratingMean = sum([y for y in ytrain])/len(ytrain)\n",
    "\n",
    "print(\"Mean of ratings in training set: {}\".format(ratingMean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ratingMean\n",
    "\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item]\n",
    "\n",
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))\n",
    "    \n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(user, book) for user,book in Xtrain]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost\n",
    "\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(dataset)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for value in zip(Xtrain,ytrain):\n",
    "        x,rating = value\n",
    "        user = x[0]\n",
    "        book = x[1]\n",
    "        pred = prediction(user,book)\n",
    "        diff = pred - rating\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[user] += 2/N*diff\n",
    "        dItemBiases[book] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 1e-05\n",
      "MSE = 1.473873856464982\n",
      "MSE = 1.456431368523354\n",
      "MSE = 1.3926990442309493\n",
      "MSE = 7.992699748763671\n",
      "MSE = 1.369015386525905\n",
      "MSE = 1.2039672102034578\n",
      "MSE = 1.201903208089255\n",
      "MSE = 1.1938184067777753\n",
      "MSE = 1.1642185158566818\n",
      "MSE = 1.0444310777472559\n",
      "MSE = 0.9961973927697352\n",
      "MSE = 0.9570780562063416\n",
      "MSE = 0.9424563174422734\n",
      "MSE = 0.9288203623178237\n",
      "MSE = 0.9226515027555446\n",
      "MSE = 0.9188661478677207\n",
      "MSE = 0.9181155350634062\n",
      "MSE = 0.9168705154643094\n",
      "MSE = 0.9170858556162015\n",
      "MSE = 0.9157322652244629\n",
      "MSE = 0.9149927307736362\n",
      "MSE = 0.9142339137164336\n",
      "MSE = 0.9141373205699372\n",
      "MSE = 0.9132090084612581\n",
      "MSE = 0.9797877181377072\n",
      "MSE = 0.912983782825024\n",
      "MSE = 0.9128937238404544\n",
      "MSE = 0.9129145065234482\n",
      "MSE = 0.9128795012236577\n",
      "MSE = 0.9127529835819246\n",
      "MSE = 0.913431250117685\n",
      "MSE = 0.9128888237328417\n",
      "MSE = 0.9128972885782255\n",
      "MSE = 0.9128438048084605\n",
      "MSE = 0.9127817050062647\n",
      "MSE = 0.9128158900139834\n",
      "MSE = 0.9126876402347415\n",
      "MSE = 0.9128705733836319\n",
      "MSE = 0.9129293007563997\n",
      "MSE = 0.9129050335954015\n",
      "MSE = 0.9128614371164295\n",
      "MSE = 0.9128238177204869\n",
      "MSE = 0.9126080695000812\n",
      "MSE = 0.9127869002070147\n",
      "MSE = 0.9127851094774888\n",
      "MSE = 0.9127863069306136\n",
      "MSE = 0.9127883191139359\n",
      "MSE = 0.9128086313299494\n",
      "MSE = 0.9128471652044159\n",
      "MSE = 0.9128341395445202\n",
      "MSE = 0.9145441958714796\n",
      "MSE = 0.9128751817242488\n",
      "MSE = 0.9128403298301886\n",
      "MSE = 0.912835212775375\n",
      "MSE = 0.9128343299288718\n",
      "MSE = 0.9128341734540172\n",
      "MSE = 0.912834145588351\n",
      "MSE = 0.9128341406217663\n",
      "MSE = 0.9128341397365001\n",
      "MSE = 0.9128341395786655\n",
      "MSE = 0.9128341395505541\n",
      "MSE = 0.9128341395455946\n",
      "MSE = 0.9128341395446422\n",
      "MSE = 0.9128341395452867\n",
      "MSE = 0.9128341395446997\n",
      "MSE = 0.9128341395446471\n",
      "MSE = 0.912834139544682\n",
      "MSE = 0.9128341395446489\n",
      "MSE = 0.9128341395446471\n",
      "Save best theta...\n",
      "1.0741282593688577\n",
      "Lambda = 1.3e-05\n",
      "MSE = 1.4818879988651132\n",
      "MSE = 2.2925612550697956\n",
      "MSE = 1.4738027770551698\n",
      "MSE = 1.473551930788236\n",
      "MSE = 1.4731880808412519\n",
      "MSE = 1.4719398546579197\n",
      "MSE = 1.4689947950976479\n",
      "MSE = 1.461182727664514\n",
      "MSE = 1.4422740980872266\n",
      "MSE = 1.4000157129815474\n",
      "MSE = 1.3258237237655202\n",
      "MSE = 1.243990087182269\n",
      "MSE = 1.2033869370535633\n",
      "MSE = 1.1974133878584146\n",
      "MSE = 1.1974777386710698\n",
      "MSE = 1.1972258972911094\n",
      "MSE = 1.194208469008155\n",
      "MSE = 1.1860227179674028\n",
      "MSE = 1.1651493943648057\n",
      "MSE = 1.1283170015763255\n",
      "MSE = 1.0877730873917009\n",
      "MSE = 1.065449723840445\n",
      "MSE = 62.566556770359306\n",
      "MSE = 1.0616550171836256\n",
      "MSE = 1.059790152284822\n",
      "MSE = 1.058373106757916\n",
      "MSE = 1.0512402217162118\n",
      "MSE = 1.0325678169550982\n",
      "MSE = 1.0090661077308665\n",
      "MSE = 0.9835908079987827\n",
      "MSE = 0.973759745623918\n",
      "MSE = 0.9694452644474405\n",
      "MSE = 0.9663223698113852\n",
      "MSE = 0.9641471667795707\n",
      "MSE = 0.9627596006550938\n",
      "MSE = 0.9599232273690772\n",
      "MSE = 0.9579408942411834\n",
      "MSE = 0.9547838587224465\n",
      "MSE = 0.9504519086659104\n",
      "MSE = 0.9454261198694308\n",
      "MSE = 0.9414462442492366\n",
      "MSE = 1.2894525431657577\n",
      "MSE = 0.9419426312510318\n",
      "MSE = 0.9410935206266599\n",
      "MSE = 0.9407481864542498\n",
      "MSE = 0.939776968516518\n",
      "MSE = 0.9373733365793383\n",
      "MSE = 0.9362309266002209\n",
      "MSE = 0.9283853193908768\n",
      "MSE = 0.9270606977979341\n",
      "MSE = 0.9270560013202266\n",
      "MSE = 0.9271809986603341\n",
      "MSE = 0.924890242554623\n",
      "MSE = 1.1037332299649514\n",
      "MSE = 0.9245085772994488\n",
      "MSE = 0.9225608727215713\n",
      "MSE = 0.9469805876266658\n",
      "MSE = 0.9220939328184258\n",
      "MSE = 0.9216674257373317\n",
      "MSE = 0.9200174836897036\n",
      "MSE = 0.9209810244122776\n",
      "MSE = 0.9231160453057401\n",
      "MSE = 0.9215446388657483\n",
      "MSE = 0.9204924027524687\n",
      "MSE = 0.919661828985809\n",
      "MSE = 0.9200489024579526\n",
      "MSE = 0.9196998271592419\n",
      "MSE = 0.9195429350479309\n",
      "MSE = 0.9194964720239199\n",
      "MSE = 0.9190637091997976\n",
      "MSE = 0.9206098681794451\n",
      "MSE = 0.9191070075769594\n",
      "MSE = 0.9191230334028908\n",
      "MSE = 0.9190849585867703\n",
      "MSE = 0.9189516111373703\n",
      "MSE = 0.918673839010081\n",
      "MSE = 0.9176772373276769\n",
      "MSE = 0.9173322236553872\n",
      "MSE = 0.9175196434211694\n",
      "MSE = 0.9174100258978493\n",
      "MSE = 0.9172544367293927\n",
      "MSE = 0.9172526156224609\n",
      "MSE = 0.9170608479835198\n",
      "MSE = 0.9191377238549271\n",
      "MSE = 0.9170777980448657\n",
      "MSE = 0.9168401557760719\n",
      "MSE = 0.91680054047516\n",
      "MSE = 0.9166259983338757\n",
      "MSE = 0.9164224070630121\n",
      "MSE = 0.9161215400678682\n",
      "MSE = 0.9155682633809656\n",
      "MSE = 0.9157953070613454\n",
      "MSE = 0.9156081165124627\n",
      "MSE = 0.9157055360194264\n",
      "MSE = 0.9157771493193998\n",
      "MSE = 0.9159380648479493\n",
      "MSE = 0.9166746512044283\n",
      "MSE = 0.9159823606026722\n",
      "MSE = 0.9160529702083393\n",
      "MSE = 0.917520055277755\n",
      "MSE = 0.9162231405796666\n",
      "MSE = 0.9163155184289784\n",
      "MSE = 0.9162370295833807\n",
      "MSE = 0.9161695692617626\n",
      "MSE = 0.9161970631268082\n",
      "MSE = 0.916285555410315\n",
      "MSE = 0.9188202644370156\n",
      "MSE = 0.9163250526015213\n",
      "MSE = 0.9162941033442233\n",
      "MSE = 0.9164961700114646\n",
      "MSE = 0.9171683927514536\n",
      "MSE = 0.9165532643148263\n",
      "MSE = 0.9168070443022033\n",
      "MSE = 0.9166195647639306\n",
      "MSE = 0.9168995075542049\n",
      "MSE = 0.9166669547020194\n",
      "MSE = 0.9167682539078037\n",
      "MSE = 0.916682237439216\n",
      "MSE = 0.9168220767683224\n",
      "MSE = 0.9167030791524804\n",
      "MSE = 0.9166859427854943\n",
      "MSE = 0.9166829158356753\n",
      "MSE = 0.9166823623069045\n",
      "MSE = 0.9166822604452433\n",
      "MSE = 0.9166822416786929\n",
      "MSE = 0.9166822382205801\n",
      "MSE = 0.9166822375832084\n",
      "MSE = 0.9166822374657096\n",
      "MSE = 0.9166822374440811\n",
      "MSE = 0.9166822374400617\n",
      "MSE = 0.9166822374427299\n",
      "MSE = 0.9166822374404276\n",
      "MSE = 0.9166822374401304\n",
      "MSE = 0.9166822374403201\n",
      "MSE = 0.9166822374403201\n",
      "Save best theta...\n",
      "1.0735889944967134\n",
      "Lambda = 1.5e-05\n",
      "MSE = 1.481378125424722\n",
      "MSE = 2.2971593477100076\n",
      "MSE = 1.4737986702866603\n",
      "MSE = 1.4735519362254723\n",
      "MSE = 1.473177856335563\n",
      "MSE = 1.4719250355563018\n",
      "MSE = 1.468946800333569\n",
      "MSE = 1.4610723747348806\n",
      "MSE = 1.4419979853911973\n",
      "MSE = 1.399428478467204\n",
      "MSE = 1.3248548026398128\n",
      "MSE = 1.242983161984011\n",
      "MSE = 1.2028874493661568\n",
      "MSE = 1.1973112739833303\n",
      "MSE = 1.1974704326467394\n",
      "MSE = 1.1972519606385348\n",
      "MSE = 1.1943474491524302\n",
      "MSE = 1.186279160253385\n",
      "MSE = 1.1654280308834544\n",
      "MSE = 1.1285798896220545\n",
      "MSE = 1.0882248107773493\n",
      "MSE = 1.0653821105978143\n",
      "MSE = 44.85141486575742\n",
      "MSE = 1.061746331611171\n",
      "MSE = 1.0600049859402183\n",
      "MSE = 1.0583190642186915\n",
      "MSE = 1.0491565910869691\n",
      "MSE = 1.0282817926464427\n",
      "MSE = 1.0035792408686088\n",
      "MSE = 0.9916960713401355\n",
      "MSE = 0.982413826956607\n",
      "MSE = 0.977274533672793\n",
      "MSE = 0.969523261808489\n",
      "MSE = 0.9609155127414793\n",
      "MSE = 2.7477965493572376\n",
      "MSE = 0.9600946092849098\n",
      "MSE = 0.9561342809735596\n",
      "MSE = 0.9543858130526365\n",
      "MSE = 0.9510693927976283\n",
      "MSE = 0.9414617987061323\n",
      "MSE = 1.4998362692168643\n",
      "MSE = 0.9333128823711139\n",
      "MSE = 0.9337142877534653\n",
      "MSE = 0.9593213344274725\n",
      "MSE = 0.9335086827213211\n",
      "MSE = 0.9335723847881433\n",
      "MSE = 0.933189651081991\n",
      "MSE = 0.9308770890120911\n",
      "MSE = 1.6146171089983505\n",
      "MSE = 0.9308229588418433\n",
      "MSE = 0.9294759108604784\n",
      "MSE = 0.9276507034042653\n",
      "MSE = 0.9267019772469608\n",
      "MSE = 0.9265410563910019\n",
      "MSE = 0.9261523106911844\n",
      "MSE = 0.9257176948920842\n",
      "MSE = 0.9252197417534345\n",
      "MSE = 0.9249866375996239\n",
      "MSE = 0.924024645092308\n",
      "MSE = 0.9236530042109583\n",
      "MSE = 0.9231807829201346\n",
      "MSE = 0.9227171714651353\n",
      "MSE = 0.9229235346988877\n",
      "MSE = 0.9226623165654033\n",
      "MSE = 0.9223765086600045\n",
      "MSE = 0.9222744417374613\n",
      "MSE = 0.9222573276508085\n",
      "MSE = 0.9215796171319357\n",
      "MSE = 0.919259727245093\n",
      "MSE = 0.9205321482457983\n",
      "MSE = 0.9207404771725873\n",
      "MSE = 0.9205016783154722\n",
      "MSE = 0.9206533823928568\n",
      "MSE = 0.9209320791659915\n",
      "MSE = 0.9209277585399258\n",
      "MSE = 0.9203397851978337\n",
      "MSE = 0.9205433588886831\n",
      "MSE = 0.9205016947493662\n",
      "MSE = 0.9203364085445161\n",
      "MSE = 0.920441045645129\n",
      "MSE = 0.920761318786218\n",
      "MSE = 0.919865109008102\n",
      "MSE = 0.9200708426264893\n",
      "MSE = 0.9204914178671285\n",
      "MSE = 0.9201620868956859\n",
      "MSE = 0.9204745939065785\n",
      "MSE = 0.9253880020263934\n",
      "MSE = 0.9205289724765644\n",
      "MSE = 0.9204821716945251\n",
      "MSE = 0.9204758119682342\n",
      "MSE = 0.9204747939948601\n",
      "MSE = 0.9204746268909647\n",
      "MSE = 0.9204745993472125\n",
      "MSE = 0.9204745948041417\n",
      "MSE = 0.920474594054566\n",
      "MSE = 0.920474593930965\n",
      "MSE = 0.920474593910548\n",
      "MSE = 0.9204745939072444\n",
      "MSE = 0.920474593906658\n",
      "MSE = 0.920474593906583\n",
      "MSE = 0.9204745939065796\n",
      "MSE = 0.9204745939065785\n",
      "MSE = 0.9204745939065792\n",
      "MSE = 0.9204745939065785\n",
      "MSE = 0.9204745939065793\n",
      "MSE = 0.9204745939065785\n",
      "MSE = 0.9204741561841275\n",
      "MSE = 0.9204727348613666\n",
      "MSE = 0.9204726772106441\n",
      "MSE = 0.9204724810839743\n",
      "MSE = 0.9204709304708184\n",
      "MSE = 0.9204689823573898\n",
      "MSE = 0.9204671556824465\n",
      "MSE = 0.9204673538999774\n",
      "MSE = 0.9204681773061134\n",
      "MSE = 0.9204675545174575\n",
      "MSE = 0.9204668525783057\n",
      "MSE = 0.9204661537781976\n",
      "MSE = 0.9204651342658782\n",
      "MSE = 0.920464701943061\n",
      "MSE = 0.920467052602099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.9204773987964511\n",
      "MSE = 0.9205008896861069\n",
      "MSE = 0.9205641897659114\n",
      "MSE = 0.9205021149886699\n",
      "MSE = 0.920526938844379\n",
      "MSE = 0.9205359138749829\n",
      "MSE = 0.9205349594077047\n",
      "MSE = 0.9205337180239839\n",
      "MSE = 0.920533022976005\n",
      "MSE = 0.9205380861846606\n",
      "MSE = 0.9205590367903724\n",
      "MSE = 0.9206010820906585\n",
      "MSE = 0.9205978315337793\n",
      "MSE = 0.9205898525519265\n",
      "MSE = 0.920637792522961\n",
      "MSE = 0.9206302504042437\n",
      "MSE = 0.9206231300225696\n",
      "MSE = 0.9206194989551776\n",
      "MSE = 0.9211533454981179\n",
      "MSE = 0.9206147349746115\n",
      "MSE = 0.9206103173405396\n",
      "MSE = 0.9206328784657091\n",
      "MSE = 0.9205502182739058\n",
      "MSE = 0.9205881626679796\n",
      "MSE = 0.9205948835834266\n",
      "MSE = 0.9205849873875404\n",
      "MSE = 0.9208202587247981\n",
      "MSE = 0.9205768682622121\n",
      "MSE = 0.9205516158512381\n",
      "MSE = 0.9205227000333837\n",
      "MSE = 0.920482813803834\n",
      "MSE = 0.9204443061738207\n",
      "MSE = 0.9203738860768645\n",
      "MSE = 0.9204156913896236\n",
      "MSE = 0.9204257320344986\n",
      "MSE = 0.9204319066346137\n",
      "MSE = 0.9204264851928531\n",
      "MSE = 0.9204258368546867\n",
      "MSE = 0.9204257468784579\n",
      "MSE = 0.9204257341417151\n",
      "MSE = 0.9204257323336373\n",
      "MSE = 0.9204257320769976\n",
      "MSE = 0.9204257320405456\n",
      "MSE = 0.9204257320353273\n",
      "MSE = 0.920425732034596\n",
      "MSE = 0.9204257320344988\n",
      "MSE = 0.9204257320345\n",
      "MSE = 0.9204257320344988\n",
      "Save best theta...\n",
      "1.0734200113625347\n",
      "Lambda = 1.7e-05\n",
      "MSE = 1.4811804647362536\n",
      "MSE = 2.2989695434223663\n",
      "MSE = 1.473797040475686\n",
      "MSE = 1.473551934124409\n",
      "MSE = 1.4731735137933264\n",
      "MSE = 1.4719183939579128\n",
      "MSE = 1.468925482759017\n",
      "MSE = 1.461022477529896\n",
      "MSE = 1.4418710121827527\n",
      "MSE = 1.3991443156463492\n",
      "MSE = 1.324339555306592\n",
      "MSE = 1.2423613454725024\n",
      "MSE = 1.2025490684965188\n",
      "MSE = 1.1972757647888987\n",
      "MSE = 1.1975227334385172\n",
      "MSE = 1.197340930871478\n",
      "MSE = 1.194574717590222\n",
      "MSE = 1.1866608562480738\n",
      "MSE = 1.1659250161207997\n",
      "MSE = 1.1291328142459138\n",
      "MSE = 1.0889442934447844\n",
      "MSE = 1.0655215042937967\n",
      "MSE = 34.55645685737947\n",
      "MSE = 1.0619991599058591\n",
      "MSE = 1.060297479142427\n",
      "MSE = 1.0582828593909677\n",
      "MSE = 1.0468216364663432\n",
      "MSE = 1.0239749077053566\n",
      "MSE = 0.9971781399092462\n",
      "MSE = 1.0004239485748523\n",
      "MSE = 0.991267063106572\n",
      "MSE = 0.9860305125425023\n",
      "MSE = 0.9761245480630415\n",
      "MSE = 0.9704914884737553\n",
      "MSE = 0.9662771403463625\n",
      "MSE = 0.9652373790547245\n",
      "MSE = 0.9643642804111888\n",
      "MSE = 0.9632764084497568\n",
      "MSE = 0.9550092634679813\n",
      "MSE = 0.9530649532870422\n",
      "MSE = 0.9496596348581592\n",
      "MSE = 0.9448059962336467\n",
      "MSE = 5.302361774540402\n",
      "MSE = 0.9444647462760944\n",
      "MSE = 0.9431668050013324\n",
      "MSE = 0.9423389877035919\n",
      "MSE = 0.9422129556882272\n",
      "MSE = 0.9418995328052419\n",
      "MSE = 1.0160865635506315\n",
      "MSE = 0.942893835814424\n",
      "MSE = 0.9437210243173426\n",
      "MSE = 0.9449171271315246\n",
      "MSE = 0.9453361059026318\n",
      "MSE = 0.945526670093465\n",
      "MSE = 0.9456153452082058\n",
      "MSE = 0.9453465861713234\n",
      "MSE = 1.573844582328061\n",
      "MSE = 0.9453795057302556\n",
      "MSE = 0.9447181944641686\n",
      "MSE = 0.9441079012733181\n",
      "MSE = 0.9436792894738025\n",
      "MSE = 0.9432908431442245\n",
      "MSE = 0.9486720863319531\n",
      "MSE = 0.9435700997822\n",
      "MSE = 0.9427308838136906\n",
      "MSE = 0.9379519971073681\n",
      "MSE = 0.9408901668886568\n",
      "MSE = 0.9391397110671104\n",
      "MSE = 0.9347804150382113\n",
      "MSE = 0.9325651664268509\n",
      "MSE = 0.9322473904143218\n",
      "MSE = 0.9319209986636031\n",
      "MSE = 0.9320773772261249\n",
      "MSE = 0.9318029665803159\n",
      "MSE = 0.9304596983848946\n",
      "MSE = 0.9288061243225817\n",
      "MSE = 0.9248059752514489\n",
      "MSE = 0.9268197989484896\n",
      "MSE = 0.926152174680809\n",
      "MSE = 0.9260950248282082\n",
      "MSE = 0.9270424724971004\n",
      "MSE = 0.9262180420989823\n",
      "MSE = 0.926124496992463\n",
      "MSE = 0.9261860633603952\n",
      "MSE = 0.9264326675364373\n",
      "MSE = 0.9265509833623767\n",
      "MSE = 0.9266825415373071\n",
      "MSE = 0.9298363827562596\n",
      "MSE = 0.9267422885354816\n",
      "MSE = 0.9262491098142879\n",
      "MSE = 0.9258525139600037\n",
      "MSE = 0.9255058301495319\n",
      "MSE = 0.9253782132502638\n",
      "MSE = 0.9255855301130524\n",
      "MSE = 0.9260245882013673\n",
      "MSE = 0.9256956697509446\n",
      "MSE = 0.9261689758864621\n",
      "MSE = 0.9261743729742233\n",
      "MSE = 0.9261103074365314\n",
      "MSE = 0.9254950041013712\n",
      "MSE = 0.9251548432276901\n",
      "MSE = 0.9241469527608048\n",
      "MSE = 0.9249177152866386\n",
      "MSE = 0.9249335508366889\n",
      "MSE = 0.9255767676035962\n",
      "MSE = 0.9249808877646525\n",
      "MSE = 0.9250659367085617\n",
      "MSE = 0.9250199821493194\n",
      "MSE = 0.9249647860200493\n",
      "MSE = 0.9248420675423944\n",
      "MSE = 0.9247141181596855\n",
      "MSE = 0.9245168510269752\n",
      "MSE = 0.9244831296798329\n",
      "MSE = 0.9237535370344827\n",
      "MSE = 0.9243244790134725\n",
      "MSE = 0.924382829472918\n",
      "MSE = 0.9243424792554826\n",
      "MSE = 0.9243882593313227\n",
      "MSE = 0.9243992544356645\n",
      "MSE = 0.924348007822031\n",
      "MSE = 0.9242574667825301\n",
      "MSE = 0.9240286498089381\n",
      "MSE = 0.9238083492809194\n",
      "MSE = 0.9234308222958952\n",
      "MSE = 0.9230816348751085\n",
      "MSE = 0.9230975202451981\n",
      "MSE = 0.9232893187788048\n",
      "MSE = 0.9247731630207708\n",
      "MSE = 0.9232864151740316\n",
      "MSE = 0.9233098575085902\n",
      "MSE = 0.9233426083615702\n",
      "MSE = 0.9233527366333852\n",
      "MSE = 0.923362588004276\n",
      "MSE = 0.9233519952637078\n",
      "MSE = 0.9233571229234224\n",
      "MSE = 0.9233975803121606\n",
      "MSE = 0.9234767604422158\n",
      "MSE = 0.9234092515191324\n",
      "MSE = 0.9233995448843795\n",
      "MSE = 0.923397918101275\n",
      "MSE = 0.9233976386025234\n",
      "MSE = 0.9233975903772853\n",
      "MSE = 0.9233975820503805\n",
      "MSE = 0.923397580612117\n",
      "MSE = 0.9233975803639007\n",
      "MSE = 0.9233975803210747\n",
      "MSE = 0.9233975803136476\n",
      "MSE = 0.9233975803123312\n",
      "MSE = 0.9233975803131973\n",
      "MSE = 0.9233975803124401\n",
      "MSE = 0.9233975803123518\n",
      "MSE = 0.9233975803123312\n",
      "1.0737866337278643\n",
      "Lambda = 1.8e-05\n",
      "MSE = 1.4810926748339204\n",
      "MSE = 2.299778513793813\n",
      "MSE = 1.4737963096431803\n",
      "MSE = 1.4735519323163366\n",
      "MSE = 1.473171509698061\n",
      "MSE = 1.4719152641300866\n",
      "MSE = 1.4689154744196522\n",
      "MSE = 1.4609989337343883\n",
      "MSE = 1.4418110136673234\n",
      "MSE = 1.399009350262573\n",
      "MSE = 1.3240937679489506\n",
      "MSE = 1.2420637876852965\n",
      "MSE = 1.2023890450389791\n",
      "MSE = 1.1972625575805458\n",
      "MSE = 1.1975515274322284\n",
      "MSE = 1.1973879361765707\n",
      "MSE = 1.1946914801949966\n",
      "MSE = 1.1868552400440597\n",
      "MSE = 1.1661807459633977\n",
      "MSE = 1.129419983207906\n",
      "MSE = 1.0893353534925807\n",
      "MSE = 1.0656290189419202\n",
      "MSE = 30.114655659002842\n",
      "MSE = 1.0621520463152283\n",
      "MSE = 1.060441795459785\n",
      "MSE = 1.0582011587977598\n",
      "MSE = 1.0452632531822592\n",
      "MSE = 1.0213927051746792\n",
      "MSE = 0.9939191721544588\n",
      "MSE = 1.0005101203000664\n",
      "MSE = 0.9829411590238434\n",
      "MSE = 0.9820568723028936\n",
      "MSE = 0.9781698845588204\n",
      "MSE = 0.9709249528093935\n",
      "MSE = 0.9641068340003677\n",
      "MSE = 0.9601029438458277\n",
      "MSE = 2.801647029823204\n",
      "MSE = 0.9601208565832577\n",
      "MSE = 0.9573661105941683\n",
      "MSE = 0.950322201698353\n",
      "MSE = 0.9496438711500182\n",
      "MSE = 0.9407629847096601\n",
      "MSE = 0.940916702667364\n",
      "MSE = 0.9412451863612381\n",
      "MSE = 0.9402609422901944\n",
      "MSE = 0.9388896249144801\n",
      "MSE = 0.9367832085147344\n",
      "MSE = 0.9356555841706357\n",
      "MSE = 0.9327596189465854\n",
      "MSE = 0.9311479505871596\n",
      "MSE = 0.932096458871555\n",
      "MSE = 0.9310773921565166\n",
      "MSE = 0.931603837268352\n",
      "MSE = 0.9324329852938\n",
      "MSE = 0.9322940437686379\n",
      "MSE = 0.9316113224959617\n",
      "MSE = 0.9301568276305577\n",
      "MSE = 0.9871618757323677\n",
      "MSE = 0.929871832447946\n",
      "MSE = 0.9286346453371577\n",
      "MSE = 0.9286528500736888\n",
      "MSE = 0.9302317365734634\n",
      "MSE = 0.9286173834163213\n",
      "MSE = 0.9266149137197518\n",
      "MSE = 0.9277703664484198\n",
      "MSE = 0.9284744124378941\n",
      "MSE = 0.9284856784487163\n",
      "MSE = 0.9285570771391872\n",
      "MSE = 0.9284827540746445\n",
      "MSE = 0.9281893995465043\n",
      "MSE = 0.9338004724267218\n",
      "MSE = 0.9286612494040415\n",
      "MSE = 0.9283112036717531\n",
      "MSE = 0.9475419883650102\n",
      "MSE = 0.9283391680233177\n",
      "MSE = 0.9287148800430941\n",
      "MSE = 0.9288148108538804\n",
      "MSE = 0.9280510413980398\n",
      "MSE = 0.9263547181881038\n",
      "MSE = 0.9275004537524473\n",
      "MSE = 0.9270851665951046\n",
      "MSE = 0.9265783215706409\n",
      "MSE = 0.925923576837197\n",
      "MSE = 0.926249069320316\n",
      "MSE = 0.926194369156659\n",
      "MSE = 0.9262166648786484\n",
      "MSE = 0.9263518243873056\n",
      "MSE = 0.926666781384386\n",
      "MSE = 0.9270073421674829\n",
      "MSE = 0.9267513905231232\n",
      "MSE = 0.9272742678183684\n",
      "MSE = 0.9269006401056546\n",
      "MSE = 0.927332931365617\n",
      "MSE = 0.927021048787008\n",
      "MSE = 0.9273450448686816\n",
      "MSE = 0.9271118680739205\n",
      "MSE = 0.9273465904599312\n",
      "MSE = 0.927340641579299\n",
      "MSE = 0.9268742023493053\n",
      "MSE = 0.9266598687526849\n",
      "MSE = 0.9262638812231027\n",
      "MSE = 0.9259581118359311\n",
      "MSE = 0.9259609656972769\n",
      "MSE = 0.9258667638240429\n",
      "MSE = 0.9301102178423019\n",
      "MSE = 0.9262718705319507\n",
      "MSE = 0.9264225280568186\n",
      "MSE = 0.9265365859957533\n",
      "MSE = 0.9263795536919426\n",
      "MSE = 0.9262974833020662\n",
      "MSE = 0.9262881449875069\n",
      "MSE = 0.9262625762061858\n",
      "MSE = 0.9262023951223993\n",
      "MSE = 0.926064358332873\n",
      "MSE = 0.9255832519499334\n",
      "MSE = 0.9257257738905194\n",
      "MSE = 0.9256353357821117\n",
      "MSE = 0.9256712548824297\n",
      "MSE = 0.9256626952174186\n",
      "MSE = 0.9254612569414576\n",
      "MSE = 0.9254513482280575\n",
      "MSE = 0.9253954662363069\n",
      "MSE = 0.92520248675827\n",
      "MSE = 0.9251112270365744\n",
      "MSE = 0.9250919569798317\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.9267501579513642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.9249552569011027\n",
      "MSE = 0.9249101714877459\n",
      "MSE = 0.9249085066484912\n",
      "MSE = 0.9249084444275144\n",
      "MSE = 0.924908442101058\n",
      "MSE = 0.9249084420140499\n",
      "MSE = 0.9249084420108274\n",
      "MSE = 0.9249084420106874\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.924908442010685\n",
      "MSE = 0.9249084420106826\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.9249084420106828\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.9249084420106826\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.924908542871265\n",
      "MSE = 0.9249086594490874\n",
      "MSE = 0.9249091298540754\n",
      "MSE = 0.9249110769735128\n",
      "MSE = 0.9249390826868168\n",
      "MSE = 0.9249622153199516\n",
      "MSE = 0.9249732867920089\n",
      "MSE = 0.9249745067977744\n",
      "MSE = 0.9249744290110493\n",
      "MSE = 0.9249749609816713\n",
      "MSE = 0.9249780101582573\n",
      "MSE = 0.9249891772413189\n",
      "MSE = 0.9250185087410511\n",
      "MSE = 0.9250786223689966\n",
      "MSE = 0.925161568799013\n",
      "MSE = 0.9250974876908933\n",
      "MSE = 0.9252042137902682\n",
      "MSE = 0.9251154976401953\n",
      "MSE = 0.9251955471478489\n",
      "MSE = 0.9251332075932325\n",
      "MSE = 0.9251928380255553\n",
      "MSE = 0.9251454347146999\n",
      "MSE = 0.9251909256450205\n",
      "MSE = 0.9251535726572686\n",
      "MSE = 0.9251907289237253\n",
      "MSE = 0.9251592962929313\n",
      "MSE = 0.9251546668700522\n",
      "MSE = 0.9251537900016109\n",
      "MSE = 0.9251536161537736\n",
      "MSE = 0.9251535813752259\n",
      "MSE = 0.9251535744050614\n",
      "MSE = 0.9251535730076516\n",
      "MSE = 0.9251535727275539\n",
      "MSE = 0.9251535726713357\n",
      "MSE = 0.9251535727084589\n",
      "MSE = 0.9251535726786206\n",
      "MSE = 0.9251535726727103\n",
      "MSE = 0.9251535726713357\n",
      "1.074256054597343\n",
      "Lambda = 2e-05\n",
      "MSE = 1.480782609599551\n",
      "MSE = 2.3026602297569463\n",
      "MSE = 1.4737936933489413\n",
      "MSE = 1.4735519208248637\n",
      "MSE = 1.4731640261214578\n",
      "MSE = 1.4719032367976044\n",
      "MSE = 1.4688772674607835\n",
      "MSE = 1.4609090930043307\n",
      "MSE = 1.4415861818820448\n",
      "MSE = 1.398525916515037\n",
      "MSE = 1.3232899420764597\n",
      "MSE = 1.241213258995564\n",
      "MSE = 1.2019610156824978\n",
      "MSE = 1.1971766801179082\n",
      "MSE = 1.1975451324582524\n",
      "MSE = 1.1974151834128588\n",
      "MSE = 1.1948462943401592\n",
      "MSE = 1.187143626203621\n",
      "MSE = 1.166548797131064\n",
      "MSE = 1.1298202243517987\n",
      "MSE = 1.0900754246567508\n",
      "MSE = 1.065851256776384\n",
      "MSE = 19.811691530507584\n",
      "MSE = 1.0624308420769686\n",
      "MSE = 1.0605924218506475\n",
      "MSE = 1.0575257807578773\n",
      "MSE = 1.0397348941770654\n",
      "MSE = 1.0135302560789397\n",
      "MSE = 0.9896253343853096\n",
      "MSE = 1.0743871990132967\n",
      "MSE = 0.9744780093438327\n",
      "MSE = 0.9747105044713731\n",
      "MSE = 0.9700364320071698\n",
      "MSE = 0.9635280752873712\n",
      "MSE = 0.9600954192537828\n",
      "MSE = 0.9531711906970576\n",
      "MSE = 0.9501754777236822\n",
      "MSE = 0.95088601061806\n",
      "MSE = 0.9407224870381344\n",
      "MSE = 0.9455233413665062\n",
      "MSE = 0.9499880996425656\n",
      "MSE = 0.9506692720313672\n",
      "MSE = 0.9531554597260574\n",
      "MSE = 0.9495291968747999\n",
      "MSE = 0.9478415884203568\n",
      "MSE = 0.9446627108689971\n",
      "MSE = 0.9415409408678643\n",
      "MSE = 0.9414249896946553\n",
      "MSE = 0.9410042451092625\n",
      "MSE = 0.9411027523517155\n",
      "MSE = 0.9412612594096592\n",
      "MSE = 0.9414749473850823\n",
      "MSE = 0.9415598072734906\n",
      "MSE = 0.9410185559086814\n",
      "MSE = 0.9400019005226885\n",
      "MSE = 0.950756850766013\n",
      "MSE = 0.937925292153498\n",
      "MSE = 0.9362392475476387\n",
      "MSE = 0.9351421448418142\n",
      "MSE = 0.9345938377693424\n",
      "MSE = 0.9435094357297791\n",
      "MSE = 0.9347523624763705\n",
      "MSE = 0.9347019547297027\n",
      "MSE = 0.9332952682069576\n",
      "MSE = 0.9324757135844453\n",
      "MSE = 0.9328310232346769\n",
      "MSE = 0.9319984452517807\n",
      "MSE = 0.9320135451383839\n",
      "MSE = 0.9313159031027947\n",
      "MSE = 1.012210380184477\n",
      "MSE = 0.9318198344520495\n",
      "MSE = 0.931408691201508\n",
      "MSE = 0.9315279267616221\n",
      "MSE = 0.9300940229366682\n",
      "MSE = 0.9308258318879098\n",
      "MSE = 0.9309548656206904\n",
      "MSE = 0.9314738851014824\n",
      "MSE = 0.93762585417876\n",
      "MSE = 0.9312872105432057\n",
      "MSE = 0.9314637522280633\n",
      "MSE = 0.9314829196288407\n",
      "MSE = 0.9315523592158403\n",
      "MSE = 0.9315257058039683\n",
      "MSE = 0.9322100143126073\n",
      "MSE = 0.9317568372318153\n",
      "MSE = 0.9325775339033847\n",
      "MSE = 0.9321809215008424\n",
      "MSE = 0.9310857923624575\n",
      "MSE = 0.9300362586560907\n",
      "MSE = 0.9306913382576198\n",
      "MSE = 0.9306439889242025\n",
      "MSE = 0.9306872959886173\n",
      "MSE = 0.930777646601682\n",
      "MSE = 1.2258714246658524\n",
      "MSE = 0.9307231490463891\n",
      "MSE = 0.9307887580149575\n",
      "MSE = 0.9307902764386434\n",
      "MSE = 0.930730126664204\n",
      "MSE = 0.9306655800130453\n",
      "MSE = 0.930549149298414\n",
      "MSE = 0.9302746512624558\n",
      "MSE = 0.9762493851940954\n",
      "MSE = 0.9304827985229125\n",
      "MSE = 0.9302997809044782\n",
      "MSE = 0.9302781058734769\n",
      "MSE = 0.9302751342742298\n",
      "MSE = 0.9302747189542694\n",
      "MSE = 0.9302746607522405\n",
      "MSE = 0.9302746525929255\n",
      "MSE = 0.9302746514490031\n",
      "MSE = 0.9302746512886017\n",
      "MSE = 0.9302746512661351\n",
      "MSE = 0.9302746512629271\n",
      "MSE = 0.9302746512650606\n",
      "MSE = 0.9302746512631481\n",
      "MSE = 0.9302746512643906\n",
      "MSE = 0.930274651263309\n",
      "MSE = 0.9302746512631767\n",
      "MSE = 0.9302746512631481\n",
      "1.076102050248465\n",
      "Lambda = 5e-05\n",
      "MSE = 1.480975436583329\n",
      "MSE = 2.3008636259568163\n",
      "MSE = 1.4737953273741136\n",
      "MSE = 1.4735519342763534\n",
      "MSE = 1.4731687980415085\n",
      "MSE = 1.4719109832606465\n",
      "MSE = 1.468900962781183\n",
      "MSE = 1.4609556622713\n",
      "MSE = 1.4416384144548213\n",
      "MSE = 1.3982914432049682\n",
      "MSE = 1.3219536881499483\n",
      "MSE = 1.2384398625314763\n",
      "MSE = 1.201053202562287\n",
      "MSE = 1.1990836877917377\n",
      "MSE = 1.2003085347436457\n",
      "MSE = 1.2007459199480397\n",
      "MSE = 1.2001627603022602\n",
      "MSE = 1.1947334621535712\n",
      "MSE = 1.1766797326425413\n",
      "MSE = 1.1416911902945535\n",
      "MSE = 1.1178875608812093\n",
      "MSE = 1.0884805549465206\n",
      "MSE = 1.0786977871208152\n",
      "MSE = 1.0742740683891674\n",
      "MSE = 1.0510748555863154\n",
      "MSE = 1.0346031580118895\n",
      "MSE = 1.015835582992465\n",
      "MSE = 1.0113609950245799\n",
      "MSE = 1.0109817438270101\n",
      "MSE = 1.0161169546432633\n",
      "MSE = 1.012618873626611\n",
      "MSE = 1.0101472064737123\n",
      "MSE = 1.0112333422047117\n",
      "MSE = 1.0102231496239726\n",
      "MSE = 1.008347018723588\n",
      "MSE = 1.0039860652542805\n",
      "MSE = 0.9999469121598262\n",
      "MSE = 0.9952273547581975\n",
      "MSE = 0.9921999320693642\n",
      "MSE = 0.9925531220197422\n",
      "MSE = 0.9982586651398829\n",
      "MSE = 0.9940847631986757\n",
      "MSE = 0.9923263871233331\n",
      "MSE = 0.9923163599740704\n",
      "MSE = 0.9947703312184507\n",
      "MSE = 0.9939565970689924\n",
      "MSE = 0.9938997305334042\n",
      "MSE = 0.9934125187556055\n",
      "MSE = 0.9910210575621071\n",
      "MSE = 0.9902132606670471\n",
      "MSE = 0.9884769950764605\n",
      "MSE = 0.9865292600720069\n",
      "MSE = 0.9861664758951941\n",
      "MSE = 0.9876114068574283\n",
      "MSE = 1.007402798257137\n",
      "MSE = 0.9877536814626554\n",
      "MSE = 0.9889159749596844\n",
      "MSE = 0.9923593423943071\n",
      "MSE = 0.9892370291676542\n",
      "MSE = 0.9889909318383029\n",
      "MSE = 0.9897078239031675\n",
      "MSE = 0.9886959670722332\n",
      "MSE = 0.989196076807751\n",
      "MSE = 0.9890064114607124\n",
      "MSE = 0.9881019604611899\n",
      "MSE = 0.9873170966187735\n",
      "MSE = 0.9870279218575266\n",
      "MSE = 0.9860574897945563\n",
      "MSE = 0.9866720988985086\n",
      "MSE = 0.9862086732033744\n",
      "MSE = 0.9874510313746889\n",
      "MSE = 0.986389574245995\n",
      "MSE = 0.9862403961327059\n",
      "MSE = 0.9862144080989202\n",
      "MSE = 0.9862097156221421\n",
      "MSE = 0.9862088628686151\n",
      "MSE = 0.9862087077185808\n",
      "MSE = 0.9862086794846412\n",
      "MSE = 0.9862086743463776\n",
      "MSE = 0.9862086734114877\n",
      "MSE = 0.9862086732411239\n",
      "MSE = 0.9862086732101618\n",
      "MSE = 0.9862086732044806\n",
      "MSE = 0.9862086732035765\n",
      "MSE = 0.9862086732041726\n",
      "MSE = 0.9862086732036389\n",
      "MSE = 0.9862086732035856\n",
      "MSE = 0.9862086732036193\n",
      "MSE = 0.9862086732035913\n",
      "MSE = 0.9862086732036006\n",
      "MSE = 0.9862086388059201\n",
      "MSE = 0.9862081364386155\n",
      "MSE = 0.9862081799542812\n",
      "MSE = 0.9862084577159919\n",
      "MSE = 0.9862097063374408\n",
      "MSE = 0.9862174129042051\n",
      "MSE = 0.9862346517562868\n",
      "MSE = 0.98627172484501\n",
      "MSE = 0.986314692784542\n",
      "MSE = 0.9863358098659601\n",
      "MSE = 0.9863334558561064\n",
      "MSE = 0.9863297205222723\n",
      "MSE = 0.9863265914045789\n",
      "MSE = 0.9863212378427167\n",
      "MSE = 0.9863197662038489\n",
      "MSE = 0.9863340821150922\n",
      "MSE = 0.9863870054449013\n",
      "MSE = 0.9864797968077259\n",
      "MSE = 0.9865360384153341\n",
      "MSE = 0.9865589051635214\n",
      "MSE = 0.9865751551984833\n",
      "MSE = 0.9871542075122601\n",
      "MSE = 0.9866058437220085\n",
      "MSE = 0.9865801870071526\n",
      "MSE = 0.9865760853614504\n",
      "MSE = 0.9865753308391263\n",
      "MSE = 0.9865751884966998\n",
      "MSE = 0.9865751615158629\n",
      "MSE = 0.9865751563970535\n",
      "MSE = 0.9865751554258191\n",
      "MSE = 0.9865751552414297\n",
      "MSE = 0.9865751552066401\n",
      "MSE = 0.9865751551997303\n",
      "MSE = 0.9865751551986697\n",
      "MSE = 0.9865751551985008\n",
      "MSE = 0.9865751551984846\n",
      "MSE = 0.9865751551984955\n",
      "MSE = 0.986575155198487\n",
      "MSE = 0.9865751551984846\n",
      "MSE = 0.9865751551984862\n",
      "MSE = 0.9865751551984846\n",
      "MSE = 0.9865752012769823\n",
      "MSE = 0.9865752114853221\n",
      "MSE = 0.9865755201057644\n",
      "MSE = 0.986576500910678\n",
      "MSE = 0.9865797143751865\n",
      "MSE = 0.986587235655349\n",
      "MSE = 0.9866007463807037\n",
      "MSE = 0.9866154896033811\n",
      "MSE = 0.9866224495308488\n",
      "MSE = 0.9866217018043837\n",
      "MSE = 0.9866208178554028\n",
      "MSE = 0.9866200292775751\n",
      "MSE = 0.9866209667297883\n",
      "MSE = 0.986628078391819\n",
      "MSE = 0.9866494759450831\n",
      "MSE = 0.9866879427205978\n",
      "MSE = 0.9870204255822513\n",
      "MSE = 0.9866869814911187\n",
      "MSE = 0.9867089284043093\n",
      "MSE = 0.986712075993279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.9867104509815025\n",
      "MSE = 0.9867094291989861\n",
      "MSE = 0.9867081093267677\n",
      "MSE = 0.9867104391852625\n",
      "MSE = 0.9867241918666048\n",
      "MSE = 0.9867605720141834\n",
      "MSE = 0.9868166188258535\n",
      "MSE = 0.9867749470878149\n",
      "MSE = 0.9868569702638745\n",
      "MSE = 0.9867953077462814\n",
      "MSE = 0.9868661281270799\n",
      "MSE = 0.9868102664157358\n",
      "MSE = 0.9868656551361018\n",
      "MSE = 0.9868202978994945\n",
      "MSE = 0.986864887275064\n",
      "MSE = 0.9868272802456884\n",
      "MSE = 0.9868216530546479\n",
      "MSE = 0.9868205712934234\n",
      "MSE = 0.9868203534822335\n",
      "MSE = 0.9868203092176432\n",
      "MSE = 0.9868203002051508\n",
      "MSE = 0.986820298369123\n",
      "MSE = 0.9868202979954149\n",
      "MSE = 0.9868202979190244\n",
      "MSE = 0.986820297969391\n",
      "MSE = 0.986820297929405\n",
      "MSE = 0.9868202979210295\n",
      "MSE = 0.9868202979190244\n",
      "1.105219727805016\n"
     ]
    }
   ],
   "source": [
    "lamb_values = [1e-5,1.3e-5,1.5e-5,1.7e-5,1.8e-5,2e-5,5e-5]\n",
    "MSE_valid =[]\n",
    "best_theta = []\n",
    "best_MSE = 0\n",
    "\n",
    "for lamb in lamb_values:\n",
    "    print(\"Lambda = {}\".format(lamb))\n",
    "  \n",
    "    theta_init = [alpha] + [0.0]*(nUsers+nItems)\n",
    "    unpack(theta_init)\n",
    "    \n",
    "    def cost(theta, labels, lamb):\n",
    "        unpack(theta)\n",
    "        predictions = [prediction(user, book) for user,book in Xtrain]\n",
    "        cost = MSE(predictions, labels)\n",
    "        print(\"MSE = \" + str(cost))\n",
    "        for u in userBiases:\n",
    "            cost += lamb*userBiases[u]**2\n",
    "        for i in itemBiases:\n",
    "            cost += lamb*itemBiases[i]**2\n",
    "        return cost\n",
    "\n",
    "    def derivative(theta, labels, lamb):\n",
    "        unpack(theta)\n",
    "        N = len(dataset)\n",
    "        dalpha = 0\n",
    "        dUserBiases = defaultdict(float)\n",
    "        dItemBiases = defaultdict(float)\n",
    "        for value in zip(Xtrain,ytrain):\n",
    "            x,rating = value\n",
    "            user = x[0]\n",
    "            book = x[1]\n",
    "            pred = prediction(user,book)\n",
    "            diff = pred - rating\n",
    "            dalpha += 2/N*diff\n",
    "            dUserBiases[user] += 2/N*diff\n",
    "            dItemBiases[book] += 2/N*diff\n",
    "        for u in userBiases:\n",
    "            dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "        for i in itemBiases:\n",
    "            dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "        dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "        return numpy.array(dtheta)\n",
    "    \n",
    "    theta,_,_ = scipy.optimize.fmin_l_bfgs_b(cost, theta_init, derivative, args = (ytrain, lamb))\n",
    "\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(user, book) for user,book in Xvalid]\n",
    "    cost = MSE(predictions, yvalid)\n",
    "    \n",
    "    if best_MSE is 0:\n",
    "        best_MSE = cost\n",
    "        best_theta = theta\n",
    "        print(\"Save best theta...\")\n",
    "    else:\n",
    "        if cost < best_MSE:\n",
    "            best_MSE = cost\n",
    "            best_theta = theta\n",
    "            print(\"Save best theta...\")\n",
    "    \n",
    "    MSE_valid.append(cost)\n",
    "    print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamb-1e-05 gives Validation MSE of 1.0741282593688577\n",
      "Lamb-1.3e-05 gives Validation MSE of 1.0735889944967134\n",
      "Lamb-1.5e-05 gives Validation MSE of 1.0734200113625347\n",
      "Lamb-1.7e-05 gives Validation MSE of 1.0737866337278643\n",
      "Lamb-1.8e-05 gives Validation MSE of 1.074256054597343\n",
      "Lamb-2e-05 gives Validation MSE of 1.076102050248465\n",
      "Lamb-5e-05 gives Validation MSE of 1.105219727805016\n"
     ]
    }
   ],
   "source": [
    "for lamb, MSE in zip(lamb_values ,MSE_valid):\n",
    "    print (\"Lamb-{} gives Validation MSE of {}\".format(lamb, MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18528\n",
      "11357\n",
      "3.8115271045716583\n",
      "[-0.55554225 -0.62195259 -3.27765648 ...  0.32498864 -0.19803966\n",
      " -0.00600419]\n",
      "[ 0.36158633  0.64962104 -0.6115337  ...  0.02200522  0.17283829\n",
      " -0.22501942]\n"
     ]
    }
   ],
   "source": [
    "print(len(best_theta))\n",
    "\n",
    "print(best_theta[0])\n",
    "print(best_theta[1:nUsers+1])\n",
    "print(best_theta[nUsers+1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 Kaggle Submission - Lambda=1.2e-5,  MSE=1.143, User_Name='Luke Liem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0aca62c198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.xlabel('Lambda')\n",
    "plt.title('Validation MSE')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.plot(lamb_values, MSE_valid, label='Validation MSE')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.81233938,  0.3113362 ,  0.46095858, ..., -0.3010043 ,\n",
       "       -0.01805242, -0.26187858])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rating baseline: compute averages for each user, or return \n",
    "### the global average if we've never seen the user before\n",
    "\n",
    "predictions = open(\"assignment1/predictions_Rating.txt\", 'w')\n",
    "unpack(best_theta)\n",
    "\n",
    "for l in open(\"assignment1/pairs_Rating.txt\"):\n",
    "  # write header  \n",
    "  if l.startswith(\"userID\"):\n",
    "    predictions.write(l)\n",
    "    continue\n",
    "    \n",
    "  # write user-item-rating\n",
    "  u,b = l.strip().split('-')\n",
    "  predictions.write(u + '-' + b + ',' + str(prediction(u,b)) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Model\n",
    "\n",
    "### Try to replicate Assignment Result for Bias-Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_index(list, id):\n",
    "    return list.index(id)\n",
    "\n",
    "def id_to_index_reformat(X):\n",
    "    x_new = []\n",
    "    \n",
    "    for x in X:\n",
    "        uid = x[0]\n",
    "        bid = x[1]\n",
    "        # print(uid,bid)\n",
    "        \n",
    "        uidx = lookup_index(user_ids,uid)\n",
    "        bidx = lookup_index(book_ids,bid)\n",
    "        # print(uidx,bidx)\n",
    "        \n",
    "        x_new.append([uidx,bidx])\n",
    "    return x_new\n",
    "    \n",
    "\n",
    "def l2_regularize(array):\n",
    "    loss = torch.sum(array ** 2.0)\n",
    "    return loss\n",
    "\n",
    "class MF(nn.Module):\n",
    "    # itr = 0\n",
    "    \n",
    "    def __init__(self, n_user, n_item, k=1, c_vector=1.0, c_bias=1.0, writer=None, mean=0):\n",
    "        super(MF, self).__init__()\n",
    "        self.writer = writer\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.c_bias = c_bias\n",
    "        self.c_vector = c_vector\n",
    "        \n",
    "        # gammas (users and items)\n",
    "        # self.user = nn.Embedding(n_user, k)\n",
    "        # self.item = nn.Embedding(n_item, k)\n",
    "        \n",
    "        # alpha and betas (users and items)\n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        # Initialize\n",
    "        self.bias.data.fill_(mean)\n",
    "        self.bias_user.weight.data.fill_(0)\n",
    "        self.bias_item.weight.data.fill_(0)\n",
    "    \n",
    "    def forward(self, train_x):\n",
    "        user_id = train_x[:, 0]\n",
    "        item_id = train_x[:, 1]\n",
    "        # vector_user = self.user(user_id)\n",
    "        # vector_item = self.item(item_id)\n",
    "        \n",
    "        # Pull out biases\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        biases = (self.bias + bias_user + bias_item)\n",
    "        \n",
    "        # ui_interaction = torch.sum(vector_user * vector_item, dim=1)\n",
    "        \n",
    "        # Add bias prediction to the interaction prediction\n",
    "        # prediction = ui_interaction + biases\n",
    "        \n",
    "        prediction = biases\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "        \n",
    "        # BUG - PyTorch optimizer already takes care of regularization!!!\n",
    "        \n",
    "        # prior_bias_user =  l2_regularize(self.bias_user.weight) * self.c_bias\n",
    "        # prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias\n",
    "        # prior_user =  l2_regularize(self.user.weight) * self.c_vector\n",
    "        # prior_item = l2_regularize(self.item.weight) * self.c_vector\n",
    "        # total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item\n",
    "        \n",
    "        total = loss_mse\n",
    "        \n",
    "        # for name, var in locals().items():\n",
    "        #    if type(var) is torch.Tensor and var.nelement() == 1 and self.writer is not None:\n",
    "        #        self.writer.add_scalar(name, var, self.itr)\n",
    "        \n",
    "        return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 3 columns):\n",
      "userID    200000 non-null object\n",
      "bookID    200000 non-null object\n",
      "rating    200000 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 4.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>bookID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u79354815</td>\n",
       "      <td>b14275065</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u56917948</td>\n",
       "      <td>b82152306</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u97915914</td>\n",
       "      <td>b44882292</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u49688858</td>\n",
       "      <td>b79927466</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u08384938</td>\n",
       "      <td>b05683889</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userID     bookID  rating\n",
       "0  u79354815  b14275065       4\n",
       "1  u56917948  b82152306       5\n",
       "2  u97915914  b44882292       5\n",
       "3  u49688858  b79927466       5\n",
       "4  u08384938  b05683889       2"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../datasets/cse258/assignment1/train_Interactions.csv\")\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7170 ['b14275065', 'b82152306', 'b44882292']\n",
      "11357 ['u79354815', 'u56917948', 'u97915914']\n",
      "b52453648\n",
      "203\n",
      "u67309666\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "book_ids = list(data['bookID'].unique())\n",
    "user_ids = list(data['userID'].unique())\n",
    "\n",
    "print(len(book_ids), book_ids[:3])\n",
    "print(len(user_ids), user_ids[:3])\n",
    "\n",
    "print(book_ids[203])\n",
    "print(lookup_index(book_ids,'b52453648'))\n",
    "\n",
    "print(user_ids[108])\n",
    "print(lookup_index(user_ids,'u67309666'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 0.0001\n",
      "MSE = 1.472911714464408\n",
      "MSE = 1.4555257179723937\n",
      "MSE = 1.3919639784789475\n",
      "MSE = 7.490819683692902\n",
      "MSE = 1.376739341545793\n",
      "MSE = 1.2204798698615391\n",
      "MSE = 1.218624237734379\n",
      "MSE = 1.211367353215079\n",
      "MSE = 1.1849901152863394\n",
      "MSE = 1.0959368812255847\n",
      "MSE = 1.0763647182868918\n",
      "MSE = 1.0639778627822707\n",
      "MSE = 1.0646558928619114\n",
      "MSE = 1.0653861842715504\n",
      "MSE = 1.0645559180032453\n",
      "MSE = 1.0635716123352426\n",
      "MSE = 1.063520431831953\n",
      "MSE = 1.064112002933143\n",
      "MSE = 1.063545660268213\n",
      "MSE = 1.0635222496589456\n",
      "MSE = 1.0635205667898529\n",
      "MSE = 1.0635204418734128\n",
      "MSE = 1.0635204325792058\n",
      "MSE = 1.0635204318876086\n",
      "MSE = 1.0635204318360503\n",
      "MSE = 1.0635204318322693\n",
      "MSE = 1.0635204318322953\n",
      "MSE = 1.0635204318342046\n",
      "MSE = 1.0635204318324025\n",
      "MSE = 1.063520431832299\n",
      "MSE = 1.0635204318322953\n",
      "Save best theta...\n",
      "1.1775359456991676\n",
      "Lambda = 8e-05\n",
      "MSE = 1.476119044755357\n",
      "MSE = 2.3379034429529293\n",
      "MSE = 1.472791016758991\n",
      "MSE = 1.4725847660744202\n",
      "MSE = 1.4720051472399327\n",
      "MSE = 1.4705350695072075\n",
      "MSE = 1.4666881176987476\n",
      "MSE = 1.4569375758164216\n",
      "MSE = 1.4331864446120983\n",
      "MSE = 1.3818676407886303\n",
      "MSE = 1.299017103236589\n",
      "MSE = 1.166131861982756\n",
      "MSE = 1.1051003453458212\n",
      "MSE = 1.106587410219358\n",
      "MSE = 1.1083524756257412\n",
      "MSE = 1.108756589604985\n",
      "MSE = 1.1037178945074704\n",
      "MSE = 1.0803139093896565\n",
      "MSE = 1.0894952002600233\n",
      "MSE = 1.0804672459408307\n",
      "MSE = 1.0657033351632539\n",
      "MSE = 1.0599373144166144\n",
      "MSE = 1.056636673197011\n",
      "MSE = 1.0424444120749885\n",
      "MSE = 1.0902151975206793\n",
      "MSE = 1.0432970497047045\n",
      "MSE = 1.044251963733992\n",
      "MSE = 1.0339643968882986\n",
      "MSE = 1.0442525153472235\n",
      "MSE = 1.0373721929038424\n",
      "MSE = 1.0359341951420467\n",
      "MSE = 1.0375405647887277\n",
      "MSE = 1.0363814370391937\n",
      "MSE = 1.0360064187399955\n",
      "MSE = 1.0367780804288529\n",
      "MSE = 1.0361331728272143\n",
      "MSE = 1.0356134742732572\n",
      "MSE = 1.0359613313692189\n",
      "MSE = 1.0357418498065942\n",
      "MSE = 1.034710043431415\n",
      "MSE = 1.0353861498268904\n",
      "MSE = 1.035595341465014\n",
      "MSE = 1.0356396930147653\n",
      "MSE = 1.0356011685886743\n",
      "MSE = 1.0355961581463227\n",
      "MSE = 1.0355954569379595\n",
      "MSE = 1.0355953578123418\n",
      "MSE = 1.0355953437796688\n",
      "MSE = 1.0355953417927222\n",
      "MSE = 1.03559534151144\n",
      "MSE = 1.0355953414716026\n",
      "MSE = 1.0355953414659393\n",
      "MSE = 1.0355953414652115\n",
      "MSE = 1.0355953414650274\n",
      "MSE = 1.0355953414650145\n",
      "MSE = 1.035595341465014\n",
      "MSE = 1.035595341465014\n",
      "MSE = 1.035595341465014\n",
      "MSE = 1.0355953414650143\n",
      "MSE = 1.035595341465014\n",
      "MSE = 1.0355953414650143\n",
      "MSE = 1.035595341465014\n",
      "MSE = 1.0355952369593917\n",
      "MSE = 1.0355950818230242\n",
      "MSE = 1.03559320039298\n",
      "MSE = 1.0355899463744813\n",
      "MSE = 1.0355820185288735\n",
      "MSE = 1.035570774372255\n",
      "MSE = 1.0355558240003748\n",
      "MSE = 1.0355534527547672\n",
      "MSE = 1.0355529497501081\n",
      "MSE = 1.0355517557077567\n",
      "MSE = 1.035551196066369\n",
      "MSE = 1.0355513025077037\n",
      "MSE = 1.0355513244492982\n",
      "MSE = 1.0355513617762602\n",
      "MSE = 1.035551674190648\n",
      "MSE = 1.0355527797361266\n",
      "MSE = 1.0355555460373502\n",
      "MSE = 1.0355601052964651\n",
      "MSE = 1.035586463450067\n",
      "MSE = 1.0355615221056114\n",
      "MSE = 1.0355760692033653\n",
      "MSE = 1.035566232193139\n",
      "MSE = 1.0355652146041776\n",
      "MSE = 1.035563586210123\n",
      "MSE = 1.0355622878544437\n",
      "MSE = 1.0358927974402439\n",
      "MSE = 1.035561048988863\n",
      "MSE = 1.0355634507657103\n",
      "MSE = 1.035823104035973\n",
      "MSE = 1.0355587877267303\n",
      "MSE = 1.03556139546145\n",
      "MSE = 1.0355593751513108\n",
      "MSE = 1.0355390848487724\n",
      "MSE = 1.035524620466146\n",
      "MSE = 1.0355350527649867\n",
      "MSE = 1.0355296743458104\n",
      "MSE = 1.0355222925137508\n",
      "MSE = 1.0354747408430585\n",
      "MSE = 1.0354284739245077\n",
      "MSE = 1.0353571715054226\n",
      "MSE = 1.035040528354624\n",
      "MSE = 1.0352655060026994\n",
      "MSE = 1.0353212845158695\n",
      "MSE = 1.0351871411560531\n",
      "MSE = 1.0351357873212108\n",
      "MSE = 1.0351019990481383\n",
      "MSE = 1.0348815302552337\n",
      "MSE = 1.0350448177427796\n",
      "MSE = 1.0350818186528903\n",
      "MSE = 1.0350411742998133\n",
      "MSE = 1.0349618147063573\n",
      "MSE = 1.0348661093275064\n",
      "MSE = 1.0347637155815503\n",
      "MSE = 1.0340221277396453\n",
      "MSE = 1.0305998819702873\n",
      "MSE = 1.033579011216853\n",
      "MSE = 1.0332523354081724\n",
      "MSE = 1.0334617147696568\n",
      "MSE = 1.0331991639430196\n",
      "MSE = 1.033927726989177\n",
      "MSE = 1.0316965885954499\n",
      "MSE = 1.0332710460161223\n",
      "MSE = 1.0320337694199488\n",
      "MSE = 1.0329203027702512\n",
      "MSE = 1.0324335478190207\n",
      "MSE = 1.0326013839842068\n",
      "MSE = 1.0332540657242906\n",
      "MSE = 1.0338920249139325\n",
      "MSE = 1.0334049410061688\n",
      "MSE = 1.0336388695250802\n",
      "MSE = 1.03375644640473\n",
      "MSE = 1.0338142998175146\n",
      "MSE = 1.0340743928388512\n",
      "MSE = 1.0338767852401751\n",
      "MSE = 1.0338979180303525\n",
      "MSE = 1.0338691139944618\n",
      "MSE = 1.033970276776563\n",
      "MSE = 1.0339346533224765\n",
      "MSE = 1.0338513219500443\n",
      "MSE = 1.0339164705697579\n",
      "MSE = 1.0339502999623509\n",
      "MSE = 1.0340214592019659\n",
      "MSE = 1.0339608707890178\n",
      "MSE = 1.0338561862406048\n",
      "MSE = 1.0335073421079077\n",
      "MSE = 1.0337254102010904\n",
      "MSE = 1.0336232044877511\n",
      "MSE = 1.0332212221347488\n",
      "MSE = 1.0325101561053132\n",
      "MSE = 1.032462735777953\n",
      "MSE = 1.0323804465806612\n",
      "MSE = 1.0319786493107208\n",
      "MSE = 1.031301440306049\n",
      "MSE = 1.0298559837447132\n",
      "MSE = 1.0309374305326433\n",
      "MSE = 1.0312029897300499\n",
      "MSE = 1.0312330977953026\n",
      "MSE = 1.030679549062481\n",
      "MSE = 1.0309550736176418\n",
      "MSE = 1.0306290821965707\n",
      "MSE = 1.0293783565081203\n",
      "MSE = 1.0289244552303716\n",
      "MSE = 1.0284388316352635\n",
      "MSE = 1.0279449542614778\n",
      "MSE = 1.0277667107082202\n",
      "MSE = 1.027655199700979\n",
      "MSE = 1.0275293618516388\n",
      "MSE = 1.0274681664623129\n",
      "MSE = 1.027403665545183\n",
      "MSE = 1.027288297656751\n",
      "MSE = 1.0271600662262967\n",
      "MSE = 1.02722278523951\n",
      "MSE = 1.0272902124275256\n",
      "MSE = 1.0272447667008728\n",
      "MSE = 1.0273254686929307\n",
      "MSE = 1.0273782708548638\n",
      "MSE = 1.0273338320241556\n",
      "MSE = 1.0273270580751586\n",
      "MSE = 1.0273257807243885\n",
      "MSE = 1.027325530339793\n",
      "MSE = 1.0273254808874848\n",
      "MSE = 1.0273254711057016\n",
      "MSE = 1.0273254691703035\n",
      "MSE = 1.0273254687874382\n",
      "MSE = 1.027325468711613\n",
      "MSE = 1.0273254686966138\n",
      "MSE = 1.0273254687064992\n",
      "MSE = 1.0273254686985567\n",
      "MSE = 1.0273254686970437\n",
      "MSE = 1.0273254686966138\n",
      "Save best theta...\n",
      "1.1570730940030738\n",
      "Lambda = 6e-05\n",
      "MSE = 1.476666340106276\n",
      "MSE = 2.332619128582312\n",
      "MSE = 1.472791511316795\n",
      "MSE = 1.4725796478131867\n",
      "MSE = 1.472052833750493\n",
      "MSE = 1.4706525072985976\n",
      "MSE = 1.4670509360329256\n",
      "MSE = 1.4578505140676836\n",
      "MSE = 1.435441563055763\n",
      "MSE = 1.386555917491326\n",
      "MSE = 1.3053077353133398\n",
      "MSE = 1.2058031925779529\n",
      "MSE = 1.1483449851176224\n",
      "MSE = 1.1431835840025621\n",
      "MSE = 1.1407582756796015\n",
      "MSE = 1.13341083553036\n",
      "MSE = 1.113348028765946\n",
      "MSE = 1.0891032694901828\n",
      "MSE = 1.087370398136343\n",
      "MSE = 1.0873016197465126\n",
      "MSE = 1.087221781282206\n",
      "MSE = 1.086554773814793\n",
      "MSE = 1.0837472008288038\n",
      "MSE = 1.0752577138697617\n",
      "MSE = 1.057748247681351\n",
      "MSE = 2.347406088989271\n",
      "MSE = 1.0607809871696605\n",
      "MSE = 1.0457286254828086\n",
      "MSE = 1.040839409568\n",
      "MSE = 1.0413140278220099\n",
      "MSE = 1.0418251314764304\n",
      "MSE = 1.0430103035833012\n",
      "MSE = 1.0453959007044242\n",
      "MSE = 1.0457632194144908\n",
      "MSE = 1.0403721427361994\n",
      "MSE = 1.029170225172433\n",
      "MSE = 1.0212561916145335\n",
      "MSE = 1.0169609535034054\n",
      "MSE = 1.439746272248598\n",
      "MSE = 1.0189893764880837\n",
      "MSE = 1.0175431290234975\n",
      "MSE = 1.014114661064508\n",
      "MSE = 1.0072877140804344\n",
      "MSE = 1.0124498193783424\n",
      "MSE = 1.0115144564369565\n",
      "MSE = 1.0107348991385472\n",
      "MSE = 1.0104360479806094\n",
      "MSE = 1.0098790697053084\n",
      "MSE = 1.0079900938839164\n",
      "MSE = 1.0066082443776685\n",
      "MSE = 1.000439909643025\n",
      "MSE = 1.0052957995574479\n",
      "MSE = 1.0044611798968035\n",
      "MSE = 1.002642048824656\n",
      "MSE = 1.003390404316867\n",
      "MSE = 1.002820016570072\n",
      "MSE = 1.0031956035098286\n",
      "MSE = 1.0080103755886904\n",
      "MSE = 1.003537825774337\n",
      "MSE = 1.0032406812934735\n",
      "MSE = 1.0032019265933252\n",
      "MSE = 1.003196498108961\n",
      "MSE = 1.0031957302325676\n",
      "MSE = 1.003195621463661\n",
      "MSE = 1.0031956060535443\n",
      "MSE = 1.0031956038702987\n",
      "MSE = 1.0031956035609624\n",
      "MSE = 1.003195603517047\n",
      "MSE = 1.0031956035108536\n",
      "MSE = 1.003195603509956\n",
      "MSE = 1.0031956035105396\n",
      "MSE = 1.003195603510045\n",
      "MSE = 1.003195603509959\n",
      "MSE = 1.003195603509959\n",
      "MSE = 1.0031956035100023\n",
      "MSE = 1.0031956035099614\n",
      "MSE = 1.003195603509959\n",
      "Save best theta...\n",
      "1.1386666039096667\n",
      "Lambda = 5e-05\n",
      "MSE = 1.4770030001173313\n",
      "MSE = 2.329338812448061\n",
      "MSE = 1.4728001633263659\n",
      "MSE = 1.4725849886090885\n",
      "MSE = 1.4720831172072648\n",
      "MSE = 1.4707138188421653\n",
      "MSE = 1.467225179219486\n",
      "MSE = 1.4582757935200188\n",
      "MSE = 1.4364859978837987\n",
      "MSE = 1.38875916038592\n",
      "MSE = 1.3084951844879793\n",
      "MSE = 1.2214967117484603\n",
      "MSE = 1.1811887747876926\n",
      "MSE = 1.1791370962640926\n",
      "MSE = 1.1796846181359475\n",
      "MSE = 1.1789535631765518\n",
      "MSE = 1.1731200288948895\n",
      "MSE = 1.154946165134956\n",
      "MSE = 1.118184475596605\n",
      "MSE = 2.5169650745398395\n",
      "MSE = 1.120135251150454\n",
      "MSE = 1.0918930243037095\n",
      "MSE = 1.0816085887591464\n",
      "MSE = 1.0802023363095445\n",
      "MSE = 1.0793623783571538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.0755826536727273\n",
      "MSE = 1.064329163914763\n",
      "MSE = 1.0459837725756636\n",
      "MSE = 1.0264969843614045\n",
      "MSE = 1.0318745364152961\n",
      "MSE = 1.0272989867652491\n",
      "MSE = 1.0269275848184884\n",
      "MSE = 1.0128956860437517\n",
      "MSE = 1.011448653415465\n",
      "MSE = 1.008924831963294\n",
      "MSE = 1.0043755410997557\n",
      "MSE = 1.0204616478382984\n",
      "MSE = 1.0060045724344557\n",
      "MSE = 1.0041767290391173\n",
      "MSE = 0.9966446756779305\n",
      "MSE = 0.994257001788545\n",
      "MSE = 0.9935237754136971\n",
      "MSE = 0.9940148315645186\n",
      "MSE = 0.9946703221290986\n",
      "MSE = 0.9952191086694048\n",
      "MSE = 0.9956543269167126\n",
      "MSE = 1.001204375742941\n",
      "MSE = 0.9953873843464361\n",
      "MSE = 0.9939895061598008\n",
      "MSE = 0.9907717704898347\n",
      "MSE = 0.9896108641437549\n",
      "MSE = 0.9884742721900621\n",
      "MSE = 0.9877533668737032\n",
      "MSE = 1.1181419652324187\n",
      "MSE = 0.9876005975426527\n",
      "MSE = 0.9877715887676483\n",
      "MSE = 0.9892533410685949\n",
      "MSE = 0.9881362146679821\n",
      "MSE = 0.9902859920172785\n",
      "MSE = 0.9884264611126778\n",
      "MSE = 0.9884870671825574\n",
      "MSE = 0.9883218371742423\n",
      "MSE = 1.0015371746649422\n",
      "MSE = 0.9883138502855044\n",
      "MSE = 0.9876978935705786\n",
      "MSE = 1.083167719357064\n",
      "MSE = 0.9885892193861419\n",
      "MSE = 0.9878754334364244\n",
      "MSE = 0.9870049252296892\n",
      "MSE = 0.9858521968709609\n",
      "MSE = 0.9850097659009172\n",
      "MSE = 0.9831248028620432\n",
      "MSE = 0.9847289870001595\n",
      "MSE = 0.9846963173089315\n",
      "MSE = 0.9847186187945381\n",
      "MSE = 0.9848384425266651\n",
      "MSE = 0.9850555377987537\n",
      "MSE = 0.9854668723136915\n",
      "MSE = 0.9859070788094333\n",
      "MSE = 0.9858536051821244\n",
      "MSE = 0.9858866826424747\n",
      "MSE = 0.9858864361240621\n",
      "MSE = 0.9858330755791946\n",
      "MSE = 0.985709249725705\n",
      "MSE = 0.9855506446978554\n",
      "MSE = 0.9855420827017143\n",
      "MSE = 0.9855117591036034\n",
      "MSE = 0.9854965578239997\n",
      "MSE = 0.9857394271055415\n",
      "MSE = 0.9888630458051382\n",
      "MSE = 0.9857731621716808\n",
      "MSE = 0.9857841314998258\n",
      "MSE = 0.9857063159589673\n",
      "MSE = 0.9857433290174208\n",
      "MSE = 0.9858017299352018\n",
      "MSE = 0.9856226524974697\n",
      "MSE = 0.9857049030210572\n",
      "MSE = 0.9858851256909839\n",
      "MSE = 0.9857514905629754\n",
      "MSE = 0.9921929370427992\n",
      "MSE = 0.9857556709354717\n",
      "MSE = 0.9858609322965122\n",
      "MSE = 0.9857761971178192\n",
      "MSE = 0.9858087405289392\n",
      "MSE = 0.985818798097727\n",
      "MSE = 0.9857893353122438\n",
      "MSE = 0.9857875156820315\n",
      "MSE = 0.9857861979683389\n",
      "MSE = 0.9857859589959352\n",
      "MSE = 0.9857979078484346\n",
      "MSE = 0.9858307281090866\n",
      "MSE = 0.9858039980068153\n",
      "MSE = 0.9857991913521723\n",
      "MSE = 0.9858023619052717\n",
      "MSE = 0.9857998505442489\n",
      "MSE = 0.9857993302558512\n",
      "MSE = 0.9857991913521723\n",
      "Save best theta...\n",
      "1.1266337171951946\n",
      "Lambda = 4e-05\n",
      "MSE = 1.4775761373609133\n",
      "MSE = 2.3235392718828294\n",
      "MSE = 1.4728014058185503\n",
      "MSE = 1.472580584475194\n",
      "MSE = 1.472113882465846\n",
      "MSE = 1.4707855125347746\n",
      "MSE = 1.4674515604964324\n",
      "MSE = 1.4588411062987545\n",
      "MSE = 1.4378856443747012\n",
      "MSE = 1.391707489613806\n",
      "MSE = 1.3128800796162843\n",
      "MSE = 1.2294174773630662\n",
      "MSE = 1.192806078188014\n",
      "MSE = 1.1907403786626425\n",
      "MSE = 1.1915973972297371\n",
      "MSE = 1.1918516592560164\n",
      "MSE = 1.1901999744686615\n",
      "MSE = 1.1826599859155793\n",
      "MSE = 1.1610959261445661\n",
      "MSE = 1.123027028389845\n",
      "MSE = 1.2721304775712212\n",
      "MSE = 1.122094287236202\n",
      "MSE = 1.0873519224408092\n",
      "MSE = 1.0727859905178443\n",
      "MSE = 1.0696383031282246\n",
      "MSE = 1.0675325871048078\n",
      "MSE = 1.0563110232980806\n",
      "MSE = 1.0391768470164147\n",
      "MSE = 1.015104294026006\n",
      "MSE = 1.0247394037345432\n",
      "MSE = 1.0139765193444017\n",
      "MSE = 1.0162259977314019\n",
      "MSE = 1.216244900556745\n",
      "MSE = 1.0142419049854534\n",
      "MSE = 1.0147906671658808\n",
      "MSE = 0.9819523004722506\n",
      "MSE = 0.9915159969579729\n",
      "MSE = 0.9916510451987086\n",
      "MSE = 0.9922066357842723\n",
      "MSE = 0.9934277043172439\n",
      "MSE = 0.9921778207861154\n",
      "MSE = 0.9867223820489806\n",
      "MSE = 0.9801093855685562\n",
      "MSE = 0.9817072878710729\n",
      "MSE = 0.9795018609130607\n",
      "MSE = 0.9776143986884537\n",
      "MSE = 0.9741665433008038\n",
      "MSE = 0.9709122534227838\n",
      "MSE = 0.9724536644562778\n",
      "MSE = 0.9722997341097149\n",
      "MSE = 0.9730757766260857\n",
      "MSE = 0.9742514152418646\n",
      "MSE = 0.9746042404842988\n",
      "MSE = 0.9748866814661297\n",
      "MSE = 0.9745417910096115\n",
      "MSE = 0.9731971164784631\n",
      "MSE = 0.9713882825494581\n",
      "MSE = 0.966264840664723\n",
      "MSE = 0.9696647606418929\n",
      "MSE = 0.9692884257681197\n",
      "MSE = 0.9568837992340424\n",
      "MSE = 0.9683341574827379\n",
      "MSE = 0.9683849366372107\n",
      "MSE = 0.9683577818424971\n",
      "MSE = 0.9682631128996612\n",
      "MSE = 0.9680969081029935\n",
      "MSE = 0.9690251125994039\n",
      "MSE = 0.9682516692234433\n",
      "MSE = 0.9674057254669116\n",
      "MSE = 0.9667662445648795\n",
      "MSE = 0.966597603786131\n",
      "MSE = 0.9664408063954378\n",
      "MSE = 0.9662805487140661\n",
      "MSE = 0.9661162394667107\n",
      "MSE = 1.0024486766277936\n",
      "MSE = 0.9661183539605261\n",
      "MSE = 0.9660905184132439\n",
      "MSE = 0.9661118668276497\n",
      "MSE = 0.9661442175840276\n",
      "MSE = 0.9661681453051583\n",
      "MSE = 0.9661720589530208\n",
      "MSE = 0.9671845948838078\n",
      "MSE = 0.9662380841748875\n",
      "MSE = 0.9663311765756087\n",
      "MSE = 0.966677003982081\n",
      "MSE = 0.9681368164735736\n",
      "MSE = 0.9666611711296773\n",
      "MSE = 0.9668961027195375\n",
      "MSE = 0.9675650288713616\n",
      "MSE = 0.9668997860110853\n",
      "MSE = 0.967030971208027\n",
      "MSE = 0.9671648380147868\n",
      "MSE = 0.9670303931478256\n",
      "MSE = 0.9670455306845954\n",
      "MSE = 0.9675187567297997\n",
      "MSE = 0.9672024036726599\n",
      "MSE = 0.9670222125884417\n",
      "MSE = 0.9671278610147409\n",
      "MSE = 0.967101960677453\n",
      "MSE = 0.9670727172574476\n",
      "MSE = 0.9670696193175216\n",
      "MSE = 0.9671388666195042\n",
      "MSE = 0.9672578188427281\n",
      "MSE = 0.9671542300734881\n",
      "MSE = 0.9672401155658017\n",
      "MSE = 0.9672782941354094\n",
      "MSE = 0.9672896038457176\n",
      "MSE = 0.9672879618027965\n",
      "MSE = 0.9672122739182114\n",
      "MSE = 0.9680342084001781\n",
      "MSE = 0.9672447760918236\n",
      "MSE = 0.9671911971164433\n",
      "MSE = 0.9668976600532493\n",
      "MSE = 0.9666075979121651\n",
      "MSE = 0.9667724856564437\n",
      "MSE = 0.966582237382631\n",
      "MSE = 1.0237429622074858\n",
      "MSE = 0.9665701596504681\n",
      "MSE = 0.9662614330616269\n",
      "MSE = 0.9646264531133645\n",
      "MSE = 0.9658908978623104\n",
      "MSE = 0.9658224331707023\n",
      "MSE = 0.9657269316091104\n",
      "MSE = 0.9652087239473869\n",
      "MSE = 0.9640315423022873\n",
      "MSE = 0.9623822015429637\n",
      "MSE = 0.9635400790491959\n",
      "MSE = 0.963713546486203\n",
      "MSE = 0.9636065736844338\n",
      "MSE = 0.963587257638347\n",
      "MSE = 0.9689656949918778\n",
      "MSE = 0.9635194172616471\n",
      "MSE = 0.9628972543846951\n",
      "MSE = 1.0674281240649042\n",
      "MSE = 0.9630919147679985\n",
      "MSE = 0.9620446429314282\n",
      "MSE = 0.9606481189283228\n",
      "MSE = 0.9591821138932832\n",
      "MSE = 0.9567442641883634\n",
      "MSE = 0.9580228166231377\n",
      "MSE = 0.9581695980974334\n",
      "MSE = 0.9586351226674157\n",
      "MSE = 0.9585685135392604\n",
      "MSE = 0.9583750543930354\n",
      "MSE = 0.9578606430429808\n",
      "MSE = 0.9572966117730993\n",
      "MSE = 0.9574098432798988\n",
      "MSE = 0.9571679752215305\n",
      "MSE = 0.9568570391882677\n",
      "MSE = 0.9566237423268104\n",
      "MSE = 0.9561933154950147\n",
      "MSE = 0.9557878481850812\n",
      "MSE = 0.9553984400619908\n",
      "MSE = 0.9552477810852364\n",
      "MSE = 0.9552206643368067\n",
      "MSE = 0.961322965572764\n",
      "MSE = 0.9548078538935836\n",
      "MSE = 0.954818711744885\n",
      "MSE = 0.9548208376493268\n",
      "MSE = 0.9549561743142472\n",
      "MSE = 0.9548659817272244\n",
      "MSE = 0.9548382072943258\n",
      "MSE = 0.9546859364584379\n",
      "MSE = 0.9546300004631111\n",
      "MSE = 0.9550403058042861\n",
      "MSE = 0.954656240151688\n",
      "MSE = 0.9545746650375421\n",
      "MSE = 0.9544395150343221\n",
      "MSE = 0.954235089167066\n",
      "MSE = 0.953971639653283\n",
      "MSE = 0.9532570654403996\n",
      "MSE = 0.9535230686503813\n",
      "MSE = 0.9531764205883089\n",
      "MSE = 0.953006626117025\n",
      "MSE = 0.9529344816625928\n",
      "MSE = 0.9529073616120963\n",
      "MSE = 0.9527909239254759\n",
      "MSE = 0.9527538571070461\n",
      "MSE = 0.95238576654515\n",
      "MSE = 0.9524748875350064\n",
      "MSE = 0.9523337689086074\n",
      "MSE = 0.9519769026977911\n",
      "MSE = 0.9516890206047743\n",
      "MSE = 0.9499064053216701\n",
      "MSE = 0.9499239455136563\n",
      "MSE = 0.9491961684725021\n",
      "MSE = 0.9482353642959375\n",
      "MSE = 0.9488029713355889\n",
      "MSE = 0.9471142038435094\n",
      "MSE = 0.9459669019139304\n",
      "MSE = 0.9453091195155826\n",
      "MSE = 0.9427195265065955\n",
      "MSE = 3.787846185981177\n",
      "MSE = 0.9425178288337868\n",
      "MSE = 0.9418306581092354\n",
      "MSE = 0.9774504150674029\n",
      "MSE = 0.9407904575958226\n",
      "MSE = 0.9410039045249889\n",
      "MSE = 0.9400802401497526\n",
      "MSE = 0.9496820468812744\n",
      "MSE = 0.9396339929643094\n",
      "MSE = 0.9372308444997223\n",
      "MSE = 0.936592218163443\n",
      "MSE = 0.935418413437571\n",
      "MSE = 0.9355987080919218\n",
      "MSE = 0.9357389200990157\n",
      "MSE = 0.9354608990272222\n",
      "MSE = 0.9354033827664932\n",
      "MSE = 0.9345701945887365\n",
      "MSE = 0.9974285253025822\n",
      "MSE = 0.9340317972069139\n",
      "MSE = 0.931095702775714\n",
      "MSE = 0.9308891690400655\n",
      "MSE = 0.9299307502395023\n",
      "MSE = 0.9288012850436492\n",
      "MSE = 0.9294365723250632\n",
      "MSE = 0.928743247339163\n",
      "MSE = 0.9273255282682751\n",
      "MSE = 0.9252657428005185\n",
      "MSE = 0.9260497818223403\n",
      "MSE = 0.9257932846944942\n",
      "MSE = 0.9255134291994734\n",
      "MSE = 0.9248561095432919\n",
      "MSE = 0.9239013411479727\n",
      "MSE = 0.9232500880243973\n",
      "MSE = 0.9222875212328059\n",
      "MSE = 0.941238464339447\n",
      "MSE = 0.922364909442672\n",
      "MSE = 0.9210262490043845\n",
      "MSE = 0.9209318920447537\n",
      "MSE = 0.9207755951308233\n",
      "MSE = 0.9183490909723152\n",
      "MSE = 0.9201695690421281\n",
      "MSE = 0.9195338783180259\n",
      "MSE = 0.9164579710116906\n",
      "MSE = 1.9595302135263504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.9170057946779203\n",
      "MSE = 0.9162376545275299\n",
      "MSE = 0.91450882320836\n",
      "MSE = 0.8950190269023306\n",
      "MSE = 0.9086502577825432\n",
      "MSE = 0.909068951479074\n",
      "MSE = 0.9088098129966606\n",
      "MSE = 0.9089679080349716\n",
      "MSE = 0.9087998766392931\n",
      "MSE = 0.9081875343910999\n",
      "MSE = 0.9070125909552779\n",
      "MSE = 0.9063289623135159\n",
      "MSE = 0.9038696422650696\n",
      "MSE = 0.9054282962532146\n",
      "MSE = 0.906608545605546\n",
      "MSE = 0.9056674985256213\n",
      "MSE = 0.9059335481625073\n",
      "MSE = 0.9060669037556501\n",
      "MSE = 0.9061577910255978\n",
      "MSE = 0.9059417163286224\n",
      "MSE = 0.9075279164282988\n",
      "MSE = 0.9057096767353222\n",
      "MSE = 0.9048199155898777\n",
      "MSE = 0.903607990628909\n",
      "MSE = 0.9029395521262881\n",
      "MSE = 0.9028035709619533\n",
      "MSE = 0.9026951043453832\n",
      "MSE = 0.9021823485513939\n",
      "MSE = 1.1643824958205886\n",
      "MSE = 0.9022408843387715\n",
      "MSE = 0.9013647071620187\n",
      "MSE = 0.8942106219150132\n",
      "MSE = 0.8985093159338395\n",
      "MSE = 0.8983643693722195\n",
      "MSE = 0.8980775785935566\n",
      "MSE = 0.8974979295039172\n",
      "MSE = 0.8976465622763516\n",
      "MSE = 0.897580344034824\n",
      "MSE = 0.8974843500963314\n",
      "MSE = 0.8974220195531337\n",
      "MSE = 0.8969646253149275\n",
      "MSE = 0.8951143881365395\n",
      "MSE = 0.8957131658616193\n",
      "MSE = 0.8952161837935446\n",
      "MSE = 0.8953079690340153\n",
      "MSE = 0.883098265642973\n",
      "MSE = 0.8944346204607301\n",
      "MSE = 0.8946004301188812\n",
      "MSE = 0.8924776343832984\n",
      "MSE = 0.8938890909944059\n",
      "MSE = 0.894030420767309\n",
      "MSE = 0.8939934588993463\n",
      "MSE = 0.8938398319668288\n",
      "MSE = 0.8935400489907586\n",
      "MSE = 0.893002809289663\n",
      "MSE = 0.8890370572646126\n",
      "MSE = 0.8920500659885924\n",
      "MSE = 0.8916143662254138\n",
      "MSE = 0.8917717145327874\n",
      "MSE = 0.8920397803052795\n",
      "MSE = 0.8922974536696805\n",
      "MSE = 0.8924825705615875\n",
      "MSE = 0.8924150035961215\n",
      "MSE = 0.8922342963414273\n",
      "MSE = 0.8922127757621966\n",
      "MSE = 0.8912918003209769\n",
      "MSE = 0.891289102126467\n",
      "MSE = 0.8915258038558546\n",
      "MSE = 0.8969000809038647\n",
      "MSE = 0.8915142354322382\n",
      "MSE = 0.891582780109545\n",
      "MSE = 0.973831987258186\n",
      "MSE = 0.8912993050134237\n",
      "MSE = 0.8914605116149198\n",
      "MSE = 0.890688907824783\n",
      "MSE = 0.8887860901248357\n",
      "MSE = 0.8878712947008208\n",
      "MSE = 0.8878563489218475\n",
      "MSE = 0.8865087858560743\n",
      "MSE = 0.8865601415823375\n",
      "MSE = 0.8856651516966912\n",
      "MSE = 0.8861015310451594\n",
      "MSE = 0.9010168108441431\n",
      "MSE = 0.8861455978057525\n",
      "MSE = 0.8861091182418097\n",
      "MSE = 0.8861031286656497\n",
      "MSE = 0.8861070778700973\n",
      "MSE = 0.8861039435414072\n",
      "MSE = 0.8861033003141607\n",
      "MSE = 0.8861031286656497\n",
      "1.1428255125095133\n",
      "Lambda = 3e-05\n",
      "MSE = 1.4781501490584525\n",
      "MSE = 2.317754543073394\n",
      "MSE = 1.4728029736465889\n",
      "MSE = 1.472576673138386\n",
      "MSE = 1.4721375875758487\n",
      "MSE = 1.4708382545840024\n",
      "MSE = 1.4676208626096996\n",
      "MSE = 1.4592623093100758\n",
      "MSE = 1.4389382938289585\n",
      "MSE = 1.3939715294999815\n",
      "MSE = 1.316455460227791\n",
      "MSE = 1.2337993009591173\n",
      "MSE = 1.1965861419781043\n",
      "MSE = 1.1934637549977578\n",
      "MSE = 1.194144963573273\n",
      "MSE = 1.1941949573576789\n",
      "MSE = 1.192276377042785\n",
      "MSE = 1.1851546986741093\n",
      "MSE = 1.16489043312643\n",
      "MSE = 1.1280459768598936\n",
      "MSE = 1.0928382673064805\n",
      "MSE = 1.0669025947225017\n",
      "MSE = 1.0604604304606011\n",
      "MSE = 1.0563417293135897\n",
      "MSE = 1.0494140267010361\n",
      "MSE = 1.0226794368094958\n",
      "MSE = 1.0073207619699678\n",
      "MSE = 1.0059838655791151\n",
      "MSE = 1.0045713424477642\n",
      "MSE = 0.9735478843588331\n",
      "MSE = 0.9780057463638304\n",
      "MSE = 0.9801025368014139\n",
      "MSE = 0.9812107372017285\n",
      "MSE = 0.9798569203125101\n",
      "MSE = 0.9726392221336185\n",
      "MSE = 0.9694775952586876\n",
      "MSE = 0.9657128273217864\n",
      "MSE = 0.9628038230310536\n",
      "MSE = 0.9622934730133106\n",
      "MSE = 0.9603566500102666\n",
      "MSE = 1.0164299950344504\n",
      "MSE = 0.9596147330972667\n",
      "MSE = 0.9563851508401203\n",
      "MSE = 0.9540341733422911\n",
      "MSE = 0.955057806951064\n",
      "MSE = 0.9543912565376121\n",
      "MSE = 0.9537850407013447\n",
      "MSE = 0.9532468373475539\n",
      "MSE = 0.9529526364215078\n",
      "MSE = 0.952655504300228\n",
      "MSE = 0.9519525023134627\n",
      "MSE = 0.9511759072389684\n",
      "MSE = 0.9462496385608867\n",
      "MSE = 0.9483646624859541\n",
      "MSE = 0.9497727790660935\n",
      "MSE = 1.0312627134236019\n",
      "MSE = 0.9498649626143797\n",
      "MSE = 0.9498891768745309\n",
      "MSE = 0.9506178386204466\n",
      "MSE = 0.9510369024912974\n",
      "MSE = 0.9514570234241765\n",
      "MSE = 0.9528778897442874\n",
      "MSE = 0.9519198106022602\n",
      "MSE = 0.9518102262425522\n",
      "MSE = 0.9515423549713117\n",
      "MSE = 0.9512900456450392\n",
      "MSE = 0.9507502880779902\n",
      "MSE = 0.949923630135193\n",
      "MSE = 0.9487304764868755\n",
      "MSE = 0.949877156271909\n",
      "MSE = 0.9483044304724069\n",
      "MSE = 0.9496680669503796\n",
      "MSE = 0.9484506025018978\n",
      "MSE = 0.9479291530819008\n",
      "MSE = 0.9480211315081672\n",
      "MSE = 0.9482153747723812\n",
      "MSE = 0.9486644364784561\n",
      "MSE = 0.9491179101442322\n",
      "MSE = 0.9492708809404998\n",
      "MSE = 0.9498901516574574\n",
      "MSE = 0.9494278828761126\n",
      "MSE = 0.9491909841079248\n",
      "MSE = 0.9490699925998044\n",
      "MSE = 0.9488505396041222\n",
      "MSE = 0.948524622056765\n",
      "MSE = 0.9481453020398396\n",
      "MSE = 0.9476567353475084\n",
      "MSE = 0.947714080319718\n",
      "MSE = 0.9493430788129819\n",
      "MSE = 0.9479522990886691\n",
      "MSE = 0.9481945797655846\n",
      "MSE = 0.9483186560111082\n",
      "MSE = 0.9484952466349215\n",
      "MSE = 0.9487545568784024\n",
      "MSE = 0.9486996590189284\n",
      "MSE = 0.9491863949465242\n",
      "MSE = 0.9486676017045309\n",
      "MSE = 0.9485215551819396\n",
      "MSE = 0.9481677367573997\n",
      "MSE = 0.9473679576994645\n",
      "MSE = 0.9475955515620192\n",
      "MSE = 0.9473288787823956\n",
      "MSE = 0.947347656063639\n",
      "MSE = 0.9477596085805038\n",
      "MSE = 0.9474713421865029\n",
      "MSE = 0.9477156373351692\n",
      "MSE = 0.9477633297550985\n",
      "MSE = 0.9480362884415486\n",
      "MSE = 0.9477377845282754\n",
      "MSE = 0.9474203954095701\n",
      "MSE = 0.9471909561383182\n",
      "MSE = 0.9472046894162726\n",
      "MSE = 0.9472289324643116\n",
      "MSE = 0.9472178519852255\n",
      "MSE = 1.0375123729784514\n",
      "MSE = 0.9471910458582388\n",
      "MSE = 0.9471151436960469\n",
      "MSE = 0.9470098323072444\n",
      "MSE = 0.9466977191396122\n",
      "MSE = 0.9461887849777629\n",
      "MSE = 0.9447094973474555\n",
      "MSE = 0.9456128769724114\n",
      "MSE = 0.9453385763613473\n",
      "MSE = 0.9453701720698813\n",
      "MSE = 0.9453456605949907\n",
      "MSE = 0.9453455000542715\n",
      "MSE = 0.9452976922813434\n",
      "MSE = 0.9450764620088011\n",
      "MSE = 0.9446389988117828\n",
      "MSE = 0.9425677834144833\n",
      "MSE = 0.9438272137498956\n",
      "MSE = 0.9433737341817802\n",
      "MSE = 0.9418540064158479\n",
      "MSE = 0.9414046615299809\n",
      "MSE = 0.9399230610762819\n",
      "MSE = 0.9408133579660382\n",
      "MSE = 0.9393407044236823\n",
      "MSE = 0.9378309419991389\n",
      "MSE = 0.9385282246241312\n",
      "MSE = 0.9378002161899865\n",
      "MSE = 0.937096995545694\n",
      "MSE = 0.9359975155064735\n",
      "MSE = 0.9337136514850128\n",
      "MSE = 0.9286470951472255\n",
      "MSE = 0.9181257394125688\n",
      "MSE = 0.9268571384821817\n",
      "MSE = 0.923347634922918\n",
      "MSE = 0.9213516946111197\n",
      "MSE = 0.9189461349955145\n",
      "MSE = 0.9149663964961885\n",
      "MSE = 0.917981748233986\n",
      "MSE = 0.9192120202359835\n",
      "MSE = 0.9187947224001979\n",
      "MSE = 0.9141548741274238\n",
      "MSE = 0.9162093622674864\n",
      "MSE = 0.9167327983145301\n",
      "MSE = 0.9177198449360182\n",
      "MSE = 0.9168250643620101\n",
      "MSE = 0.9150001431450638\n",
      "MSE = 0.9102103511846619\n",
      "MSE = 0.9122791212193027\n",
      "MSE = 0.9253922952921758\n",
      "MSE = 0.9124659267751037\n",
      "MSE = 0.910714488337695\n",
      "MSE = 0.912988898959394\n",
      "MSE = 0.9125903829557364\n",
      "MSE = 0.9119859424289365\n",
      "MSE = 0.9114849077487522\n",
      "MSE = 0.9110067286852206\n",
      "MSE = 0.9087765247376074\n",
      "MSE = 0.9159745149476174\n",
      "MSE = 0.9101021384397506\n",
      "MSE = 0.9093949355069372\n",
      "MSE = 0.9093781534360371\n",
      "MSE = 0.9092477606540252\n",
      "MSE = 0.9087963737446324\n",
      "MSE = 0.9077771206211968\n",
      "MSE = 1.1148861356822606\n",
      "MSE = 0.9080488479471992\n",
      "MSE = 0.9069356117677289\n",
      "MSE = 0.9066360618929667\n",
      "MSE = 0.9066376043871398\n",
      "MSE = 0.9067380081592106\n",
      "MSE = 0.9069681738615339\n",
      "MSE = 0.9069223729912091\n",
      "MSE = 0.9073360458818158\n",
      "MSE = 0.9068841824385739\n",
      "MSE = 0.906525689959756\n",
      "MSE = 0.9053938944609651\n",
      "MSE = 0.9037138447187281\n",
      "MSE = 0.9044255885965896\n",
      "MSE = 0.9041892660567283\n",
      "MSE = 0.9105891428501095\n",
      "MSE = 0.9045032602368591\n",
      "MSE = 0.9044641170174634\n",
      "MSE = 0.8983923334456441\n",
      "MSE = 0.9815132959427478\n",
      "MSE = 0.8969024615807152\n",
      "MSE = 0.8977061225211732\n",
      "MSE = 0.8966325693687691\n",
      "MSE = 0.893634421559901\n",
      "MSE = 0.892519409668872\n",
      "MSE = 0.8924811790700299\n",
      "MSE = 0.8923499032229042\n",
      "MSE = 0.8891578583750479\n",
      "MSE = 1.2961019694385452\n",
      "MSE = 0.8890407031392066\n",
      "MSE = 0.8873058427977216\n",
      "MSE = 0.8846063057425098\n",
      "MSE = 0.8813622203164647\n",
      "MSE = 0.8786914414083098\n",
      "MSE = 0.874876407253011\n",
      "MSE = 0.8771202555728487\n",
      "MSE = 0.8743941574739206\n",
      "MSE = 0.8728011610137063\n",
      "MSE = 0.8728594848588217\n",
      "MSE = 0.8702386959326784\n",
      "MSE = 0.8623532269419671\n",
      "MSE = 0.8670661297175021\n",
      "MSE = 0.8652063658626702\n",
      "MSE = 0.8660097462320131\n",
      "MSE = 0.8652104012305785\n",
      "MSE = 0.8651008268417932\n",
      "MSE = 0.8644335793896962\n",
      "MSE = 0.8632735184546176\n",
      "MSE = 0.8615780738638149\n",
      "MSE = 0.946361851771902\n",
      "MSE = 0.8615882122242986\n",
      "MSE = 0.8610421333937603\n",
      "MSE = 0.8612453867072162\n",
      "MSE = 0.8619783638129107\n",
      "MSE = 0.8618462899572572\n",
      "MSE = 0.9257952779067417\n",
      "MSE = 0.8617968247886549\n",
      "MSE = 0.8613158827484185\n",
      "MSE = 0.8602571583245532\n",
      "MSE = 0.8600658017531443\n",
      "MSE = 0.8592957536795475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.8585964470123668\n",
      "MSE = 0.8569704340515758\n",
      "MSE = 0.8529107140973115\n",
      "MSE = 0.8549433884808229\n",
      "MSE = 0.8544046378971571\n",
      "MSE = 0.8527513277514053\n",
      "MSE = 0.8487485247848395\n",
      "MSE = 0.8509424682967276\n",
      "MSE = 0.848856656726996\n",
      "MSE = 0.849396890321369\n",
      "MSE = 0.9424481667120903\n",
      "MSE = 0.850033511897837\n",
      "MSE = 0.8495170202427994\n",
      "MSE = 0.8462082052799454\n",
      "MSE = 0.8458012040307595\n",
      "MSE = 0.8440596209997014\n",
      "MSE = 0.8445041994547856\n",
      "MSE = 0.8439252591303092\n",
      "MSE = 0.8442123151422326\n",
      "MSE = 0.8447481616592483\n",
      "MSE = 0.8452808952540861\n",
      "MSE = 0.8467239340213041\n",
      "MSE = 0.8455176308786582\n",
      "MSE = 0.846349735916283\n",
      "MSE = 0.845949684109099\n",
      "MSE = 0.8455227441242865\n",
      "MSE = 0.8448138505641233\n",
      "MSE = 0.8442061030273547\n",
      "MSE = 0.8431387941374671\n",
      "MSE = 0.8432321205772354\n",
      "MSE = 0.843057941364262\n",
      "MSE = 0.8423814947577712\n",
      "MSE = 0.8424894994014707\n",
      "MSE = 0.8429451140883399\n",
      "MSE = 0.8444615278282847\n",
      "MSE = 0.8430780993864094\n",
      "MSE = 0.8435527798003968\n",
      "MSE = 0.8429775995122187\n",
      "MSE = 0.8432165448026745\n",
      "MSE = 0.843605025344151\n",
      "MSE = 0.8435194801067911\n",
      "MSE = 0.8470907706787875\n",
      "MSE = 0.844301435739408\n",
      "MSE = 0.8511240075083428\n",
      "MSE = 0.8446537882860371\n",
      "MSE = 0.8593526912675691\n",
      "MSE = 0.8447116471467802\n",
      "MSE = 0.8455590997572222\n",
      "MSE = 0.8457407775165274\n",
      "MSE = 0.8458735625430275\n",
      "MSE = 1.022669894478673\n",
      "MSE = 0.8459296647533985\n",
      "MSE = 0.8458855924277894\n",
      "MSE = 0.8458017629870838\n",
      "MSE = 0.8457535033520244\n",
      "MSE = 0.8454773599701662\n",
      "MSE = 0.845142687371181\n",
      "MSE = 0.8452806320690516\n",
      "MSE = 0.8448460621813242\n",
      "MSE = 0.8448604668176078\n",
      "MSE = 0.8450937390286932\n",
      "MSE = 0.8448538571378216\n",
      "MSE = 0.8450753853445258\n",
      "MSE = 0.8454840705939705\n",
      "MSE = 1.1665726879562799\n",
      "MSE = 0.8453851971105164\n",
      "MSE = 0.8458706550279034\n",
      "MSE = 0.8476145297700801\n",
      "MSE = 0.8462436935272046\n",
      "MSE = 0.8462007584621151\n",
      "MSE = 0.8440281283514237\n",
      "MSE = 0.8450465709671143\n",
      "MSE = 0.8446628479331387\n",
      "MSE = 0.8474960800202019\n",
      "MSE = 0.8445462280456603\n",
      "MSE = 0.8441508901858006\n",
      "MSE = 0.8436772347357128\n",
      "MSE = 0.8434587471976268\n",
      "MSE = 0.8433127000493987\n",
      "MSE = 0.8430950848334181\n",
      "MSE = 0.8434393505021022\n",
      "MSE = 0.8431717310017096\n",
      "MSE = 0.8432704661239734\n",
      "MSE = 0.8432932408709426\n",
      "MSE = 0.8432797441085969\n",
      "MSE = 0.8431782265854497\n",
      "MSE = 0.8430536125620659\n",
      "MSE = 0.8426613089632841\n",
      "MSE = 0.8447629679030701\n",
      "MSE = 0.8427367832045017\n",
      "MSE = 0.8427187047214023\n",
      "MSE = 0.8426137711435889\n",
      "MSE = 0.842446795487497\n",
      "MSE = 0.8419329185196758\n",
      "MSE = 0.8416373419661198\n",
      "MSE = 0.8410344868939947\n",
      "MSE = 0.8406048653278715\n",
      "MSE = 0.8396408186332655\n",
      "MSE = 0.83949572616744\n",
      "MSE = 0.8395564941178078\n",
      "MSE = 0.8390402637434248\n",
      "MSE = 0.8380060152169224\n",
      "MSE = 0.8388520435499417\n",
      "MSE = 0.838542950867426\n",
      "MSE = 0.8372916696009454\n",
      "MSE = 0.8370422919870992\n",
      "MSE = 0.8369945513469963\n",
      "MSE = 0.8347454561097802\n",
      "MSE = 0.8360255654535835\n",
      "MSE = 0.8360762546906997\n",
      "MSE = 0.8341840867336476\n",
      "MSE = 0.8334340821370017\n",
      "MSE = 0.8324584500086144\n",
      "MSE = 0.8387768892900759\n",
      "MSE = 0.8325506372098308\n",
      "MSE = 0.8314586204130312\n",
      "MSE = 0.8297511477514462\n",
      "MSE = 0.8313445544700097\n",
      "MSE = 0.8311623660725325\n",
      "MSE = 0.8322318402633483\n",
      "MSE = 0.8309159313276713\n",
      "MSE = 0.8311338355703167\n",
      "MSE = 0.8301164552209415\n",
      "MSE = 0.8297045362863998\n",
      "MSE = 0.8307786401114322\n",
      "MSE = 0.8300799202500457\n",
      "MSE = 0.8304069935994765\n",
      "MSE = 0.8306000506397836\n",
      "MSE = 0.8307915421186458\n",
      "MSE = 0.8300726786044998\n",
      "MSE = 0.8297887952174017\n",
      "MSE = 0.8286587395120484\n",
      "MSE = 0.8285844644830652\n",
      "MSE = 0.828379121352113\n",
      "MSE = 0.8281963406970522\n",
      "MSE = 0.8283833178744384\n",
      "MSE = 0.8273565511880172\n",
      "MSE = 0.8278631808055704\n",
      "MSE = 0.8282252990533674\n",
      "MSE = 0.8279335316873703\n",
      "MSE = 0.8275661925257912\n",
      "MSE = 0.8279924033835055\n",
      "MSE = 0.8280632056046866\n",
      "MSE = 0.8280350671763287\n",
      "MSE = 0.8282656959312825\n",
      "MSE = 0.828091916697661\n",
      "MSE = 0.8280601531661571\n",
      "MSE = 0.8280799236439854\n",
      "MSE = 0.8281674046995404\n",
      "MSE = 0.8284080831935309\n",
      "MSE = 0.8316161447890087\n",
      "MSE = 0.8285408692969874\n",
      "MSE = 0.8284241866898454\n",
      "MSE = 0.8284102075380447\n",
      "MSE = 0.8284083664525934\n",
      "MSE = 0.8284081210168857\n",
      "MSE = 0.828408088245022\n",
      "MSE = 0.8284080838682023\n",
      "MSE = 0.8284080832836297\n",
      "MSE = 0.8284080832055036\n",
      "MSE = 0.8284080831951022\n",
      "MSE = 0.8284080831937035\n",
      "MSE = 0.8284080831935382\n",
      "MSE = 0.8284080831935311\n",
      "MSE = 0.8284080831935311\n",
      "MSE = 0.8284080831935311\n",
      "1.1657385669622484\n",
      "Lambda = 2e-05\n",
      "MSE = 1.4782900493764934\n",
      "MSE = 2.3164146976240554\n",
      "MSE = 1.472808649061798\n",
      "MSE = 1.4725810833691362\n",
      "MSE = 1.4721476195016696\n",
      "MSE = 1.4708537152777137\n",
      "MSE = 1.4676594684848325\n",
      "MSE = 1.4593526499052543\n",
      "MSE = 1.4391754984467606\n",
      "MSE = 1.3945815674761997\n",
      "MSE = 1.3177593303850919\n",
      "MSE = 1.2358120450486303\n",
      "MSE = 1.1979637419669753\n",
      "MSE = 1.1937595667727734\n",
      "MSE = 1.1941181463054793\n",
      "MSE = 1.193958780078428\n",
      "MSE = 1.1912788817974174\n",
      "MSE = 1.1833338174160604\n",
      "MSE = 1.1621652721731472\n",
      "MSE = 1.1247892684734953\n",
      "MSE = 1.085739613686139\n",
      "MSE = 1.0617608156703116\n",
      "MSE = 2.875809030633014\n",
      "MSE = 1.0582850206326586\n",
      "MSE = 1.055947191855623\n",
      "MSE = 1.0499899226687452\n",
      "MSE = 1.0182351697127443\n",
      "MSE = 0.9902531856792026\n",
      "MSE = 0.992822791501619\n",
      "MSE = 0.9934852306987191\n",
      "MSE = 0.9935376145812176\n",
      "MSE = 0.9903338198194681\n",
      "MSE = 0.9809524057703858\n",
      "MSE = 0.96389178746271\n",
      "MSE = 0.9489098720784757\n",
      "MSE = 21.6817816801185\n",
      "MSE = 0.9491129461547829\n",
      "MSE = 0.9473426301824892\n",
      "MSE = 0.9461441163700892\n",
      "MSE = 0.943618423489758\n",
      "MSE = 0.9407466906634516\n",
      "MSE = 1.0520656235865813\n",
      "MSE = 0.9405677530150952\n",
      "MSE = 0.9403421011908836\n",
      "MSE = 0.9385117937585808\n",
      "MSE = 0.9363043397065439\n",
      "MSE = 0.9328631798223173\n",
      "MSE = 0.9318769619544894\n",
      "MSE = 0.9527738736501707\n",
      "MSE = 0.9319347894924839\n",
      "MSE = 0.9321285396249487\n",
      "MSE = 0.9327597514404312\n",
      "MSE = 0.933263976941058\n",
      "MSE = 0.9329249725779608\n",
      "MSE = 0.9354492400376446\n",
      "MSE = 0.9329265537691973\n",
      "MSE = 0.9315259700768972\n",
      "MSE = 0.9302344560104829\n",
      "MSE = 0.9289577327759103\n",
      "MSE = 0.9280984726562461\n",
      "MSE = 0.9267555486920623\n",
      "MSE = 0.927275889244037\n",
      "MSE = 0.938569126905775\n",
      "MSE = 0.9272153717438221\n",
      "MSE = 0.9273162185424043\n",
      "MSE = 0.927049753835809\n",
      "MSE = 0.927220091804676\n",
      "MSE = 0.9270351537152418\n",
      "MSE = 0.9266587228946956\n",
      "MSE = 0.9264399779458451\n",
      "MSE = 0.9258035113571239\n",
      "MSE = 0.9254252807750972\n",
      "MSE = 0.9244909146214539\n",
      "MSE = 0.9247584681920312\n",
      "MSE = 0.9244335748475614\n",
      "MSE = 0.9247354789966952\n",
      "MSE = 0.9248753100367755\n",
      "MSE = 0.9246767443407529\n",
      "MSE = 0.9234643239669995\n",
      "MSE = 0.921423506794583\n",
      "MSE = 0.9129534997282163\n",
      "MSE = 0.9114584382225887\n",
      "MSE = 0.9063013114800518\n",
      "MSE = 1.2011175132934766\n",
      "MSE = 0.9060553674382701\n",
      "MSE = 0.9058722052505217\n",
      "MSE = 0.9071110254426704\n",
      "MSE = 6.674618844781029\n",
      "MSE = 0.9071416889018737\n",
      "MSE = 0.9071262305652087\n",
      "MSE = 0.9191517439739787\n",
      "MSE = 0.9068124495018398\n",
      "MSE = 0.9047056540388567\n",
      "MSE = 0.9011906270746363\n",
      "MSE = 0.8948617790970419\n",
      "MSE = 0.8860230677452013\n",
      "MSE = 0.8830874967083171\n",
      "MSE = 0.9039005549449206\n",
      "MSE = 0.8825033529597335\n",
      "MSE = 0.883053171275894\n",
      "MSE = 0.8782913127611853\n",
      "MSE = 0.8827776340725935\n",
      "MSE = 0.8782135709353999\n",
      "MSE = 0.8789211069543106\n",
      "MSE = 0.8786889750997647\n",
      "MSE = 0.8771239540405321\n",
      "MSE = 0.8761193636433977\n",
      "MSE = 0.8736906235084497\n",
      "MSE = 0.8730320107329619\n",
      "MSE = 0.8726167500240737\n",
      "MSE = 0.872532914000058\n",
      "MSE = 0.8742552063035718\n",
      "MSE = 0.8728731557675233\n",
      "MSE = 0.8730851309849016\n",
      "MSE = 0.8737292300928609\n",
      "MSE = 0.9433695506959319\n",
      "MSE = 0.874369489682964\n",
      "MSE = 0.8747271878540709\n",
      "MSE = 0.8743231188300966\n",
      "MSE = 0.8723097315029201\n",
      "MSE = 0.9010759323942634\n",
      "MSE = 0.871414172793233\n",
      "MSE = 0.8675990651062415\n",
      "MSE = 0.8652649155277005\n",
      "MSE = 0.8622323417103918\n",
      "MSE = 0.8586516676213081\n",
      "MSE = 1.2142992652289557\n",
      "MSE = 0.8584253127730883\n",
      "MSE = 0.853281238742656\n",
      "MSE = 0.8509411075200239\n",
      "MSE = 0.8509952597382123\n",
      "MSE = 0.8520813831213057\n",
      "MSE = 0.852261595839207\n",
      "MSE = 0.8522672167625674\n",
      "MSE = 0.8511101842531286\n",
      "MSE = 0.8983985586966735\n",
      "MSE = 0.8506994461997022\n",
      "MSE = 0.8488352655953998\n",
      "MSE = 0.844598372006157\n",
      "MSE = 0.8421840436895103\n",
      "MSE = 0.8411353434275284\n",
      "MSE = 0.836570805157473\n",
      "MSE = 0.8429431164268796\n",
      "MSE = 0.8377841343178554\n",
      "MSE = 0.8381469518736739\n",
      "MSE = 0.8389266538033439\n",
      "MSE = 0.838734573725869\n",
      "MSE = 0.8384988486128649\n",
      "MSE = 0.8380065771012373\n",
      "MSE = 0.8377464199734832\n",
      "MSE = 0.8377447233542781\n",
      "MSE = 0.8360768377608802\n",
      "MSE = 0.8313760743939269\n",
      "MSE = 0.8269356698014066\n",
      "MSE = 1.3351729582524137\n",
      "MSE = 0.8277516634455029\n",
      "MSE = 0.8262213262239857\n",
      "MSE = 0.8317171976751163\n",
      "MSE = 0.8276200663303168\n",
      "MSE = 0.8285469308339686\n",
      "MSE = 0.8288359834356055\n",
      "MSE = 0.8529946243995769\n",
      "MSE = 0.8289508511273267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.8283960024157585\n",
      "MSE = 0.9172021408359706\n",
      "MSE = 0.8272573240987943\n",
      "MSE = 0.8263063453814898\n",
      "MSE = 0.825043291851139\n",
      "MSE = 0.825257605326523\n",
      "MSE = 0.8347147350787698\n",
      "MSE = 0.8260234414909252\n",
      "MSE = 0.8221730573868835\n",
      "MSE = 0.8212654580055114\n",
      "MSE = 0.8204637650502777\n",
      "MSE = 0.821503884887902\n",
      "MSE = 0.8214571887082184\n",
      "MSE = 0.8213369394955304\n",
      "MSE = 0.8222893039657382\n",
      "MSE = 0.8230427272835671\n",
      "MSE = 0.8232554013849219\n",
      "MSE = 0.839008151974039\n",
      "MSE = 0.8236292945591295\n",
      "MSE = 0.822186445688762\n",
      "MSE = 0.957427333142911\n",
      "MSE = 0.822345914946513\n",
      "MSE = 0.8195916292739747\n",
      "MSE = 0.8192500798832865\n",
      "MSE = 0.8193450037371821\n",
      "MSE = 0.8179092469022592\n",
      "MSE = 0.8180791317308784\n",
      "MSE = 0.817820476921967\n",
      "MSE = 0.8179925103766029\n",
      "MSE = 0.8181847090186192\n",
      "MSE = 0.818481499373656\n",
      "MSE = 0.8187797325752159\n",
      "MSE = 0.8187998219756946\n",
      "MSE = 0.8180680734356542\n",
      "MSE = 8.257795888758272\n",
      "MSE = 0.8179406989120317\n",
      "MSE = 0.8163096519581409\n",
      "MSE = 0.8149502814502254\n",
      "MSE = 0.8144973511034558\n",
      "MSE = 0.8143919656717177\n",
      "MSE = 0.814163334699292\n",
      "MSE = 0.8135893095061847\n",
      "MSE = 0.8346472102588455\n",
      "MSE = 0.8137814338536233\n",
      "MSE = 0.813029989253565\n",
      "MSE = 0.8097120379910326\n",
      "MSE = 0.8111170598250804\n",
      "MSE = 0.8110696994510378\n",
      "MSE = 0.8113409104976379\n",
      "MSE = 0.8113877348849488\n",
      "MSE = 0.8114056033883024\n",
      "MSE = 0.810450554230223\n",
      "MSE = 0.8109587360405247\n",
      "MSE = 0.8107309825368063\n",
      "MSE = 0.8092928917809076\n",
      "MSE = 0.8068773917508947\n",
      "MSE = 0.8051888062064008\n",
      "MSE = 0.8027670459196533\n",
      "MSE = 0.804163154179722\n",
      "MSE = 0.8035280886098666\n",
      "MSE = 0.8037416172883671\n",
      "MSE = 0.8038308886567876\n",
      "MSE = 0.8038048140561422\n",
      "MSE = 0.8032990159087905\n",
      "MSE = 0.8017492970038078\n",
      "MSE = 0.7965422932533802\n",
      "MSE = 0.8002878699566172\n",
      "MSE = 0.7985897545041588\n",
      "MSE = 0.7977820935672437\n",
      "MSE = 0.7975144893165562\n",
      "MSE = 0.797420122150116\n",
      "MSE = 0.7973284926336496\n",
      "MSE = 0.7972311419855166\n",
      "MSE = 0.797720656927869\n",
      "MSE = 0.7972917647375455\n",
      "MSE = 0.7957645081339954\n",
      "MSE = 0.7960543597400027\n",
      "MSE = 0.795933730253516\n",
      "MSE = 0.7949856011337173\n",
      "MSE = 0.793987451726492\n",
      "MSE = 0.7955460314739466\n",
      "MSE = 0.7942955444161417\n",
      "MSE = 0.7924229509923099\n",
      "MSE = 0.7920312653766534\n",
      "MSE = 0.7918775790697711\n",
      "MSE = 0.7916724525459398\n",
      "MSE = 0.7917786154531954\n",
      "MSE = 0.7922870859086419\n",
      "MSE = 0.7926363515885635\n",
      "MSE = 0.7928833615621701\n",
      "MSE = 0.7923580562698057\n",
      "MSE = 0.791166624456645\n",
      "MSE = 0.7910197423406491\n",
      "MSE = 0.7906720343443076\n",
      "MSE = 0.7897560278525416\n",
      "MSE = 0.7901299789282645\n",
      "MSE = 0.7903906454779042\n",
      "MSE = 0.7903894176094249\n",
      "MSE = 0.7906130700576031\n",
      "MSE = 0.789872620554323\n",
      "MSE = 0.787604953280142\n",
      "MSE = 0.7864947177209987\n",
      "MSE = 0.7859973695727176\n",
      "MSE = 0.7875682475419834\n",
      "MSE = 0.7862743133771136\n",
      "MSE = 0.7867012025996883\n",
      "MSE = 0.7777622967916987\n",
      "MSE = 0.7859353857020552\n",
      "MSE = 0.7862215652321821\n",
      "MSE = 0.786267355706375\n",
      "MSE = 0.7861675980517958\n",
      "MSE = 0.7859466741741544\n",
      "MSE = 0.7850800898462935\n",
      "MSE = 0.7854684557605399\n",
      "MSE = 0.7855954381172594\n",
      "MSE = 0.7856806614576721\n",
      "MSE = 0.7853879603031378\n",
      "MSE = 0.7895493237615078\n",
      "MSE = 0.7857002008583702\n",
      "MSE = 0.7857852262187581\n",
      "MSE = 0.7797831208951244\n",
      "MSE = 0.7847523585361966\n",
      "MSE = 0.785264816952341\n",
      "MSE = 0.7854456058637844\n",
      "MSE = 0.7853875341368765\n",
      "MSE = 0.7854497781328539\n",
      "MSE = 0.7842913732815588\n",
      "MSE = 0.7851963273277064\n",
      "MSE = 0.7853754659838328\n",
      "MSE = 0.7854588810827263\n",
      "MSE = 0.7857165596350912\n",
      "MSE = 0.7859933184092873\n",
      "MSE = 0.7860218434059092\n",
      "MSE = 0.7868694008555148\n",
      "MSE = 0.7859978950585454\n",
      "MSE = 0.7859030673483951\n",
      "MSE = 0.785807501806396\n",
      "MSE = 0.7857435612877238\n",
      "MSE = 0.7856047706393654\n",
      "MSE = 0.7852659764409856\n",
      "MSE = 0.7847021145265995\n",
      "MSE = 0.784540459408333\n",
      "MSE = 0.7833600778000352\n",
      "MSE = 0.7836001291503006\n",
      "MSE = 0.7836494855044696\n",
      "MSE = 0.7834655869328881\n",
      "MSE = 0.7825450455479154\n",
      "MSE = 0.7813062094775557\n",
      "MSE = 0.7819874385505008\n",
      "MSE = 0.7816067796967795\n",
      "MSE = 0.7817154497290318\n",
      "MSE = 0.7830578320972841\n",
      "MSE = 0.7820212099532032\n",
      "MSE = 0.7822787802298491\n",
      "MSE = 0.7823100951511069\n",
      "MSE = 0.7821786346461401\n",
      "MSE = 0.7816814473314783\n",
      "MSE = 0.7877626968434625\n",
      "MSE = 0.7814253829097584\n",
      "MSE = 0.7805217229535055\n",
      "MSE = 0.7798375357620421\n",
      "MSE = 0.7795427657226922\n",
      "MSE = 0.7795332101283113\n",
      "MSE = 0.7795424444946523\n",
      "MSE = 0.7797296511075412\n",
      "MSE = 0.7804085395898227\n",
      "MSE = 0.7811565803727091\n",
      "MSE = 0.7814301035139838\n",
      "MSE = 0.7818321040719167\n",
      "MSE = 0.7815417967648722\n",
      "MSE = 0.7818544690406725\n",
      "MSE = 0.782235781390628\n",
      "MSE = 0.7826396290108245\n",
      "MSE = 0.7832079562514467\n",
      "MSE = 0.7833823904754381\n",
      "MSE = 0.7864518848551448\n",
      "MSE = 0.7835184797349584\n",
      "MSE = 0.7828929090583799\n",
      "MSE = 0.782268361916469\n",
      "MSE = 0.7819366330594903\n",
      "MSE = 0.7814306864859586\n",
      "MSE = 0.8521878506576461\n",
      "MSE = 0.7815348270863786\n",
      "MSE = 0.7815099566685516\n",
      "MSE = 0.7818407295773417\n",
      "MSE = 0.7815854745256445\n",
      "MSE = 0.7825269655061317\n",
      "MSE = 0.7827185324082185\n",
      "MSE = 0.7824161028613662\n",
      "MSE = 0.7913901294782714\n",
      "MSE = 0.7825835612411666\n",
      "MSE = 0.7824985484391141\n",
      "MSE = 0.7819887595944035\n",
      "MSE = 0.7818578712486942\n",
      "MSE = 0.7803300983080611\n",
      "MSE = 0.7809045602388779\n",
      "MSE = 0.780126472593853\n",
      "MSE = 0.7790115147641664\n",
      "MSE = 0.7790297781011114\n",
      "MSE = 0.7944968735409732\n",
      "MSE = 0.7791961793852893\n",
      "MSE = 0.7790778876016937\n",
      "MSE = 0.7798507628612188\n",
      "MSE = 0.7798896409205788\n",
      "MSE = 0.7796682625259684\n",
      "MSE = 0.7797902020687387\n",
      "MSE = 0.7797593004707176\n",
      "MSE = 0.7797436245276533\n",
      "MSE = 0.7796786091035385\n",
      "MSE = 0.7795910913943777\n",
      "MSE = 0.7793061114694184\n",
      "MSE = 0.7794278141209857\n",
      "MSE = 0.7793337169941608\n",
      "MSE = 0.778893925926896\n",
      "MSE = 0.7777786492576321\n",
      "MSE = 0.7793478574014386\n",
      "MSE = 0.7777940950420295\n",
      "MSE = 0.7772122000423503\n",
      "MSE = 0.7770545251590633\n",
      "MSE = 0.7769329980679748\n",
      "MSE = 0.8633785867959617\n",
      "MSE = 0.7769677129863332\n",
      "MSE = 0.7769304126633411\n",
      "MSE = 0.7776627448984662\n",
      "MSE = 0.7770461205668907\n",
      "MSE = 0.7757764581409202\n",
      "MSE = 0.7768691537793244\n",
      "MSE = 0.7768507156868584\n",
      "MSE = 0.7750015798774992\n",
      "MSE = 0.7751468501031779\n",
      "MSE = 0.7753005867009073\n",
      "MSE = 0.7751745925206782\n",
      "MSE = 0.8167677706552013\n",
      "MSE = 0.7753471706003204\n",
      "MSE = 0.7751444496885382\n",
      "MSE = 0.774999805923711\n",
      "MSE = 0.7748213626881104\n",
      "MSE = 0.7749077468911537\n",
      "MSE = 0.7749354639686685\n",
      "MSE = 0.7749516941519851\n",
      "MSE = 0.7767557096051854\n",
      "MSE = 0.7750781564041899\n",
      "MSE = 0.7749782200811287\n",
      "MSE = 0.7752783048120292\n",
      "MSE = 0.7758601767462896\n",
      "MSE = 0.7759629671326741\n",
      "MSE = 0.7759295644914743\n",
      "MSE = 0.7757048020330581\n",
      "MSE = 0.7756952583458259\n",
      "MSE = 0.7755326604282821\n",
      "MSE = 0.7752173951789124\n",
      "MSE = 0.7750874581153158\n",
      "MSE = 0.7750289681975421\n",
      "MSE = 0.7750985370947999\n",
      "MSE = 0.7745372764897319\n",
      "MSE = 0.7748278613500942\n",
      "MSE = 0.7748093333338307\n",
      "MSE = 0.7747226540169966\n",
      "MSE = 0.7746792681466079\n",
      "MSE = 0.7747651663025602\n",
      "MSE = 0.7741745428790129\n",
      "MSE = 0.7744523805439739\n",
      "MSE = 0.7746215195736458\n",
      "MSE = 0.7746717265764002\n",
      "MSE = 0.7746814848318472\n",
      "MSE = 0.7746906834231094\n",
      "MSE = 0.7746863235693282\n",
      "MSE = 0.774677802932594\n",
      "MSE = 1.8909601409712997\n",
      "MSE = 0.7745848270606049\n",
      "MSE = 0.7744893016965407\n",
      "MSE = 0.7738819065425313\n",
      "MSE = 0.7738979310683775\n",
      "MSE = 0.77374193570612\n",
      "MSE = 0.774394193115674\n",
      "MSE = 0.7738169299595397\n",
      "MSE = 0.773786356752715\n",
      "MSE = 0.7731890051560367\n",
      "MSE = 0.7734595374630849\n",
      "MSE = 0.7733988197932764\n",
      "MSE = 0.7732722605321551\n",
      "MSE = 0.7732825631481226\n",
      "MSE = 0.7736229355704879\n",
      "MSE = 0.7733289186858426\n",
      "MSE = 0.7732934924263339\n",
      "MSE = 0.7733168188193938\n",
      "MSE = 0.7733951872901088\n",
      "MSE = 0.7734455902702073\n",
      "MSE = 0.7735159624526505\n",
      "MSE = 0.7737594002005594\n",
      "MSE = 0.7724939538107347\n",
      "MSE = 0.7735261032709005\n",
      "MSE = 0.7735303348822705\n",
      "MSE = 0.7732768837342345\n",
      "MSE = 0.7731585449445453\n",
      "MSE = 0.7753687729981377\n",
      "MSE = 0.7732313778112891\n",
      "MSE = 0.7731127681939599\n",
      "MSE = 0.7727697724450826\n",
      "MSE = 0.7717634985227482\n",
      "MSE = 0.7725238061706595\n",
      "MSE = 0.7723319183034583\n",
      "MSE = 0.7723471783683182\n",
      "MSE = 0.7722582406621328\n",
      "MSE = 0.7878227132241282\n",
      "MSE = 0.7723894954679933\n",
      "MSE = 0.7722740231451118\n",
      "MSE = 0.7722603881223727\n",
      "MSE = 0.7722585375661102\n",
      "MSE = 0.7722582818017589\n",
      "MSE = 0.7722582463642197\n",
      "MSE = 0.7722582414524811\n",
      "MSE = 0.7722582407717392\n",
      "MSE = 0.772258240677265\n",
      "MSE = 0.7722582406641944\n",
      "MSE = 0.7722582406624345\n",
      "MSE = 0.7722582406636094\n",
      "MSE = 0.7722582406625751\n",
      "MSE = 0.7722582406624354\n",
      "MSE = 0.7722582406624358\n",
      "MSE = 0.7722582406624354\n",
      "1.1767235833357126\n",
      "Lambda = 1e-05\n",
      "MSE = 1.4787041955550746\n",
      "MSE = 2.312399738719062\n",
      "MSE = 1.4728182363020663\n",
      "MSE = 1.4725869325473437\n",
      "MSE = 1.4721688834789601\n",
      "MSE = 1.4708891830907425\n",
      "MSE = 1.4677569010612532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.4595824051087134\n",
      "MSE = 1.4397500587976888\n",
      "MSE = 1.3958808607770867\n",
      "MSE = 1.3201442179703786\n",
      "MSE = 1.238900997949577\n",
      "MSE = 1.199979607059792\n",
      "MSE = 1.1943816447484816\n",
      "MSE = 1.1943200713684774\n",
      "MSE = 1.1939628952474237\n",
      "MSE = 1.1904788023338346\n",
      "MSE = 1.1816416607218894\n",
      "MSE = 1.1596152235876558\n",
      "MSE = 1.1217657399697962\n",
      "MSE = 1.0814962650911468\n",
      "MSE = 1.0606819246884762\n",
      "MSE = 10.998205444381815\n",
      "MSE = 1.0570501462267918\n",
      "MSE = 1.0552037913433439\n",
      "MSE = 1.0535158122587651\n",
      "MSE = 1.0445794390613892\n",
      "MSE = 1.0242352002403134\n",
      "MSE = 0.9997590593104911\n",
      "MSE = 0.9822145219235967\n",
      "MSE = 0.9710048923998245\n",
      "MSE = 0.966822833445079\n",
      "MSE = 0.9602826893960739\n",
      "MSE = 0.9504815259330631\n",
      "MSE = 4.63473515414855\n",
      "MSE = 0.9480860404693756\n",
      "MSE = 0.9447420140428063\n",
      "MSE = 0.9438585983366932\n",
      "MSE = 0.9412432217640337\n",
      "MSE = 0.939259803445872\n",
      "MSE = 0.9339069739382438\n",
      "MSE = 0.9274463531232949\n",
      "MSE = 0.9262053217040933\n",
      "MSE = 0.925145692871793\n",
      "MSE = 0.9523044579794122\n",
      "MSE = 0.9239233075045008\n",
      "MSE = 0.9225338803367421\n",
      "MSE = 0.9199997476720998\n",
      "MSE = 0.9209437144793496\n",
      "MSE = 0.9199260254980699\n",
      "MSE = 0.9188721287920006\n",
      "MSE = 0.9190912161960814\n",
      "MSE = 0.9177859266788575\n",
      "MSE = 0.9180036762810903\n",
      "MSE = 0.9181561163288636\n",
      "MSE = 0.9177302392303814\n",
      "MSE = 0.9168203825551874\n",
      "MSE = 0.9768656984892771\n",
      "MSE = 0.9174580207135846\n",
      "MSE = 0.916102274532969\n",
      "MSE = 0.9152427018200031\n",
      "MSE = 0.9145799055387157\n",
      "MSE = 0.9140241888924004\n",
      "MSE = 0.9173499340133967\n",
      "MSE = 0.914214191116437\n",
      "MSE = 0.914133978792263\n",
      "MSE = 0.9144875986903658\n",
      "MSE = 0.914727056145185\n",
      "MSE = 0.9147952282653781\n",
      "MSE = 0.9148973956765962\n",
      "MSE = 0.9150526752228918\n",
      "MSE = 0.9152536083314331\n",
      "MSE = 0.9154114011437664\n",
      "MSE = 0.9154795439576285\n",
      "MSE = 0.9143154059783182\n",
      "MSE = 0.9149213270601286\n",
      "MSE = 0.9147755568166429\n",
      "MSE = 0.914119963469706\n",
      "MSE = 0.9141704088276704\n",
      "MSE = 0.9141357155993376\n",
      "MSE = 0.9140595477371193\n",
      "MSE = 0.9137511418412988\n",
      "MSE = 0.9133639369098558\n",
      "MSE = 0.9129420179122667\n",
      "MSE = 0.912003792199898\n",
      "MSE = 0.911790336649218\n",
      "MSE = 0.9115913173222908\n",
      "MSE = 0.910977639398747\n",
      "MSE = 0.9388841702327\n",
      "MSE = 0.9111559460683609\n",
      "MSE = 0.9110231145307224\n",
      "MSE = 0.911266136372551\n",
      "MSE = 0.9112355084654263\n",
      "MSE = 0.91119188559044\n",
      "MSE = 0.9105227196654299\n",
      "MSE = 0.910463805912963\n",
      "MSE = 0.9416193093922859\n",
      "MSE = 0.9106220336964282\n",
      "MSE = 0.9105437457412068\n",
      "MSE = 0.9105192651134597\n",
      "MSE = 0.9105518048745389\n",
      "MSE = 0.9103356628673416\n",
      "MSE = 0.9102291484869697\n",
      "MSE = 0.9099262125131942\n",
      "MSE = 0.9097918573787895\n",
      "MSE = 0.9087230642881922\n",
      "MSE = 0.9179765563611233\n",
      "MSE = 0.9092184679212609\n",
      "MSE = 0.9087314770634023\n",
      "MSE = 0.9087650484136104\n",
      "MSE = 0.9076462735451203\n",
      "MSE = 0.9025163995880784\n",
      "MSE = 0.9021476751000236\n",
      "MSE = 0.9017693930916344\n",
      "MSE = 0.9013562639659682\n",
      "MSE = 0.8996528063815507\n",
      "MSE = 0.9007222366307538\n",
      "MSE = 0.9002985353036201\n",
      "MSE = 0.9000910926055725\n",
      "MSE = 0.8995217319187635\n",
      "MSE = 0.8983526112759852\n",
      "MSE = 0.894975487093236\n",
      "MSE = 0.8900944998624687\n",
      "MSE = 0.8828752000347545\n",
      "MSE = 1.9810221867009727\n",
      "MSE = 0.8820251018780546\n",
      "MSE = 0.9070086241411272\n",
      "MSE = 0.8808705443291497\n",
      "MSE = 0.8760832176414236\n",
      "MSE = 0.8741497547528708\n",
      "MSE = 0.8748317105859049\n",
      "MSE = 0.8748980883480956\n",
      "MSE = 0.8752586494517909\n",
      "MSE = 0.8749912120622956\n",
      "MSE = 0.872778588338394\n",
      "MSE = 1.8032028392978738\n",
      "MSE = 0.8730979031904196\n",
      "MSE = 0.8668732493227586\n",
      "MSE = 0.8591745747400127\n",
      "MSE = 0.8572056938701287\n",
      "MSE = 0.8629144200870404\n",
      "MSE = 0.8563494738867847\n",
      "MSE = 0.8566346984168318\n",
      "MSE = 0.857885056700315\n",
      "MSE = 0.8570272514096288\n",
      "MSE = 0.8571662685501156\n",
      "MSE = 0.8562346121175273\n",
      "MSE = 0.8567430877608445\n",
      "MSE = 0.8543144532622229\n",
      "MSE = 0.8520642356995606\n",
      "MSE = 0.8509598740722264\n",
      "MSE = 0.8442206299167393\n",
      "MSE = 1.1744861846212105\n",
      "MSE = 0.8439040199220577\n",
      "MSE = 0.8417101106189551\n",
      "MSE = 0.8420003369005269\n",
      "MSE = 0.8420753373634613\n",
      "MSE = 0.8420511509342979\n",
      "MSE = 0.8416355372579127\n",
      "MSE = 0.838752204449672\n",
      "MSE = 0.8374636208017596\n",
      "MSE = 0.8366428474837343\n",
      "MSE = 0.833996189245914\n",
      "MSE = 0.8322585861091815\n",
      "MSE = 0.8467578449379212\n",
      "MSE = 0.8313821571401536\n",
      "MSE = 0.8286237091206645\n",
      "MSE = 0.8288677183263077\n",
      "MSE = 0.8280415862707837\n",
      "MSE = 0.8283290260595975\n",
      "MSE = 0.8284621397311608\n",
      "MSE = 0.8272243277139573\n",
      "MSE = 1.047161905338862\n",
      "MSE = 0.8274801011988254\n",
      "MSE = 0.8243023842512949\n",
      "MSE = 0.8219923803897726\n",
      "MSE = 0.8202773041672188\n",
      "MSE = 0.8192212109431726\n",
      "MSE = 0.8173184154326183\n",
      "MSE = 0.8148796635705251\n",
      "MSE = 0.8114163196887253\n",
      "MSE = 0.8115086506410676\n",
      "MSE = 0.8120150535026023\n",
      "MSE = 0.8103269502447563\n",
      "MSE = 0.8111632470331541\n",
      "MSE = 0.8097019662446833\n",
      "MSE = 0.8098379689366172\n",
      "MSE = 0.809336646866922\n",
      "MSE = 0.8069407040118233\n",
      "MSE = 0.8048019554920643\n",
      "MSE = 0.7994388680009371\n",
      "MSE = 0.7963979341118801\n",
      "MSE = 0.8350651150484265\n",
      "MSE = 0.7959128067920048\n",
      "MSE = 0.7934301759997516\n",
      "MSE = 0.7876916534174733\n",
      "MSE = 0.7901165335723658\n",
      "MSE = 0.7914927802053939\n",
      "MSE = 0.7871629302417065\n",
      "MSE = 0.844127507303243\n",
      "MSE = 0.7876378585813179\n",
      "MSE = 0.7889637793425808\n",
      "MSE = 0.7891920413759995\n",
      "MSE = 0.7886686993824695\n",
      "MSE = 0.7867036981886565\n",
      "MSE = 0.7823815658721692\n",
      "MSE = 3.1491609004029857\n",
      "MSE = 0.7824027689996318\n",
      "MSE = 0.7773827704960272\n",
      "MSE = 0.8040278855268037\n",
      "MSE = 0.7774375803266735\n",
      "MSE = 0.7763174111692115\n",
      "MSE = 0.7793075504385037\n",
      "MSE = 0.7735292075281797\n",
      "MSE = 0.7728871072066832\n",
      "MSE = 1.0626632366067077\n",
      "MSE = 0.7730420820047195\n",
      "MSE = 0.7729179442746302\n",
      "MSE = 0.7709929761060451\n",
      "MSE = 0.7706500969288734\n",
      "MSE = 0.7685900743658453\n",
      "MSE = 0.7721507778195998\n",
      "MSE = 0.768472421144823\n",
      "MSE = 0.7671834450033003\n",
      "MSE = 0.7666607785993077\n",
      "MSE = 0.7661544560648053\n",
      "MSE = 0.7652792376581888\n",
      "MSE = 0.7643210703687164\n",
      "MSE = 0.7633738383636511\n",
      "MSE = 0.7692723150608647\n",
      "MSE = 0.7629592963377071\n",
      "MSE = 0.7612557918105965\n",
      "MSE = 0.7598923315562306\n",
      "MSE = 0.7584062486666229\n",
      "MSE = 0.7577692331020223\n",
      "MSE = 0.757762073169808\n",
      "MSE = 0.7574663183365014\n",
      "MSE = 0.7572432737166318\n",
      "MSE = 0.7563946288625429\n",
      "MSE = 0.7566953680465389\n",
      "MSE = 0.7558646855169923\n",
      "MSE = 0.7550576095758316\n",
      "MSE = 0.7541154417487926\n",
      "MSE = 0.7533678009634925\n",
      "MSE = 0.7557895201364788\n",
      "MSE = 0.7515164510958182\n",
      "MSE = 0.7508921801039871\n",
      "MSE = 0.7514634333310947\n",
      "MSE = 0.7514125148727434\n",
      "MSE = 0.7529657113324829\n",
      "MSE = 0.7515709290934453\n",
      "MSE = 0.751388380834784\n",
      "MSE = 0.7508975959271891\n",
      "MSE = 0.749820624362076\n",
      "MSE = 0.7491834070087352\n",
      "MSE = 0.7487754140792108\n",
      "MSE = 0.7583781170531266\n",
      "MSE = 0.7482308254024365\n",
      "MSE = 0.7480765888211764\n",
      "MSE = 0.7461922966250099\n",
      "MSE = 0.7468221524578159\n",
      "MSE = 0.7478872526069592\n",
      "MSE = 0.7480217336341003\n",
      "MSE = 0.7478566227450841\n",
      "MSE = 0.7478588615454498\n",
      "MSE = 0.747686030995676\n",
      "MSE = 0.7459521533293989\n",
      "MSE = 0.7458215314317079\n",
      "MSE = 0.7454097592411169\n",
      "MSE = 0.7450614115388872\n",
      "MSE = 0.7449036273337831\n",
      "MSE = 0.7448058964630745\n",
      "MSE = 0.7447934951105933\n",
      "MSE = 0.7441920597511626\n",
      "MSE = 0.7435000587030555\n",
      "MSE = 0.7426146839227292\n",
      "MSE = 0.742438726464812\n",
      "MSE = 0.741257937717797\n",
      "MSE = 0.7403743575521708\n",
      "MSE = 0.740389575833659\n",
      "MSE = 0.7384103136302351\n",
      "MSE = 0.7392667437869587\n",
      "MSE = 0.7397099832734035\n",
      "MSE = 0.7402947337897792\n",
      "MSE = 0.7396197335645771\n",
      "MSE = 0.7394579668128827\n",
      "MSE = 1.1769537953600786\n",
      "MSE = 0.739291053338888\n",
      "MSE = 0.7383857938616335\n",
      "MSE = 0.7399415163073155\n",
      "MSE = 0.738447578933198\n",
      "MSE = 0.7376953766017511\n",
      "MSE = 3.0717423819552105\n",
      "MSE = 0.7369457029236833\n",
      "MSE = 0.7381700602970114\n",
      "MSE = 0.7368508068614197\n",
      "MSE = 0.7355995885228809\n",
      "MSE = 0.7353368252832335\n",
      "MSE = 0.7354380927756412\n",
      "MSE = 0.7355418872660718\n",
      "MSE = 0.7799835786176538\n",
      "MSE = 0.7356811817499092\n",
      "MSE = 0.7357112086826856\n",
      "MSE = 0.7370490436881518\n",
      "MSE = 0.7360041416564621\n",
      "MSE = 0.7357576888645845\n",
      "MSE = 0.7352246193941658\n",
      "MSE = 0.7344796797378016\n",
      "MSE = 0.734419248534667\n",
      "MSE = 0.7344430924801575\n",
      "MSE = 0.7344073459794348\n",
      "MSE = 0.733091014560989\n",
      "MSE = 0.7334106187052829\n",
      "MSE = 0.733403150259077\n",
      "MSE = 0.7323718319089989\n",
      "MSE = 0.7325347037643974\n",
      "MSE = 0.7320532319797777\n",
      "MSE = 0.7317906341919225\n",
      "MSE = 0.7312558593964217\n",
      "MSE = 0.7274356981551098\n",
      "MSE = 0.7303047588177699\n",
      "MSE = 0.7304135328009483\n",
      "MSE = 0.7308623574856931\n",
      "MSE = 0.7310691820655236\n",
      "MSE = 0.7311593916631791\n",
      "MSE = 0.730873948944139\n",
      "MSE = 0.7307795401161916\n",
      "MSE = 0.7311472402826344\n",
      "MSE = 0.7306295223680112\n",
      "MSE = 0.7302979240554007\n",
      "MSE = 0.7299063565866009\n",
      "MSE = 0.7297018214287873\n",
      "MSE = 0.7294551008068405\n",
      "MSE = 0.7286428433978667\n",
      "MSE = 0.728201509568179\n",
      "MSE = 0.728349717199846\n",
      "MSE = 0.7284196195846514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.7365990466393793\n",
      "MSE = 0.7285722906808718\n",
      "MSE = 0.7287755322378261\n",
      "MSE = 0.7283643840437146\n",
      "MSE = 0.7280864000680667\n",
      "MSE = 0.7282006146160144\n",
      "MSE = 0.7280554916028112\n",
      "MSE = 0.7278948172034228\n",
      "MSE = 0.7279161372098909\n",
      "MSE = 0.7283592734165195\n",
      "MSE = 0.7287388073895867\n",
      "MSE = 0.7956188506385735\n",
      "MSE = 0.7287617793193003\n",
      "MSE = 0.7290820105964395\n",
      "MSE = 0.7291529465902408\n",
      "MSE = 0.7290382723648698\n",
      "MSE = 0.728666515167056\n",
      "MSE = 0.7286351922814971\n",
      "MSE = 0.7284692761407716\n",
      "MSE = 0.7279506956873092\n",
      "MSE = 0.7273027354909901\n",
      "MSE = 0.7266995863371288\n",
      "MSE = 0.7263446932713524\n",
      "MSE = 0.7262414295582665\n",
      "MSE = 0.7261495013544863\n",
      "MSE = 0.7258339263108953\n",
      "MSE = 0.7256499924540827\n",
      "MSE = 0.7252610623393204\n",
      "MSE = 0.726585713991451\n",
      "MSE = 0.7252988989386829\n",
      "MSE = 0.7240553717687174\n",
      "MSE = 0.7234254164425031\n",
      "MSE = 0.7230992545853282\n",
      "MSE = 0.723140441847862\n",
      "MSE = 0.7230407579231628\n",
      "MSE = 0.7290950939549644\n",
      "MSE = 0.7231899306204719\n",
      "MSE = 0.7232441594345794\n",
      "MSE = 0.7232905472334579\n",
      "MSE = 0.7231928716769518\n",
      "MSE = 0.7238370432846025\n",
      "MSE = 0.7233384251139435\n",
      "MSE = 0.7228753648056172\n",
      "MSE = 0.7223653603816298\n",
      "MSE = 0.721903844824766\n",
      "MSE = 0.7230652957432298\n",
      "MSE = 0.7219329140998195\n",
      "MSE = 0.7218096649653801\n",
      "MSE = 0.7217905206759635\n",
      "MSE = 0.7540069541925919\n",
      "MSE = 0.7216926960103258\n",
      "MSE = 0.7216600368290124\n",
      "MSE = 0.7215026071078364\n",
      "MSE = 0.7214243782593112\n",
      "MSE = 0.7213730895183588\n",
      "MSE = 0.7201144831779943\n",
      "MSE = 0.7207719937467763\n",
      "MSE = 0.7208534964466629\n",
      "MSE = 0.720741887151588\n",
      "MSE = 0.7205688106918297\n",
      "MSE = 0.7271176073045494\n",
      "MSE = 0.720803206443777\n",
      "MSE = 0.7203702981742345\n",
      "MSE = 0.7201800568857528\n",
      "MSE = 0.7196186600622262\n",
      "MSE = 0.7199069030491784\n",
      "MSE = 0.7200125307261475\n",
      "MSE = 0.7200542091081225\n",
      "MSE = 0.739085726339102\n",
      "MSE = 0.7201233172632686\n",
      "MSE = 0.7197594283867096\n",
      "MSE = 0.7199901929296841\n",
      "MSE = 0.7199604418151007\n",
      "MSE = 0.7198009501426997\n",
      "MSE = 0.7196699774398185\n",
      "MSE = 0.7196182642513771\n",
      "MSE = 0.7196115769781564\n",
      "MSE = 0.7195687231537102\n",
      "MSE = 0.7848263343688805\n",
      "MSE = 0.7196786760645192\n",
      "MSE = 0.7195883233610666\n",
      "MSE = 0.7195730688098632\n",
      "MSE = 0.7195831259373204\n",
      "MSE = 0.7195729902068474\n",
      "MSE = 0.7196007124640019\n",
      "MSE = 0.719365362543357\n",
      "MSE = 0.7194607240268618\n",
      "MSE = 0.7194320169656357\n",
      "MSE = 0.7192217608688104\n",
      "MSE = 0.7191419255560884\n",
      "MSE = 0.7191211829227532\n",
      "MSE = 0.7188448663902604\n",
      "MSE = 0.7190444972203869\n",
      "MSE = 0.7190356562539102\n",
      "MSE = 0.719024797198503\n",
      "MSE = 0.718985977436775\n",
      "MSE = 0.7189025060059469\n",
      "MSE = 0.718758281675189\n",
      "MSE = 0.7185050931514728\n",
      "MSE = 0.7180974725361005\n",
      "MSE = 0.7185289786385115\n",
      "MSE = 0.7180719071822174\n",
      "MSE = 0.7179380254479398\n",
      "MSE = 0.7180193789371995\n",
      "MSE = 0.7179314729643231\n",
      "MSE = 0.7179771267409261\n",
      "MSE = 0.7180130353644487\n",
      "MSE = 0.7180687611539139\n",
      "MSE = 0.718118213674942\n",
      "MSE = 0.7181942064542358\n",
      "MSE = 0.7182015824357882\n",
      "MSE = 0.7183106774213948\n",
      "MSE = 0.7228315030536772\n",
      "MSE = 0.7180977175726779\n",
      "MSE = 0.7180411174801298\n",
      "MSE = 0.7179173475879764\n",
      "MSE = 0.717855882437109\n",
      "MSE = 0.7176958711560023\n",
      "MSE = 0.7178220843434505\n",
      "MSE = 0.7177666424410788\n",
      "MSE = 0.7177610933890881\n",
      "MSE = 0.7177734152015768\n",
      "MSE = 0.7177982114527401\n",
      "MSE = 0.7178184867979632\n",
      "MSE = 0.7178281686857235\n",
      "MSE = 0.7178065678921224\n",
      "MSE = 0.7170614153969047\n",
      "MSE = 0.7174633087039131\n",
      "MSE = 0.7176600583225243\n",
      "MSE = 0.7175281825426443\n",
      "MSE = 0.7463776221612588\n",
      "MSE = 0.7175070381601002\n",
      "MSE = 0.7174260781547988\n",
      "MSE = 0.7172386921642697\n",
      "MSE = 0.7171053281909301\n",
      "MSE = 0.7168380114822226\n",
      "MSE = 0.7169056480323003\n",
      "MSE = 0.7170175669278249\n",
      "MSE = 0.7170298046571745\n",
      "MSE = 0.7169232905932907\n",
      "MSE = 0.7233344689589982\n",
      "MSE = 0.7170184455214169\n",
      "MSE = 0.7168521845208791\n",
      "MSE = 0.7183036133249795\n",
      "MSE = 0.7168729174871016\n",
      "MSE = 0.7167127453002392\n",
      "MSE = 0.7166791219799167\n",
      "MSE = 0.7166721367132056\n",
      "MSE = 0.7165222422202322\n",
      "MSE = 0.7165813437559126\n",
      "MSE = 0.7166593256148731\n",
      "MSE = 0.7168105379291837\n",
      "MSE = 0.7168095805913581\n",
      "MSE = 0.716645548010036\n",
      "MSE = 0.7167958014398627\n",
      "MSE = 0.7161294789282248\n",
      "MSE = 0.7161314041500799\n",
      "MSE = 0.7159866536853458\n",
      "MSE = 0.715935701437462\n",
      "MSE = 0.7159348372284423\n",
      "MSE = 0.7159233218989194\n",
      "MSE = 0.7163468821558244\n",
      "MSE = 0.7159135219832439\n",
      "MSE = 0.7158346647579029\n",
      "MSE = 0.7157833510037772\n",
      "MSE = 0.715864433553539\n",
      "MSE = 0.7158157489608107\n",
      "MSE = 0.7147864683800688\n",
      "MSE = 0.715612290080751\n",
      "MSE = 0.7167812292275282\n",
      "MSE = 0.7156140784869117\n",
      "MSE = 0.7156071647109744\n",
      "MSE = 0.7596537381015961\n",
      "MSE = 0.7156431805711115\n",
      "MSE = 0.7156798207362259\n",
      "MSE = 0.7156990181181754\n",
      "MSE = 0.7159354369359016\n",
      "MSE = 0.7163270678558437\n",
      "MSE = 0.7159568426259595\n",
      "MSE = 0.7158925696294786\n",
      "MSE = 0.715758716128972\n",
      "MSE = 0.7154917455542076\n",
      "MSE = 0.715976089355032\n",
      "MSE = 0.7156308566888256\n",
      "MSE = 0.7176234562413968\n",
      "MSE = 0.7156185778086315\n",
      "MSE = 0.7155176309254648\n",
      "MSE = 0.7152520961839801\n",
      "MSE = 0.715417826012356\n",
      "MSE = 0.715391345775799\n",
      "MSE = 0.7153146351729335\n",
      "MSE = 0.7150640530734328\n",
      "MSE = 0.715004987696333\n",
      "MSE = 0.7150355409382442\n",
      "MSE = 0.7149015519800233\n",
      "MSE = 0.7148123042229224\n",
      "MSE = 0.7144930551458878\n",
      "MSE = 0.7144032855136337\n",
      "MSE = 0.7141688484498523\n",
      "MSE = 0.7147758523719923\n",
      "MSE = 0.7141742958767481\n",
      "MSE = 0.7141102928723518\n",
      "MSE = 0.714032639556254\n",
      "MSE = 0.7141918637431554\n",
      "MSE = 0.7140762041704136\n",
      "MSE = 0.7141214898492514\n",
      "MSE = 0.714130024815616\n",
      "MSE = 0.7140482368406318\n",
      "MSE = 0.7139735535831355\n",
      "MSE = 0.7138996033241106\n",
      "MSE = 0.7138158868245469\n",
      "MSE = 0.7138318796264815\n",
      "MSE = 0.7174639392483163\n",
      "MSE = 0.7138247318847913\n",
      "MSE = 0.7139352995989354\n",
      "MSE = 0.7137549807669552\n",
      "MSE = 0.7138763272520169\n",
      "MSE = 0.7139264284584552\n",
      "MSE = 0.714013842995137\n",
      "MSE = 0.7140052919261365\n",
      "MSE = 0.7139401259694214\n",
      "MSE = 0.713880742625228\n",
      "MSE = 0.7137159733007468\n",
      "MSE = 0.7138078895937643\n",
      "MSE = 0.7137631920367821\n",
      "MSE = 0.7137455540492951\n",
      "MSE = 0.713975180362774\n",
      "MSE = 0.7137674776074544\n",
      "MSE = 0.7137477618474503\n",
      "MSE = 0.713767891098455\n",
      "MSE = 0.7137728882649085\n",
      "MSE = 0.7138103763744761\n",
      "MSE = 0.7137696729123925\n",
      "MSE = 0.7137590836649969\n",
      "MSE = 0.7137521276715747\n",
      "MSE = 0.7137625504826315\n",
      "MSE = 0.7134962864906046\n",
      "MSE = 0.713694574236704\n",
      "MSE = 0.7137646348322619\n",
      "MSE = 0.7136503975356023\n",
      "MSE = 0.7137792943256213\n",
      "MSE = 0.7136953788428319\n",
      "MSE = 0.7137160481141315\n",
      "MSE = 0.7136828749979738\n",
      "MSE = 0.7136738108428917\n",
      "MSE = 0.7136632030180321\n",
      "MSE = 0.7136482302943339\n",
      "MSE = 0.7136379425459902\n",
      "MSE = 0.7136247422441985\n",
      "MSE = 0.7135695298123875\n",
      "MSE = 0.7138156129088515\n",
      "MSE = 0.7135863776409704\n",
      "MSE = 0.7134691930410226\n",
      "MSE = 0.713485380371731\n",
      "MSE = 0.7137274141633272\n",
      "MSE = 0.713526641522932\n",
      "MSE = 0.7134822851797091\n",
      "MSE = 0.762266308945983\n",
      "MSE = 0.713478653929522\n",
      "MSE = 0.7134132309130592\n",
      "MSE = 0.718832192922232\n",
      "MSE = 0.7134256879339458\n",
      "MSE = 0.7133744250995928\n",
      "MSE = 0.7132366433943825\n",
      "MSE = 0.7132173276976512\n",
      "MSE = 0.7132035793195233\n",
      "MSE = 0.7131883038072623\n",
      "MSE = 0.7130723818983635\n",
      "MSE = 0.7130026721826198\n",
      "MSE = 0.7128965876109619\n",
      "MSE = 0.7129564628936602\n",
      "MSE = 0.7128988818898254\n",
      "MSE = 0.7128950976585402\n",
      "MSE = 0.727747137395509\n",
      "MSE = 0.7129187192796134\n",
      "MSE = 0.7129215934522485\n",
      "MSE = 0.7129191947549353\n",
      "MSE = 0.7128868579632708\n",
      "MSE = 0.7128228925675592\n",
      "MSE = 0.7130288196095664\n",
      "MSE = 0.7128246373302624\n",
      "MSE = 0.7127294397114533\n",
      "MSE = 0.7124509195157075\n",
      "MSE = 0.7126053907583675\n",
      "MSE = 0.7125043915215653\n",
      "MSE = 0.7126127545010733\n",
      "MSE = 0.7124842059806619\n",
      "MSE = 0.7123577039300771\n",
      "MSE = 0.7123479003542161\n",
      "MSE = 0.7124436102531242\n",
      "MSE = 0.7123682251020198\n",
      "MSE = 0.7123670078848682\n",
      "MSE = 0.712223489483478\n",
      "MSE = 0.7122374994150856\n",
      "MSE = 0.7122392726398435\n",
      "MSE = 0.7122775700496143\n",
      "MSE = 0.7122362279411074\n",
      "MSE = 0.712232822918752\n",
      "MSE = 0.7122530778413914\n",
      "MSE = 0.7122809271544276\n",
      "MSE = 0.7123025279649132\n",
      "MSE = 0.7123486263668569\n",
      "MSE = 0.7123502888073486\n",
      "MSE = 0.7123617002765562\n",
      "MSE = 0.7123859005713495\n",
      "MSE = 0.7124126758135845\n",
      "MSE = 0.7124388089410121\n",
      "MSE = 0.712414066438319\n",
      "MSE = 0.7124223484490836\n",
      "MSE = 0.712434967426457\n",
      "MSE = 0.7124282112570819\n",
      "MSE = 0.7124026809633396\n",
      "MSE = 0.7124024844075185\n",
      "MSE = 0.7133980896386829\n",
      "MSE = 0.7124108349049125\n",
      "MSE = 0.712423617290739\n",
      "MSE = 0.7124381571005617\n",
      "MSE = 0.7124482170453917\n",
      "MSE = 0.712456078563259\n",
      "MSE = 0.712471779350322\n",
      "MSE = 0.7124873925695805\n",
      "MSE = 0.7126580355910277\n",
      "MSE = 0.7125029072639367\n",
      "MSE = 0.712516744725731\n",
      "MSE = 0.712505297002454\n",
      "MSE = 0.7124917849233284\n",
      "MSE = 0.7124867000174536\n",
      "MSE = 0.7138820839742525\n",
      "MSE = 0.712489005727392\n",
      "MSE = 0.712481062234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.7125437363547896\n",
      "MSE = 0.7125002551050066\n",
      "MSE = 0.7124973546313075\n",
      "MSE = 0.712510934540091\n",
      "MSE = 0.7125361272593422\n",
      "MSE = 0.7125384734380659\n",
      "MSE = 0.7181373651051655\n",
      "MSE = 0.7125736149313169\n",
      "MSE = 0.7125871017479672\n",
      "MSE = 0.7126347340217359\n",
      "MSE = 0.7125951720126529\n",
      "MSE = 0.7125874253326114\n",
      "MSE = 0.7125809026435281\n",
      "MSE = 0.7125624554872824\n",
      "MSE = 0.712663025823218\n",
      "MSE = 0.7125669583629339\n",
      "MSE = 0.7125604413755442\n",
      "MSE = 0.7125647757106316\n",
      "MSE = 0.7125813794626209\n",
      "MSE = 0.712605835404076\n",
      "MSE = 0.7125835755229981\n",
      "MSE = 0.7125728882555419\n",
      "MSE = 0.7125639761276387\n",
      "MSE = 0.7125359919739824\n",
      "MSE = 0.7125798485228243\n",
      "MSE = 0.7125112086969089\n",
      "MSE = 0.7124774057296223\n",
      "MSE = 0.7124197155425187\n",
      "MSE = 0.7124304075624152\n",
      "MSE = 0.7124111220763961\n",
      "MSE = 0.7123747494617391\n",
      "MSE = 0.7123751494868648\n",
      "MSE = 0.7123784657322524\n",
      "MSE = 0.7123740957232341\n",
      "MSE = 0.7128203100912216\n",
      "MSE = 0.7123684091461245\n",
      "MSE = 0.7123586132364019\n",
      "MSE = 0.712316629808848\n",
      "MSE = 0.7122038008646362\n",
      "MSE = 0.7134938773240527\n",
      "MSE = 0.7122164713863758\n",
      "MSE = 0.712181046463066\n",
      "MSE = 0.7121619600675104\n",
      "MSE = 0.7121340842971912\n",
      "MSE = 0.7121524444430597\n",
      "MSE = 0.7121536371005835\n",
      "MSE = 0.7121586665738553\n",
      "MSE = 0.7121615110020486\n",
      "MSE = 0.7121485397335692\n",
      "MSE = 0.7121179132175474\n",
      "MSE = 0.7120999183723236\n",
      "MSE = 0.7121064475943484\n",
      "MSE = 0.7120725800703187\n",
      "MSE = 0.7120607839442251\n",
      "MSE = 0.7120549680523603\n",
      "MSE = 0.7120569087875142\n",
      "MSE = 0.7120578351223599\n",
      "MSE = 0.7121202231573655\n",
      "MSE = 0.7120768334531865\n",
      "MSE = 0.7120945655322753\n",
      "MSE = 0.7120982087368718\n",
      "MSE = 0.7120996746047862\n",
      "MSE = 0.7121050290323057\n",
      "MSE = 0.7121376588561924\n",
      "MSE = 0.7121009768553564\n",
      "MSE = 0.7121010415751534\n",
      "MSE = 0.7120729879208053\n",
      "MSE = 0.7120412478142667\n",
      "MSE = 0.7120323777898003\n",
      "MSE = 0.7120161866258687\n",
      "MSE = 0.7118229856649491\n",
      "MSE = 0.7118114784821494\n",
      "MSE = 0.7117986424881333\n",
      "MSE = 0.7118852553708378\n",
      "MSE = 0.7118903128277638\n",
      "MSE = 0.7118635988188963\n",
      "MSE = 0.7118150363788385\n",
      "MSE = 0.7117710453358433\n",
      "MSE = 0.711712998218276\n",
      "MSE = 0.7116947371415042\n",
      "MSE = 0.7116888779669592\n",
      "MSE = 0.7116795275812127\n",
      "MSE = 0.71168209768813\n",
      "MSE = 0.7116764633845516\n",
      "MSE = 0.7127972378726556\n",
      "MSE = 0.7116791716095702\n",
      "MSE = 0.7116742424969356\n",
      "MSE = 0.7116716656010493\n",
      "MSE = 0.7116813751134721\n",
      "MSE = 0.7117067015060923\n",
      "MSE = 0.7116949816555954\n",
      "MSE = 0.7118698761120364\n",
      "MSE = 0.7116855381913382\n",
      "MSE = 0.711634018893433\n",
      "MSE = 0.7130890356277926\n",
      "MSE = 0.7115379391525376\n",
      "MSE = 0.7114919182316188\n",
      "MSE = 0.7115086609118456\n",
      "MSE = 0.7114463320702692\n",
      "MSE = 0.7117060811303747\n",
      "MSE = 0.7114412488722416\n",
      "MSE = 0.7113945256853558\n",
      "MSE = 0.7113943752194375\n",
      "MSE = 0.7114160202838872\n",
      "MSE = 0.7114223039972173\n",
      "MSE = 0.7114870432158927\n",
      "MSE = 0.7119069864130224\n",
      "MSE = 0.7114851406129556\n",
      "MSE = 0.7114008445420806\n",
      "MSE = 0.7113244484077706\n",
      "MSE = 0.7112660399010938\n",
      "MSE = 0.7112560314872012\n",
      "MSE = 0.7159012493599456\n",
      "MSE = 0.7112542348212255\n",
      "MSE = 0.7112422275951155\n",
      "MSE = 0.7113611078173401\n",
      "MSE = 0.711288481795058\n",
      "MSE = 0.7112943804722903\n",
      "MSE = 0.7111503216411772\n",
      "MSE = 0.7112060624355201\n",
      "MSE = 0.7399004315582515\n",
      "MSE = 0.7112065136544821\n",
      "MSE = 0.7112321003919686\n",
      "MSE = 0.711229760721109\n",
      "MSE = 0.7112238067872974\n",
      "MSE = 0.7111985435940182\n",
      "MSE = 0.7111494842109259\n",
      "MSE = 0.7112032593050809\n",
      "MSE = 0.7111466218460404\n",
      "MSE = 0.7111279195517493\n",
      "MSE = 0.711277440433285\n",
      "MSE = 0.7111390036555558\n",
      "MSE = 0.7111416306036561\n",
      "MSE = 0.7111647793199203\n",
      "MSE = 0.7111964948689565\n",
      "MSE = 0.7118508981035289\n",
      "MSE = 0.7111959878055076\n",
      "MSE = 0.7112460796239684\n",
      "MSE = 0.7113899652180974\n",
      "MSE = 0.7112435245523155\n",
      "MSE = 0.7112597738391677\n",
      "MSE = 0.7112608749770066\n",
      "MSE = 0.7112601196710274\n",
      "MSE = 0.7112466120712326\n",
      "MSE = 0.711235785875522\n",
      "MSE = 0.7112293358521617\n",
      "MSE = 0.7112265363063857\n",
      "MSE = 0.7111725178654635\n",
      "MSE = 0.7112028475392421\n",
      "MSE = 0.7156090386370071\n",
      "MSE = 0.7112023332025948\n",
      "MSE = 0.7112000750019007\n",
      "MSE = 0.7112012239496858\n",
      "MSE = 0.711204071379708\n",
      "MSE = 0.7112066848740768\n",
      "MSE = 0.7112114693010811\n",
      "MSE = 0.7214206647105358\n",
      "MSE = 0.7112118196384795\n",
      "MSE = 0.7112127388933938\n",
      "MSE = 0.7112060003601649\n",
      "MSE = 0.7111909279354954\n",
      "MSE = 0.7111667372750407\n",
      "MSE = 0.7111369993506087\n",
      "MSE = 0.7110473820476096\n",
      "MSE = 0.7110492111286779\n",
      "MSE = 0.7110397815394658\n",
      "MSE = 0.7110456107166887\n",
      "MSE = 0.7110507733869058\n",
      "MSE = 0.7110513129455024\n",
      "MSE = 0.7110495544589815\n",
      "MSE = 0.7110454501470471\n",
      "MSE = 0.7110466673163801\n",
      "MSE = 0.7110367492637466\n",
      "MSE = 0.7110154380919117\n",
      "MSE = 0.7109396075373705\n",
      "MSE = 0.7109194621620583\n",
      "MSE = 0.7108988339616379\n",
      "MSE = 0.7108882408749906\n",
      "MSE = 0.7109560035252083\n",
      "MSE = 0.7108857291662374\n",
      "MSE = 0.7108646586754976\n",
      "MSE = 0.7120262326085631\n",
      "MSE = 0.7108502322143934\n",
      "MSE = 0.7108322960853656\n",
      "MSE = 0.7108107287060279\n",
      "MSE = 0.710763884288083\n",
      "MSE = 0.7107659107995948\n",
      "MSE = 0.7107645617152968\n",
      "MSE = 0.7108050059624014\n",
      "MSE = 0.7107854703764379\n",
      "MSE = 0.7107732788728324\n",
      "MSE = 0.7107513936830034\n",
      "MSE = 0.7107153069597866\n",
      "MSE = 0.7107306972483038\n",
      "MSE = 0.7107331668918571\n",
      "MSE = 0.7107425281472375\n",
      "MSE = 0.7108030495789028\n",
      "MSE = 0.7107506451515531\n",
      "MSE = 0.7107619517530941\n",
      "MSE = 0.7107689434810867\n",
      "MSE = 0.7107846165663335\n",
      "MSE = 0.7107899683923118\n",
      "MSE = 0.7107892177489344\n",
      "MSE = 0.710785923853095\n",
      "MSE = 0.7107780525049245\n",
      "MSE = 0.710764222339957\n",
      "MSE = 0.7107691340799053\n",
      "MSE = 0.7107641360196075\n",
      "MSE = 0.7107641157280021\n",
      "MSE = 0.7108510985466929\n",
      "MSE = 0.7107742686440419\n",
      "MSE = 0.7107759229912316\n",
      "MSE = 0.7107755902442318\n",
      "MSE = 0.7107712939505472\n",
      "MSE = 0.7107587895280992\n",
      "MSE = 0.710741384229518\n",
      "MSE = 0.7107781345492243\n",
      "MSE = 0.7107369905714699\n",
      "MSE = 0.7107195946728068\n",
      "MSE = 0.7111052355775367\n",
      "MSE = 0.7107165800810427\n",
      "MSE = 0.7107085083307839\n",
      "MSE = 0.7108718664454795\n",
      "MSE = 0.7107082644312085\n",
      "MSE = 0.7106302851150363\n",
      "MSE = 0.7106891874605057\n",
      "MSE = 0.7106908053189593\n",
      "MSE = 0.710688146550733\n",
      "MSE = 0.7107057249481825\n",
      "MSE = 0.7107039984059209\n",
      "MSE = 0.7106911521584647\n",
      "MSE = 0.7108112531718046\n",
      "MSE = 0.7106941150202796\n",
      "MSE = 0.7106873025681452\n",
      "MSE = 0.7152602295589029\n",
      "MSE = 0.7106924726331342\n",
      "MSE = 0.7106869013738042\n",
      "MSE = 0.7106881190177818\n",
      "MSE = 0.710614533211211\n",
      "MSE = 0.7106744803342725\n",
      "MSE = 0.7106716208952429\n",
      "MSE = 0.7106621832845577\n",
      "MSE = 0.7106654239527447\n",
      "MSE = 0.7106616748304352\n",
      "MSE = 0.7106340282120297\n",
      "MSE = 0.7106020974947953\n",
      "MSE = 0.710846256539733\n",
      "MSE = 0.7106065580202238\n",
      "MSE = 0.710581638598064\n",
      "MSE = 0.7106915054344921\n",
      "MSE = 0.710587420478578\n",
      "MSE = 0.7105775527581996\n",
      "MSE = 0.7105445269655102\n",
      "MSE = 0.7105340142780259\n",
      "MSE = 0.7104047338219893\n",
      "MSE = 0.7104458534356642\n",
      "MSE = 0.7104578887671699\n",
      "MSE = 0.7104580422553554\n",
      "MSE = 0.7106618668176816\n",
      "MSE = 0.710510220953302\n",
      "MSE = 0.710640976496012\n",
      "MSE = 0.7105217996236756\n",
      "MSE = 0.7105024325640507\n",
      "MSE = 0.7104766808851155\n",
      "MSE = 0.7105043809085296\n",
      "MSE = 0.7104776299205451\n",
      "MSE = 0.710473705485501\n",
      "MSE = 0.7104682291855052\n",
      "MSE = 0.7104452181163489\n",
      "MSE = 0.7104001925967808\n",
      "MSE = 0.7104222129907294\n",
      "MSE = 0.710428709258373\n",
      "MSE = 0.7103494052795178\n",
      "MSE = 0.7103913632653089\n",
      "MSE = 0.7103740608578705\n",
      "MSE = 0.7103847473067875\n",
      "MSE = 0.7103860984468147\n",
      "MSE = 0.710420636128243\n",
      "MSE = 0.7105005926231056\n",
      "MSE = 0.7104202879674991\n",
      "MSE = 0.7104085007991685\n",
      "MSE = 0.7104249760335271\n",
      "MSE = 0.7104077218748001\n",
      "MSE = 0.710384419597796\n",
      "MSE = 0.7103902501614039\n",
      "MSE = 0.7104025820303179\n",
      "MSE = 0.7104140711676837\n",
      "MSE = 0.7104376327981708\n",
      "MSE = 0.7105471742001216\n",
      "MSE = 0.7104367453239647\n",
      "MSE = 0.7104603387181071\n",
      "MSE = 0.7104558078487193\n",
      "MSE = 0.7108604375657206\n",
      "MSE = 0.7104543206082242\n",
      "MSE = 0.7104365503771287\n",
      "MSE = 0.7104333578902476\n",
      "MSE = 0.7104225933129005\n",
      "MSE = 0.7104123964402662\n",
      "MSE = 0.7104193936348391\n",
      "MSE = 0.7104321561815222\n",
      "MSE = 0.7104194737873126\n",
      "MSE = 0.710404075692527\n",
      "MSE = 0.7103311391676365\n",
      "MSE = 0.710330817113953\n",
      "MSE = 0.7103183645545769\n",
      "MSE = 0.7102550879418309\n",
      "MSE = 0.7102998654880682\n",
      "MSE = 0.7102800164485176\n",
      "MSE = 0.7102508387878342\n",
      "MSE = 0.7102687848026669\n",
      "MSE = 0.7102651601157864\n",
      "MSE = 0.7102547557117423\n",
      "MSE = 0.7102321776186397\n",
      "MSE = 0.7101970996607907\n",
      "MSE = 0.7101420763114352\n",
      "MSE = 0.7101251587027412\n",
      "MSE = 0.7102390130929441\n",
      "MSE = 0.7101336306532401\n",
      "MSE = 0.710129808207142\n",
      "MSE = 0.710174384940446\n",
      "MSE = 0.7101341024096041\n",
      "MSE = 0.7101305963016912\n",
      "MSE = 0.7101299669808765\n",
      "MSE = 0.7101298407834971\n",
      "MSE = 0.7101298149159394\n",
      "MSE = 0.710129809589816\n",
      "MSE = 0.7101298084922311\n",
      "MSE = 0.7101298082658638\n",
      "MSE = 0.7101298082191896\n",
      "MSE = 0.710129808209646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.7101298082159404\n",
      "MSE = 0.7101298082108485\n",
      "MSE = 0.7101298082142109\n",
      "MSE = 0.7101298082115223\n",
      "MSE = 0.710129808210916\n",
      "MSE = 0.7101298082108485\n",
      "1.293056691437069\n"
     ]
    }
   ],
   "source": [
    "lamb_values = [1e-4, 8e-5, 6e-5, 5e-5, 4e-5, 3e-5, 2e-5, 1e-5]\n",
    "MSE_valid =[]\n",
    "thetas=[]\n",
    "best_theta = []\n",
    "best_MSE = 0\n",
    "\n",
    "for lamb in lamb_values:\n",
    "    print(\"Lambda = {}\".format(lamb))\n",
    "  \n",
    "    theta_init = [alpha] + [0.0]*(nUsers+nItems) +  \\\n",
    "            [random.random() * 0.1 - 0.05 for k in range(K*(nUsers+nItems))]\n",
    "    unpack(theta_init)\n",
    "    \n",
    "    def cost(theta, labels, lamb):\n",
    "        unpack(theta)\n",
    "        predictions = [prediction(user, book) for user,book in Xtrain]\n",
    "        cost = MSE(predictions, labels)\n",
    "        print(\"MSE = \" + str(cost))\n",
    "        for u in users:\n",
    "            cost += lamb*userBiases[u]**2\n",
    "            for k in range(K):\n",
    "                cost += lamb*userGamma[u][k]**2\n",
    "        for i in items:\n",
    "            cost += lamb*itemBiases[i]**2\n",
    "            for k in range(K):\n",
    "                cost += lamb*itemGamma[i][k]**2\n",
    "        return cost\n",
    "\n",
    "    def derivative(theta, labels, lamb):\n",
    "        unpack(theta)\n",
    "        N = len(dataset)\n",
    "        dalpha = 0\n",
    "        dUserBiases = defaultdict(float)\n",
    "        dItemBiases = defaultdict(float)\n",
    "        dUserGamma = {}\n",
    "        dItemGamma = {}\n",
    "        for u in ratingsPerUser:\n",
    "            dUserGamma[u] = [0.0 for k in range(K)]\n",
    "        for i in ratingsPerItem:\n",
    "            dItemGamma[i] = [0.0 for k in range(K)]\n",
    "        for value in zip(Xtrain,ytrain):\n",
    "            x,rating = value\n",
    "            u = x[0]\n",
    "            i = x[1]\n",
    "            pred = prediction(u, i)\n",
    "            diff = pred - rating\n",
    "            dalpha += 2/N*diff\n",
    "            dUserBiases[u] += 2/N*diff\n",
    "            dItemBiases[i] += 2/N*diff\n",
    "            for k in range(K):\n",
    "                dUserGamma[u][k] += 2/N*itemGamma[i][k]*diff\n",
    "                dItemGamma[i][k] += 2/N*userGamma[u][k]*diff\n",
    "        for u in userBiases:\n",
    "            dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "            for k in range(K):\n",
    "                dUserGamma[u][k] += 2*lamb*userGamma[u][k]\n",
    "        for i in itemBiases:\n",
    "            dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "            for k in range(K):\n",
    "                dItemGamma[i][k] += 2*lamb*itemGamma[i][k]\n",
    "        dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "        for u in users:\n",
    "            dtheta += dUserGamma[u]\n",
    "        for i in items:\n",
    "            dtheta += dItemGamma[i]\n",
    "        return numpy.array(dtheta)\n",
    "    \n",
    "    theta,_,_ = scipy.optimize.fmin_l_bfgs_b(cost, theta_init, derivative, args = (ytrain, lamb))\n",
    "    thetas.append(theta)  # save thetas\n",
    "\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(user, book) for user,book in Xvalid]\n",
    "    cost = MSE(predictions, yvalid)\n",
    "    \n",
    "    if best_MSE is 0:\n",
    "        best_MSE = cost\n",
    "        best_theta = theta\n",
    "        print(\"Save best theta...\")\n",
    "    else:\n",
    "        if cost < best_MSE:\n",
    "            best_MSE = cost\n",
    "            best_theta = theta\n",
    "            print(\"Save best theta...\")\n",
    "    \n",
    "    MSE_valid.append(cost)\n",
    "    print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['u35030794', 'b34243773'], ['u03311330', 'b61525389'], ['u25185712', 'b48026111'], ['u19601482', 'b43546779'], ['u94100354', 'b39405098']] [[2423, 3488], [903, 1692], [4260, 2126], [6012, 983], [6456, 1313]]\n",
      "[['u46244592', 'b68658762'], ['u77810949', 'b76915592'], ['u86231895', 'b99482210'], ['u64961313', 'b25015745'], ['u38737686', 'b54883246']] [[7059, 3762], [7189, 272], [7070, 1880], [520, 2675], [6883, 1034]]\n"
     ]
    }
   ],
   "source": [
    "train_x = id_to_index_reformat(Xtrain)\n",
    "valid_x = id_to_index_reformat(Xvalid)\n",
    "\n",
    "train_y = ytrain\n",
    "valid_y = yvalid\n",
    "\n",
    "print (Xtrain[10:15], train_x[10:15])\n",
    "print (Xvalid[10:15], valid_x[10:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4355 11283 315 u62498455 b39969042\n",
      "['u62498455', 'b39969042']\n",
      "28854 4784 25 u90197731 b50835324\n",
      "['u90197731', 'b50835324']\n",
      "6355 5148 1935 u05719364 b40443264\n",
      "['u05719364', 'b40443264']\n",
      "164132 9379 117 u88101401 b92932322\n",
      "['u88101401', 'b92932322']\n",
      "153147 485 3043 u45516241 b30970717\n",
      "['u45516241', 'b30970717']\n"
     ]
    }
   ],
   "source": [
    "# Basic QC\n",
    "\n",
    "for i in range(5):\n",
    "    index = random.randint(0, len(train_x))\n",
    "    uid = train_x[index][0]\n",
    "    bid = train_x[index][1]\n",
    "    print(index, uid, bid, user_ids[uid],book_ids[bid])\n",
    "    print(Xtrain[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Iteration[0] Training Loss: 1.703\n",
      "Epoch[0] Validation Loss: 1.485 \n",
      "Epoch[0] Iteration[100] Training Loss: 1.442\n",
      "Epoch[0] Validation Loss: 1.482 \n",
      "Epoch[10] Iteration[0] Training Loss: 1.485\n",
      "Epoch[10] Validation Loss: 1.441 \n",
      "Epoch[10] Iteration[100] Training Loss: 1.481\n",
      "Epoch[10] Validation Loss: 1.439 \n",
      "Epoch[20] Iteration[0] Training Loss: 1.261\n",
      "Epoch[20] Validation Loss: 1.405 \n",
      "Epoch[20] Iteration[100] Training Loss: 1.333\n",
      "Epoch[20] Validation Loss: 1.403 \n",
      "Epoch[30] Iteration[0] Training Loss: 1.493\n",
      "Epoch[30] Validation Loss: 1.376 \n",
      "Epoch[30] Iteration[100] Training Loss: 1.090\n",
      "Epoch[30] Validation Loss: 1.373 \n",
      "Epoch[40] Iteration[0] Training Loss: 1.421\n",
      "Epoch[40] Validation Loss: 1.348 \n",
      "Epoch[40] Iteration[100] Training Loss: 1.343\n",
      "Epoch[40] Validation Loss: 1.347 \n",
      "Epoch[50] Iteration[0] Training Loss: 1.158\n",
      "Epoch[50] Validation Loss: 1.325 \n",
      "Epoch[50] Iteration[100] Training Loss: 1.244\n",
      "Epoch[50] Validation Loss: 1.324 \n",
      "Epoch[60] Iteration[0] Training Loss: 1.188\n",
      "Epoch[60] Validation Loss: 1.304 \n",
      "Epoch[60] Iteration[100] Training Loss: 1.130\n",
      "Epoch[60] Validation Loss: 1.303 \n",
      "Epoch[70] Iteration[0] Training Loss: 1.308\n",
      "Epoch[70] Validation Loss: 1.286 \n",
      "Epoch[70] Iteration[100] Training Loss: 1.299\n",
      "Epoch[70] Validation Loss: 1.285 \n",
      "Epoch[80] Iteration[0] Training Loss: 1.391\n",
      "Epoch[80] Validation Loss: 1.269 \n",
      "Epoch[80] Iteration[100] Training Loss: 1.314\n",
      "Epoch[80] Validation Loss: 1.269 \n",
      "Epoch[90] Iteration[0] Training Loss: 1.091\n",
      "Epoch[90] Validation Loss: 1.255 \n",
      "Epoch[90] Iteration[100] Training Loss: 1.156\n",
      "Epoch[90] Validation Loss: 1.254 \n",
      "Epoch[100] Iteration[0] Training Loss: 1.211\n",
      "Epoch[100] Validation Loss: 1.241 \n",
      "Epoch[100] Iteration[100] Training Loss: 1.330\n",
      "Epoch[100] Validation Loss: 1.240 \n",
      "Epoch[110] Iteration[0] Training Loss: 1.109\n",
      "Epoch[110] Validation Loss: 1.229 \n",
      "Epoch[110] Iteration[100] Training Loss: 1.249\n",
      "Epoch[110] Validation Loss: 1.228 \n",
      "Epoch[120] Iteration[0] Training Loss: 1.021\n",
      "Epoch[120] Validation Loss: 1.218 \n",
      "Epoch[120] Iteration[100] Training Loss: 1.216\n",
      "Epoch[120] Validation Loss: 1.217 \n",
      "Epoch[130] Iteration[0] Training Loss: 1.082\n",
      "Epoch[130] Validation Loss: 1.207 \n",
      "Epoch[130] Iteration[100] Training Loss: 1.107\n",
      "Epoch[130] Validation Loss: 1.207 \n",
      "Epoch[140] Iteration[0] Training Loss: 1.043\n",
      "Epoch[140] Validation Loss: 1.198 \n",
      "Epoch[140] Iteration[100] Training Loss: 0.946\n",
      "Epoch[140] Validation Loss: 1.198 \n",
      "Epoch[150] Iteration[0] Training Loss: 1.160\n",
      "Epoch[150] Validation Loss: 1.190 \n",
      "Epoch[150] Iteration[100] Training Loss: 1.141\n",
      "Epoch[150] Validation Loss: 1.189 \n",
      "Epoch[160] Iteration[0] Training Loss: 1.152\n",
      "Epoch[160] Validation Loss: 1.182 \n",
      "Epoch[160] Iteration[100] Training Loss: 1.069\n",
      "Epoch[160] Validation Loss: 1.181 \n",
      "Epoch[170] Iteration[0] Training Loss: 0.991\n",
      "Epoch[170] Validation Loss: 1.175 \n",
      "Epoch[170] Iteration[100] Training Loss: 0.852\n",
      "Epoch[170] Validation Loss: 1.174 \n",
      "Epoch[180] Iteration[0] Training Loss: 1.175\n",
      "Epoch[180] Validation Loss: 1.168 \n",
      "Epoch[180] Iteration[100] Training Loss: 1.008\n",
      "Epoch[180] Validation Loss: 1.167 \n",
      "Epoch[190] Iteration[0] Training Loss: 1.070\n",
      "Epoch[190] Validation Loss: 1.162 \n",
      "Epoch[190] Iteration[100] Training Loss: 0.986\n",
      "Epoch[190] Validation Loss: 1.161 \n",
      "Epoch[200] Iteration[0] Training Loss: 1.233\n",
      "Epoch[200] Validation Loss: 1.156 \n",
      "Epoch[200] Iteration[100] Training Loss: 1.145\n",
      "Epoch[200] Validation Loss: 1.156 \n",
      "Epoch[210] Iteration[0] Training Loss: 0.924\n",
      "Epoch[210] Validation Loss: 1.151 \n",
      "Epoch[210] Iteration[100] Training Loss: 1.080\n",
      "Epoch[210] Validation Loss: 1.150 \n",
      "Epoch[220] Iteration[0] Training Loss: 1.003\n",
      "Epoch[220] Validation Loss: 1.146 \n",
      "Epoch[220] Iteration[100] Training Loss: 1.169\n",
      "Epoch[220] Validation Loss: 1.145 \n",
      "Epoch[230] Iteration[0] Training Loss: 1.030\n",
      "Epoch[230] Validation Loss: 1.141 \n",
      "Epoch[230] Iteration[100] Training Loss: 1.030\n",
      "Epoch[230] Validation Loss: 1.141 \n",
      "Epoch[240] Iteration[0] Training Loss: 1.054\n",
      "Epoch[240] Validation Loss: 1.137 \n",
      "Epoch[240] Iteration[100] Training Loss: 1.057\n",
      "Epoch[240] Validation Loss: 1.137 \n",
      "Epoch[250] Iteration[0] Training Loss: 0.909\n",
      "Epoch[250] Validation Loss: 1.133 \n",
      "Epoch[250] Iteration[100] Training Loss: 1.093\n",
      "Epoch[250] Validation Loss: 1.133 \n",
      "Epoch[260] Iteration[0] Training Loss: 0.884\n",
      "Epoch[260] Validation Loss: 1.130 \n",
      "Epoch[260] Iteration[100] Training Loss: 0.982\n",
      "Epoch[260] Validation Loss: 1.129 \n",
      "Epoch[270] Iteration[0] Training Loss: 1.041\n",
      "Epoch[270] Validation Loss: 1.126 \n",
      "Epoch[270] Iteration[100] Training Loss: 1.038\n",
      "Epoch[270] Validation Loss: 1.126 \n",
      "Epoch[280] Iteration[0] Training Loss: 1.023\n",
      "Epoch[280] Validation Loss: 1.123 \n",
      "Epoch[280] Iteration[100] Training Loss: 0.994\n",
      "Epoch[280] Validation Loss: 1.122 \n",
      "Epoch[290] Iteration[0] Training Loss: 1.005\n",
      "Epoch[290] Validation Loss: 1.120 \n",
      "Epoch[290] Iteration[100] Training Loss: 1.002\n",
      "Epoch[290] Validation Loss: 1.119 \n",
      "Epoch[300] Iteration[0] Training Loss: 1.014\n",
      "Epoch[300] Validation Loss: 1.117 \n",
      "Epoch[300] Iteration[100] Training Loss: 1.040\n",
      "Epoch[300] Validation Loss: 1.117 \n",
      "Epoch[310] Iteration[0] Training Loss: 0.899\n",
      "Epoch[310] Validation Loss: 1.114 \n",
      "Epoch[310] Iteration[100] Training Loss: 0.979\n",
      "Epoch[310] Validation Loss: 1.114 \n",
      "Epoch[320] Iteration[0] Training Loss: 1.025\n",
      "Epoch[320] Validation Loss: 1.112 \n",
      "Epoch[320] Iteration[100] Training Loss: 1.042\n",
      "Epoch[320] Validation Loss: 1.112 \n",
      "Epoch[330] Iteration[0] Training Loss: 1.010\n",
      "Epoch[330] Validation Loss: 1.110 \n",
      "Epoch[330] Iteration[100] Training Loss: 1.068\n",
      "Epoch[330] Validation Loss: 1.110 \n",
      "Epoch[340] Iteration[0] Training Loss: 0.941\n",
      "Epoch[340] Validation Loss: 1.108 \n",
      "Epoch[340] Iteration[100] Training Loss: 0.802\n",
      "Epoch[340] Validation Loss: 1.108 \n",
      "Epoch[350] Iteration[0] Training Loss: 1.000\n",
      "Epoch[350] Validation Loss: 1.106 \n",
      "Epoch[350] Iteration[100] Training Loss: 0.934\n",
      "Epoch[350] Validation Loss: 1.106 \n",
      "Epoch[360] Iteration[0] Training Loss: 0.980\n",
      "Epoch[360] Validation Loss: 1.104 \n",
      "Epoch[360] Iteration[100] Training Loss: 1.006\n",
      "Epoch[360] Validation Loss: 1.104 \n",
      "Epoch[370] Iteration[0] Training Loss: 1.023\n",
      "Epoch[370] Validation Loss: 1.102 \n",
      "Epoch[370] Iteration[100] Training Loss: 0.912\n",
      "Epoch[370] Validation Loss: 1.102 \n",
      "Epoch[380] Iteration[0] Training Loss: 0.971\n",
      "Epoch[380] Validation Loss: 1.100 \n",
      "Epoch[380] Iteration[100] Training Loss: 0.986\n",
      "Epoch[380] Validation Loss: 1.100 \n",
      "Epoch[390] Iteration[0] Training Loss: 1.036\n",
      "Epoch[390] Validation Loss: 1.099 \n",
      "Epoch[390] Iteration[100] Training Loss: 0.883\n",
      "Epoch[390] Validation Loss: 1.099 \n",
      "Epoch[400] Iteration[0] Training Loss: 1.034\n",
      "Epoch[400] Validation Loss: 1.097 \n",
      "Epoch[400] Iteration[100] Training Loss: 0.999\n",
      "Epoch[400] Validation Loss: 1.097 \n",
      "Epoch[410] Iteration[0] Training Loss: 1.018\n",
      "Epoch[410] Validation Loss: 1.096 \n",
      "Epoch[410] Iteration[100] Training Loss: 0.902\n",
      "Epoch[410] Validation Loss: 1.096 \n",
      "Epoch[420] Iteration[0] Training Loss: 0.957\n",
      "Epoch[420] Validation Loss: 1.095 \n",
      "Epoch[420] Iteration[100] Training Loss: 0.808\n",
      "Epoch[420] Validation Loss: 1.095 \n",
      "Epoch[430] Iteration[0] Training Loss: 0.953\n",
      "Epoch[430] Validation Loss: 1.094 \n",
      "Epoch[430] Iteration[100] Training Loss: 0.833\n",
      "Epoch[430] Validation Loss: 1.094 \n",
      "Epoch[440] Iteration[0] Training Loss: 1.091\n",
      "Epoch[440] Validation Loss: 1.093 \n",
      "Epoch[440] Iteration[100] Training Loss: 1.070\n",
      "Epoch[440] Validation Loss: 1.093 \n",
      "Epoch[450] Iteration[0] Training Loss: 0.863\n",
      "Epoch[450] Validation Loss: 1.092 \n",
      "Epoch[450] Iteration[100] Training Loss: 0.945\n",
      "Epoch[450] Validation Loss: 1.092 \n",
      "Epoch[460] Iteration[0] Training Loss: 0.978\n",
      "Epoch[460] Validation Loss: 1.091 \n",
      "Epoch[460] Iteration[100] Training Loss: 1.020\n",
      "Epoch[460] Validation Loss: 1.091 \n",
      "Epoch[470] Iteration[0] Training Loss: 1.015\n",
      "Epoch[470] Validation Loss: 1.090 \n",
      "Epoch[470] Iteration[100] Training Loss: 0.929\n",
      "Epoch[470] Validation Loss: 1.090 \n",
      "Epoch[480] Iteration[0] Training Loss: 1.048\n",
      "Epoch[480] Validation Loss: 1.089 \n",
      "Epoch[480] Iteration[100] Training Loss: 0.909\n",
      "Epoch[480] Validation Loss: 1.089 \n",
      "Epoch[490] Iteration[0] Training Loss: 0.963\n",
      "Epoch[490] Validation Loss: 1.089 \n",
      "Epoch[490] Iteration[100] Training Loss: 0.800\n",
      "Epoch[490] Validation Loss: 1.088 \n",
      "Epoch[500] Iteration[0] Training Loss: 1.022\n",
      "Epoch[500] Validation Loss: 1.087 \n",
      "Epoch[500] Iteration[100] Training Loss: 0.916\n",
      "Epoch[500] Validation Loss: 1.087 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[510] Iteration[0] Training Loss: 0.921\n",
      "Epoch[510] Validation Loss: 1.087 \n",
      "Epoch[510] Iteration[100] Training Loss: 0.966\n",
      "Epoch[510] Validation Loss: 1.087 \n",
      "Epoch[520] Iteration[0] Training Loss: 0.980\n",
      "Epoch[520] Validation Loss: 1.086 \n",
      "Epoch[520] Iteration[100] Training Loss: 0.979\n",
      "Epoch[520] Validation Loss: 1.086 \n",
      "Epoch[530] Iteration[0] Training Loss: 0.997\n",
      "Epoch[530] Validation Loss: 1.085 \n",
      "Epoch[530] Iteration[100] Training Loss: 0.902\n",
      "Epoch[530] Validation Loss: 1.085 \n",
      "Epoch[540] Iteration[0] Training Loss: 1.026\n",
      "Epoch[540] Validation Loss: 1.085 \n",
      "Epoch[540] Iteration[100] Training Loss: 1.057\n",
      "Epoch[540] Validation Loss: 1.085 \n",
      "Epoch[550] Iteration[0] Training Loss: 0.994\n",
      "Epoch[550] Validation Loss: 1.084 \n",
      "Epoch[550] Iteration[100] Training Loss: 0.964\n",
      "Epoch[550] Validation Loss: 1.084 \n",
      "Epoch[560] Iteration[0] Training Loss: 1.017\n",
      "Epoch[560] Validation Loss: 1.084 \n",
      "Epoch[560] Iteration[100] Training Loss: 0.880\n",
      "Epoch[560] Validation Loss: 1.084 \n",
      "Epoch[570] Iteration[0] Training Loss: 1.072\n",
      "Epoch[570] Validation Loss: 1.083 \n",
      "Epoch[570] Iteration[100] Training Loss: 0.978\n",
      "Epoch[570] Validation Loss: 1.084 \n",
      "Epoch[580] Iteration[0] Training Loss: 0.884\n",
      "Epoch[580] Validation Loss: 1.083 \n",
      "Epoch[580] Iteration[100] Training Loss: 0.943\n",
      "Epoch[580] Validation Loss: 1.083 \n",
      "Epoch[590] Iteration[0] Training Loss: 0.914\n",
      "Epoch[590] Validation Loss: 1.082 \n",
      "Epoch[590] Iteration[100] Training Loss: 0.959\n",
      "Epoch[590] Validation Loss: 1.082 \n",
      "Epoch[600] Iteration[0] Training Loss: 0.958\n",
      "Epoch[600] Validation Loss: 1.082 \n",
      "Epoch[600] Iteration[100] Training Loss: 0.858\n",
      "Epoch[600] Validation Loss: 1.082 \n",
      "Epoch[610] Iteration[0] Training Loss: 0.986\n",
      "Epoch[610] Validation Loss: 1.082 \n",
      "Epoch[610] Iteration[100] Training Loss: 0.926\n",
      "Epoch[610] Validation Loss: 1.082 \n",
      "Epoch[620] Iteration[0] Training Loss: 0.969\n",
      "Epoch[620] Validation Loss: 1.081 \n",
      "Epoch[620] Iteration[100] Training Loss: 1.018\n",
      "Epoch[620] Validation Loss: 1.081 \n",
      "Epoch[630] Iteration[0] Training Loss: 1.042\n",
      "Epoch[630] Validation Loss: 1.081 \n",
      "Epoch[630] Iteration[100] Training Loss: 0.805\n",
      "Epoch[630] Validation Loss: 1.081 \n",
      "Epoch[640] Iteration[0] Training Loss: 0.931\n",
      "Epoch[640] Validation Loss: 1.081 \n",
      "Epoch[640] Iteration[100] Training Loss: 0.914\n",
      "Epoch[640] Validation Loss: 1.081 \n",
      "Epoch[650] Iteration[0] Training Loss: 0.879\n",
      "Epoch[650] Validation Loss: 1.080 \n",
      "Epoch[650] Iteration[100] Training Loss: 0.937\n",
      "Epoch[650] Validation Loss: 1.080 \n",
      "Epoch[660] Iteration[0] Training Loss: 1.016\n",
      "Epoch[660] Validation Loss: 1.080 \n",
      "Epoch[660] Iteration[100] Training Loss: 1.000\n",
      "Epoch[660] Validation Loss: 1.080 \n",
      "Epoch[670] Iteration[0] Training Loss: 0.879\n",
      "Epoch[670] Validation Loss: 1.080 \n",
      "Epoch[670] Iteration[100] Training Loss: 1.021\n",
      "Epoch[670] Validation Loss: 1.080 \n",
      "Epoch[680] Iteration[0] Training Loss: 0.992\n",
      "Epoch[680] Validation Loss: 1.080 \n",
      "Epoch[680] Iteration[100] Training Loss: 1.093\n",
      "Epoch[680] Validation Loss: 1.080 \n",
      "Epoch[690] Iteration[0] Training Loss: 0.935\n",
      "Epoch[690] Validation Loss: 1.079 \n",
      "Epoch[690] Iteration[100] Training Loss: 1.065\n",
      "Epoch[690] Validation Loss: 1.080 \n",
      "Epoch[700] Iteration[0] Training Loss: 0.865\n",
      "Epoch[700] Validation Loss: 1.079 \n",
      "Epoch[700] Iteration[100] Training Loss: 0.982\n",
      "Epoch[700] Validation Loss: 1.079 \n",
      "Epoch[710] Iteration[0] Training Loss: 0.980\n",
      "Epoch[710] Validation Loss: 1.079 \n",
      "Epoch[710] Iteration[100] Training Loss: 0.951\n",
      "Epoch[710] Validation Loss: 1.080 \n",
      "Epoch[720] Iteration[0] Training Loss: 0.953\n",
      "Epoch[720] Validation Loss: 1.079 \n",
      "Epoch[720] Iteration[100] Training Loss: 1.037\n",
      "Epoch[720] Validation Loss: 1.079 \n",
      "Epoch[730] Iteration[0] Training Loss: 0.948\n",
      "Epoch[730] Validation Loss: 1.079 \n",
      "Epoch[730] Iteration[100] Training Loss: 0.959\n",
      "Epoch[730] Validation Loss: 1.079 \n",
      "Epoch[740] Iteration[0] Training Loss: 0.948\n",
      "Epoch[740] Validation Loss: 1.078 \n",
      "Epoch[740] Iteration[100] Training Loss: 0.880\n",
      "Epoch[740] Validation Loss: 1.078 \n",
      "Epoch[750] Iteration[0] Training Loss: 0.983\n",
      "Epoch[750] Validation Loss: 1.078 \n",
      "Epoch[750] Iteration[100] Training Loss: 0.930\n",
      "Epoch[750] Validation Loss: 1.078 \n",
      "Epoch[760] Iteration[0] Training Loss: 0.890\n",
      "Epoch[760] Validation Loss: 1.079 \n",
      "Epoch[760] Iteration[100] Training Loss: 0.925\n",
      "Epoch[760] Validation Loss: 1.078 \n",
      "Epoch[770] Iteration[0] Training Loss: 0.920\n",
      "Epoch[770] Validation Loss: 1.078 \n",
      "Epoch[770] Iteration[100] Training Loss: 0.943\n",
      "Epoch[770] Validation Loss: 1.078 \n",
      "Epoch[780] Iteration[0] Training Loss: 0.935\n",
      "Epoch[780] Validation Loss: 1.078 \n",
      "Epoch[780] Iteration[100] Training Loss: 0.964\n",
      "Epoch[780] Validation Loss: 1.078 \n",
      "Epoch[790] Iteration[0] Training Loss: 1.034\n",
      "Epoch[790] Validation Loss: 1.078 \n",
      "Epoch[790] Iteration[100] Training Loss: 0.920\n",
      "Epoch[790] Validation Loss: 1.078 \n",
      "Epoch[800] Iteration[0] Training Loss: 0.850\n",
      "Epoch[800] Validation Loss: 1.078 \n",
      "Epoch[800] Iteration[100] Training Loss: 0.861\n",
      "Epoch[800] Validation Loss: 1.078 \n",
      "Epoch[810] Iteration[0] Training Loss: 0.865\n",
      "Epoch[810] Validation Loss: 1.078 \n",
      "Epoch[810] Iteration[100] Training Loss: 0.982\n",
      "Epoch[810] Validation Loss: 1.078 \n",
      "Epoch[820] Iteration[0] Training Loss: 0.953\n",
      "Epoch[820] Validation Loss: 1.077 \n",
      "Epoch[820] Iteration[100] Training Loss: 0.857\n",
      "Epoch[820] Validation Loss: 1.078 \n",
      "Epoch[830] Iteration[0] Training Loss: 0.975\n",
      "Epoch[830] Validation Loss: 1.077 \n",
      "Epoch[830] Iteration[100] Training Loss: 0.974\n",
      "Epoch[830] Validation Loss: 1.077 \n",
      "Epoch[840] Iteration[0] Training Loss: 0.972\n",
      "Epoch[840] Validation Loss: 1.078 \n",
      "Epoch[840] Iteration[100] Training Loss: 0.982\n",
      "Epoch[840] Validation Loss: 1.078 \n",
      "Epoch[850] Iteration[0] Training Loss: 0.913\n",
      "Epoch[850] Validation Loss: 1.077 \n",
      "Epoch[850] Iteration[100] Training Loss: 0.977\n",
      "Epoch[850] Validation Loss: 1.077 \n",
      "Epoch[860] Iteration[0] Training Loss: 0.976\n",
      "Epoch[860] Validation Loss: 1.077 \n",
      "Epoch[860] Iteration[100] Training Loss: 0.714\n",
      "Epoch[860] Validation Loss: 1.078 \n",
      "Epoch[870] Iteration[0] Training Loss: 0.836\n",
      "Epoch[870] Validation Loss: 1.077 \n",
      "Epoch[870] Iteration[100] Training Loss: 0.895\n",
      "Epoch[870] Validation Loss: 1.077 \n",
      "Epoch[880] Iteration[0] Training Loss: 0.886\n",
      "Epoch[880] Validation Loss: 1.077 \n",
      "Epoch[880] Iteration[100] Training Loss: 0.967\n",
      "Epoch[880] Validation Loss: 1.078 \n",
      "Epoch[890] Iteration[0] Training Loss: 0.839\n",
      "Epoch[890] Validation Loss: 1.077 \n",
      "Epoch[890] Iteration[100] Training Loss: 0.869\n",
      "Epoch[890] Validation Loss: 1.077 \n",
      "Epoch[900] Iteration[0] Training Loss: 0.890\n",
      "Epoch[900] Validation Loss: 1.077 \n",
      "Epoch[900] Iteration[100] Training Loss: 0.947\n",
      "Epoch[900] Validation Loss: 1.077 \n",
      "Epoch[910] Iteration[0] Training Loss: 0.974\n",
      "Epoch[910] Validation Loss: 1.077 \n",
      "Epoch[910] Iteration[100] Training Loss: 1.082\n",
      "Epoch[910] Validation Loss: 1.077 \n",
      "Epoch[920] Iteration[0] Training Loss: 0.958\n",
      "Epoch[920] Validation Loss: 1.077 \n",
      "Epoch[920] Iteration[100] Training Loss: 0.961\n",
      "Epoch[920] Validation Loss: 1.077 \n",
      "Epoch[930] Iteration[0] Training Loss: 0.874\n",
      "Epoch[930] Validation Loss: 1.077 \n",
      "Epoch[930] Iteration[100] Training Loss: 0.967\n",
      "Epoch[930] Validation Loss: 1.077 \n",
      "Epoch[940] Iteration[0] Training Loss: 0.959\n",
      "Epoch[940] Validation Loss: 1.077 \n",
      "Epoch[940] Iteration[100] Training Loss: 0.993\n",
      "Epoch[940] Validation Loss: 1.077 \n",
      "Epoch[950] Iteration[0] Training Loss: 0.883\n",
      "Epoch[950] Validation Loss: 1.077 \n",
      "Epoch[950] Iteration[100] Training Loss: 0.959\n",
      "Epoch[950] Validation Loss: 1.077 \n",
      "Epoch[960] Iteration[0] Training Loss: 0.934\n",
      "Epoch[960] Validation Loss: 1.077 \n",
      "Epoch[960] Iteration[100] Training Loss: 0.865\n",
      "Epoch[960] Validation Loss: 1.077 \n",
      "Epoch[970] Iteration[0] Training Loss: 0.939\n",
      "Epoch[970] Validation Loss: 1.077 \n",
      "Epoch[970] Iteration[100] Training Loss: 0.873\n",
      "Epoch[970] Validation Loss: 1.077 \n",
      "Epoch[980] Iteration[0] Training Loss: 0.923\n",
      "Epoch[980] Validation Loss: 1.077 \n",
      "Epoch[980] Iteration[100] Training Loss: 0.895\n",
      "Epoch[980] Validation Loss: 1.077 \n",
      "Epoch[990] Iteration[0] Training Loss: 0.887\n",
      "Epoch[990] Validation Loss: 1.077 \n",
      "Epoch[990] Iteration[100] Training Loss: 0.955\n",
      "Epoch[990] Validation Loss: 1.077 \n",
      "Epoch[1000] Iteration[0] Training Loss: 0.888\n",
      "Epoch[1000] Validation Loss: 1.077 \n",
      "Epoch[1000] Iteration[100] Training Loss: 0.905\n",
      "Epoch[1000] Validation Loss: 1.077 \n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 1e-2\n",
    "\n",
    "# New parameter for regularizing bias\n",
    "lamb = 1e-5\n",
    "batch_size = 1024\n",
    "\n",
    "n_user = len(ratingsPerUser)\n",
    "n_item = len(ratingsPerItem)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "model = MF(n_user, n_item, mean=ratingMean)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=lamb, momentum=0.9)\n",
    "\n",
    "def chunks(X, Y, size):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    starts = list(range(0, len(X), size))\n",
    "    shuffle(starts)\n",
    "    for i in starts:\n",
    "        yield (X[i:i + size], Y[i:i + size])\n",
    "        \n",
    "losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(1000+1):\n",
    "    \n",
    "    i = 0\n",
    "    for feature, target in chunks(np.array(train_x), np.array(train_y), batch_size):\n",
    "        # This zeros the gradients on every parameter. \n",
    "        # This is easy to miss and hard to troubleshoot.\n",
    "        optimizer.zero_grad()\n",
    "        # Convert \n",
    "        feature = Variable(torch.from_numpy(feature))\n",
    "        target = Variable(torch.from_numpy(target).type(torch.FloatTensor))\n",
    "        \n",
    "        if cuda:\n",
    "            feature = feature.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "        # model in training mode    \n",
    "        model.train()\n",
    "            \n",
    "        # Compute a prediction for these features\n",
    "        prediction = model.forward(feature)\n",
    "        # Compute a loss given what the true target outcome was\n",
    "        loss = model.loss(prediction, target)\n",
    "        # break\n",
    "        # Backpropagate: compute the direction / gradient every model parameter\n",
    "        # defined in your __init__ should move in in order to minimize this loss\n",
    "        # However, we're not actually changing these parameters, we're just storing\n",
    "        # how they should change.\n",
    "\n",
    "        loss.backward()\n",
    "        # Now take a step & update the model parameters. The optimizer uses the gradient at \n",
    "        # defined on every parameter in our model and nudges it in that direction.\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%100 == 0 and epoch%10 == 0:\n",
    "            print(\"Epoch[{}] Iteration[{}] Training Loss: {:.3f}\".format(epoch, i, loss.data))\n",
    "\n",
    "        # Record the loss per example\n",
    "        losses.append(loss.cpu().data.numpy() / len(feature))\n",
    "        \n",
    "        if i%100 == 0 and epoch%10 == 0:\n",
    "            val_feature = torch.from_numpy(np.array(valid_x))\n",
    "            val_target = torch.from_numpy(np.array(valid_y)).type(torch.FloatTensor)\n",
    "            \n",
    "            if cuda:\n",
    "                val_feature = val_feature.cuda()\n",
    "                val_target = val_target.cuda()\n",
    "                \n",
    "            # model in test mode    \n",
    "            model.eval()\n",
    "\n",
    "            val_pred = model.forward(val_feature)\n",
    "            val_loss = model.loss(val_pred, val_target)\n",
    "            print(\"Epoch[{}] Validation Loss: {:.3f} \".format(epoch, val_loss.data))\n",
    "            \n",
    "            # Record the validation loss per example\n",
    "            valid_losses.append(val_loss.cpu().data.numpy()/len(val_feature))\n",
    "\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.897005805232604\n",
      "3.8115271045716583\n",
      "Parameter containing:\n",
      "tensor([3.8112], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "\n",
    "print(ratingMean)\n",
    "print(best_theta[0])\n",
    "print(model.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
