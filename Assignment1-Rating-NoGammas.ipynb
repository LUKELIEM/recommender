{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd \n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(path):\n",
    "  f = open(path, 'rt')\n",
    "  f.readline()\n",
    "  for l in f:\n",
    "    yield l.strip().split(',')\n",
    "    \n",
    "def calc_model_stats(pred,label):\n",
    "    \n",
    "    TP,FP,TN,FN = calc_metrics(pred,label)\n",
    "\n",
    "    # print(\"Stats\")\n",
    "    # print(TP,FP,TN,FN)\n",
    "\n",
    "    # print(\"Predict N: {} ({}%)\".format(TN+FN,(TN+FN)/(TP+TN+FP+FN)))\n",
    "    # print(\"Predict P: {} ({}%)\".format(TP+FP,(TP+FP)/(TP+TN+FP+FN)))\n",
    "\n",
    "    accuracy, TPR, TNR, BER = calc_error_rates(TP, FP, TN, FN)\n",
    "\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    # print(\"TPR: {}\".format(TPR))\n",
    "    # print(\"TNR: {}\".format(TNR))\n",
    "    # print(\"BER: {}\".format(BER))\n",
    "    \n",
    "    return\n",
    " \n",
    "def calc_metrics(predictions, labels):\n",
    "    # Calculate True positives, false positives, etc.\n",
    "\n",
    "    TP_ = numpy.logical_and(predictions, labels)\n",
    "    FP_ = numpy.logical_and(predictions, numpy.logical_not(labels))\n",
    "    TN_ = numpy.logical_and(numpy.logical_not(predictions), numpy.logical_not(labels))\n",
    "    FN_ = numpy.logical_and(numpy.logical_not(predictions), labels)\n",
    "\n",
    "    TP=sum(TP_)\n",
    "    FP=sum(FP_)\n",
    "    TN=sum(TN_)\n",
    "    FN=sum(FN_)\n",
    "    \n",
    "    return TP,FP,TN,FN\n",
    "\n",
    "def calc_error_rates(TP, FP, TN, FN):\n",
    "    # Calculate accuracy, TPR, TNR and BER\n",
    "    accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    TPR = TP/(TP+FN)\n",
    "    TNR = TN/(TN+FP)\n",
    "    BER = 1.0 - (TPR+TNR)/2\n",
    "    \n",
    "    return accuracy, TPR, TNR, BER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training Dataset into Train and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 190001 9999\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for user,book,rating in readCSV(\"../datasets/cse258/assignment1/train_Interactions.csv\"):\n",
    "  dataset.append([user,book,rating])\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "X = [values[0:2] for values in dataset]\n",
    "y = [int(values[-1]) for values in dataset]\n",
    "\n",
    "N = len(dataset)\n",
    "Ntrain = 190001\n",
    "\n",
    "Xtrain = X[:Ntrain]\n",
    "Xvalid = X[Ntrain:]\n",
    "\n",
    "ytrain = y[:Ntrain]\n",
    "yvalid = y[Ntrain:]\n",
    "\n",
    "print(N, len(ytrain),len(yvalid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adopt code from Workbook 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11357 7170\n",
      "['u88723822', 'u26592179', 'u61403643']\n",
      "['b46609859', 'b32330627', 'b60684530']\n"
     ]
    }
   ],
   "source": [
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "\n",
    "for user,book,rating in dataset:\n",
    "    ratingsPerUser[user].append(rating)\n",
    "    ratingsPerItem[book].append(rating)\n",
    "\n",
    "N = len(Xtrain)\n",
    "nUsers = len(ratingsPerUser)\n",
    "nItems = len(ratingsPerItem)\n",
    "users = list(ratingsPerUser.keys())\n",
    "items = list(ratingsPerItem.keys())\n",
    "\n",
    "print (nUsers, nItems)\n",
    "print (users[:3])\n",
    "print (items[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of ratings in training set: 3.8970215946231863\n"
     ]
    }
   ],
   "source": [
    "ratingMean = sum([y for y in ytrain])/len(ytrain)\n",
    "\n",
    "print(\"Mean of ratings in training set: {}\".format(ratingMean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ratingMean\n",
    "\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item]\n",
    "\n",
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))\n",
    "    \n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(user, book) for user,book in Xtrain]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost\n",
    "\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(dataset)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for value in zip(Xtrain,ytrain):\n",
    "        x,rating = value\n",
    "        user = x[0]\n",
    "        book = x[1]\n",
    "        pred = prediction(user,book)\n",
    "        diff = pred - rating\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[user] += 2/N*diff\n",
    "        dItemBiases[book] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 1e-05\n",
      "MSE = 1.473873856464982\n",
      "MSE = 1.456431368523354\n",
      "MSE = 1.3926990442309493\n",
      "MSE = 7.992699748763671\n",
      "MSE = 1.369015386525905\n",
      "MSE = 1.2039672102034578\n",
      "MSE = 1.201903208089255\n",
      "MSE = 1.1938184067777753\n",
      "MSE = 1.1642185158566818\n",
      "MSE = 1.0444310777472559\n",
      "MSE = 0.9961973927697352\n",
      "MSE = 0.9570780562063416\n",
      "MSE = 0.9424563174422734\n",
      "MSE = 0.9288203623178237\n",
      "MSE = 0.9226515027555446\n",
      "MSE = 0.9188661478677207\n",
      "MSE = 0.9181155350634062\n",
      "MSE = 0.9168705154643094\n",
      "MSE = 0.9170858556162015\n",
      "MSE = 0.9157322652244629\n",
      "MSE = 0.9149927307736362\n",
      "MSE = 0.9142339137164336\n",
      "MSE = 0.9141373205699372\n",
      "MSE = 0.9132090084612581\n",
      "MSE = 0.9797877181377072\n",
      "MSE = 0.912983782825024\n",
      "MSE = 0.9128937238404544\n",
      "MSE = 0.9129145065234482\n",
      "MSE = 0.9128795012236577\n",
      "MSE = 0.9127529835819246\n",
      "MSE = 0.913431250117685\n",
      "MSE = 0.9128888237328417\n",
      "MSE = 0.9128972885782255\n",
      "MSE = 0.9128438048084605\n",
      "MSE = 0.9127817050062647\n",
      "MSE = 0.9128158900139834\n",
      "MSE = 0.9126876402347415\n",
      "MSE = 0.9128705733836319\n",
      "MSE = 0.9129293007563997\n",
      "MSE = 0.9129050335954015\n",
      "MSE = 0.9128614371164295\n",
      "MSE = 0.9128238177204869\n",
      "MSE = 0.9126080695000812\n",
      "MSE = 0.9127869002070147\n",
      "MSE = 0.9127851094774888\n",
      "MSE = 0.9127863069306136\n",
      "MSE = 0.9127883191139359\n",
      "MSE = 0.9128086313299494\n",
      "MSE = 0.9128471652044159\n",
      "MSE = 0.9128341395445202\n",
      "MSE = 0.9145441958714796\n",
      "MSE = 0.9128751817242488\n",
      "MSE = 0.9128403298301886\n",
      "MSE = 0.912835212775375\n",
      "MSE = 0.9128343299288718\n",
      "MSE = 0.9128341734540172\n",
      "MSE = 0.912834145588351\n",
      "MSE = 0.9128341406217663\n",
      "MSE = 0.9128341397365001\n",
      "MSE = 0.9128341395786655\n",
      "MSE = 0.9128341395505541\n",
      "MSE = 0.9128341395455946\n",
      "MSE = 0.9128341395446422\n",
      "MSE = 0.9128341395452867\n",
      "MSE = 0.9128341395446997\n",
      "MSE = 0.9128341395446471\n",
      "MSE = 0.912834139544682\n",
      "MSE = 0.9128341395446489\n",
      "MSE = 0.9128341395446471\n",
      "Save best theta...\n",
      "1.0741282593688577\n",
      "Lambda = 1.3e-05\n",
      "MSE = 1.4818879988651132\n",
      "MSE = 2.2925612550697956\n",
      "MSE = 1.4738027770551698\n",
      "MSE = 1.473551930788236\n",
      "MSE = 1.4731880808412519\n",
      "MSE = 1.4719398546579197\n",
      "MSE = 1.4689947950976479\n",
      "MSE = 1.461182727664514\n",
      "MSE = 1.4422740980872266\n",
      "MSE = 1.4000157129815474\n",
      "MSE = 1.3258237237655202\n",
      "MSE = 1.243990087182269\n",
      "MSE = 1.2033869370535633\n",
      "MSE = 1.1974133878584146\n",
      "MSE = 1.1974777386710698\n",
      "MSE = 1.1972258972911094\n",
      "MSE = 1.194208469008155\n",
      "MSE = 1.1860227179674028\n",
      "MSE = 1.1651493943648057\n",
      "MSE = 1.1283170015763255\n",
      "MSE = 1.0877730873917009\n",
      "MSE = 1.065449723840445\n",
      "MSE = 62.566556770359306\n",
      "MSE = 1.0616550171836256\n",
      "MSE = 1.059790152284822\n",
      "MSE = 1.058373106757916\n",
      "MSE = 1.0512402217162118\n",
      "MSE = 1.0325678169550982\n",
      "MSE = 1.0090661077308665\n",
      "MSE = 0.9835908079987827\n",
      "MSE = 0.973759745623918\n",
      "MSE = 0.9694452644474405\n",
      "MSE = 0.9663223698113852\n",
      "MSE = 0.9641471667795707\n",
      "MSE = 0.9627596006550938\n",
      "MSE = 0.9599232273690772\n",
      "MSE = 0.9579408942411834\n",
      "MSE = 0.9547838587224465\n",
      "MSE = 0.9504519086659104\n",
      "MSE = 0.9454261198694308\n",
      "MSE = 0.9414462442492366\n",
      "MSE = 1.2894525431657577\n",
      "MSE = 0.9419426312510318\n",
      "MSE = 0.9410935206266599\n",
      "MSE = 0.9407481864542498\n",
      "MSE = 0.939776968516518\n",
      "MSE = 0.9373733365793383\n",
      "MSE = 0.9362309266002209\n",
      "MSE = 0.9283853193908768\n",
      "MSE = 0.9270606977979341\n",
      "MSE = 0.9270560013202266\n",
      "MSE = 0.9271809986603341\n",
      "MSE = 0.924890242554623\n",
      "MSE = 1.1037332299649514\n",
      "MSE = 0.9245085772994488\n",
      "MSE = 0.9225608727215713\n",
      "MSE = 0.9469805876266658\n",
      "MSE = 0.9220939328184258\n",
      "MSE = 0.9216674257373317\n",
      "MSE = 0.9200174836897036\n",
      "MSE = 0.9209810244122776\n",
      "MSE = 0.9231160453057401\n",
      "MSE = 0.9215446388657483\n",
      "MSE = 0.9204924027524687\n",
      "MSE = 0.919661828985809\n",
      "MSE = 0.9200489024579526\n",
      "MSE = 0.9196998271592419\n",
      "MSE = 0.9195429350479309\n",
      "MSE = 0.9194964720239199\n",
      "MSE = 0.9190637091997976\n",
      "MSE = 0.9206098681794451\n",
      "MSE = 0.9191070075769594\n",
      "MSE = 0.9191230334028908\n",
      "MSE = 0.9190849585867703\n",
      "MSE = 0.9189516111373703\n",
      "MSE = 0.918673839010081\n",
      "MSE = 0.9176772373276769\n",
      "MSE = 0.9173322236553872\n",
      "MSE = 0.9175196434211694\n",
      "MSE = 0.9174100258978493\n",
      "MSE = 0.9172544367293927\n",
      "MSE = 0.9172526156224609\n",
      "MSE = 0.9170608479835198\n",
      "MSE = 0.9191377238549271\n",
      "MSE = 0.9170777980448657\n",
      "MSE = 0.9168401557760719\n",
      "MSE = 0.91680054047516\n",
      "MSE = 0.9166259983338757\n",
      "MSE = 0.9164224070630121\n",
      "MSE = 0.9161215400678682\n",
      "MSE = 0.9155682633809656\n",
      "MSE = 0.9157953070613454\n",
      "MSE = 0.9156081165124627\n",
      "MSE = 0.9157055360194264\n",
      "MSE = 0.9157771493193998\n",
      "MSE = 0.9159380648479493\n",
      "MSE = 0.9166746512044283\n",
      "MSE = 0.9159823606026722\n",
      "MSE = 0.9160529702083393\n",
      "MSE = 0.917520055277755\n",
      "MSE = 0.9162231405796666\n",
      "MSE = 0.9163155184289784\n",
      "MSE = 0.9162370295833807\n",
      "MSE = 0.9161695692617626\n",
      "MSE = 0.9161970631268082\n",
      "MSE = 0.916285555410315\n",
      "MSE = 0.9188202644370156\n",
      "MSE = 0.9163250526015213\n",
      "MSE = 0.9162941033442233\n",
      "MSE = 0.9164961700114646\n",
      "MSE = 0.9171683927514536\n",
      "MSE = 0.9165532643148263\n",
      "MSE = 0.9168070443022033\n",
      "MSE = 0.9166195647639306\n",
      "MSE = 0.9168995075542049\n",
      "MSE = 0.9166669547020194\n",
      "MSE = 0.9167682539078037\n",
      "MSE = 0.916682237439216\n",
      "MSE = 0.9168220767683224\n",
      "MSE = 0.9167030791524804\n",
      "MSE = 0.9166859427854943\n",
      "MSE = 0.9166829158356753\n",
      "MSE = 0.9166823623069045\n",
      "MSE = 0.9166822604452433\n",
      "MSE = 0.9166822416786929\n",
      "MSE = 0.9166822382205801\n",
      "MSE = 0.9166822375832084\n",
      "MSE = 0.9166822374657096\n",
      "MSE = 0.9166822374440811\n",
      "MSE = 0.9166822374400617\n",
      "MSE = 0.9166822374427299\n",
      "MSE = 0.9166822374404276\n",
      "MSE = 0.9166822374401304\n",
      "MSE = 0.9166822374403201\n",
      "MSE = 0.9166822374403201\n",
      "Save best theta...\n",
      "1.0735889944967134\n",
      "Lambda = 1.5e-05\n",
      "MSE = 1.481378125424722\n",
      "MSE = 2.2971593477100076\n",
      "MSE = 1.4737986702866603\n",
      "MSE = 1.4735519362254723\n",
      "MSE = 1.473177856335563\n",
      "MSE = 1.4719250355563018\n",
      "MSE = 1.468946800333569\n",
      "MSE = 1.4610723747348806\n",
      "MSE = 1.4419979853911973\n",
      "MSE = 1.399428478467204\n",
      "MSE = 1.3248548026398128\n",
      "MSE = 1.242983161984011\n",
      "MSE = 1.2028874493661568\n",
      "MSE = 1.1973112739833303\n",
      "MSE = 1.1974704326467394\n",
      "MSE = 1.1972519606385348\n",
      "MSE = 1.1943474491524302\n",
      "MSE = 1.186279160253385\n",
      "MSE = 1.1654280308834544\n",
      "MSE = 1.1285798896220545\n",
      "MSE = 1.0882248107773493\n",
      "MSE = 1.0653821105978143\n",
      "MSE = 44.85141486575742\n",
      "MSE = 1.061746331611171\n",
      "MSE = 1.0600049859402183\n",
      "MSE = 1.0583190642186915\n",
      "MSE = 1.0491565910869691\n",
      "MSE = 1.0282817926464427\n",
      "MSE = 1.0035792408686088\n",
      "MSE = 0.9916960713401355\n",
      "MSE = 0.982413826956607\n",
      "MSE = 0.977274533672793\n",
      "MSE = 0.969523261808489\n",
      "MSE = 0.9609155127414793\n",
      "MSE = 2.7477965493572376\n",
      "MSE = 0.9600946092849098\n",
      "MSE = 0.9561342809735596\n",
      "MSE = 0.9543858130526365\n",
      "MSE = 0.9510693927976283\n",
      "MSE = 0.9414617987061323\n",
      "MSE = 1.4998362692168643\n",
      "MSE = 0.9333128823711139\n",
      "MSE = 0.9337142877534653\n",
      "MSE = 0.9593213344274725\n",
      "MSE = 0.9335086827213211\n",
      "MSE = 0.9335723847881433\n",
      "MSE = 0.933189651081991\n",
      "MSE = 0.9308770890120911\n",
      "MSE = 1.6146171089983505\n",
      "MSE = 0.9308229588418433\n",
      "MSE = 0.9294759108604784\n",
      "MSE = 0.9276507034042653\n",
      "MSE = 0.9267019772469608\n",
      "MSE = 0.9265410563910019\n",
      "MSE = 0.9261523106911844\n",
      "MSE = 0.9257176948920842\n",
      "MSE = 0.9252197417534345\n",
      "MSE = 0.9249866375996239\n",
      "MSE = 0.924024645092308\n",
      "MSE = 0.9236530042109583\n",
      "MSE = 0.9231807829201346\n",
      "MSE = 0.9227171714651353\n",
      "MSE = 0.9229235346988877\n",
      "MSE = 0.9226623165654033\n",
      "MSE = 0.9223765086600045\n",
      "MSE = 0.9222744417374613\n",
      "MSE = 0.9222573276508085\n",
      "MSE = 0.9215796171319357\n",
      "MSE = 0.919259727245093\n",
      "MSE = 0.9205321482457983\n",
      "MSE = 0.9207404771725873\n",
      "MSE = 0.9205016783154722\n",
      "MSE = 0.9206533823928568\n",
      "MSE = 0.9209320791659915\n",
      "MSE = 0.9209277585399258\n",
      "MSE = 0.9203397851978337\n",
      "MSE = 0.9205433588886831\n",
      "MSE = 0.9205016947493662\n",
      "MSE = 0.9203364085445161\n",
      "MSE = 0.920441045645129\n",
      "MSE = 0.920761318786218\n",
      "MSE = 0.919865109008102\n",
      "MSE = 0.9200708426264893\n",
      "MSE = 0.9204914178671285\n",
      "MSE = 0.9201620868956859\n",
      "MSE = 0.9204745939065785\n",
      "MSE = 0.9253880020263934\n",
      "MSE = 0.9205289724765644\n",
      "MSE = 0.9204821716945251\n",
      "MSE = 0.9204758119682342\n",
      "MSE = 0.9204747939948601\n",
      "MSE = 0.9204746268909647\n",
      "MSE = 0.9204745993472125\n",
      "MSE = 0.9204745948041417\n",
      "MSE = 0.920474594054566\n",
      "MSE = 0.920474593930965\n",
      "MSE = 0.920474593910548\n",
      "MSE = 0.9204745939072444\n",
      "MSE = 0.920474593906658\n",
      "MSE = 0.920474593906583\n",
      "MSE = 0.9204745939065796\n",
      "MSE = 0.9204745939065785\n",
      "MSE = 0.9204745939065792\n",
      "MSE = 0.9204745939065785\n",
      "MSE = 0.9204745939065793\n",
      "MSE = 0.9204745939065785\n",
      "MSE = 0.9204741561841275\n",
      "MSE = 0.9204727348613666\n",
      "MSE = 0.9204726772106441\n",
      "MSE = 0.9204724810839743\n",
      "MSE = 0.9204709304708184\n",
      "MSE = 0.9204689823573898\n",
      "MSE = 0.9204671556824465\n",
      "MSE = 0.9204673538999774\n",
      "MSE = 0.9204681773061134\n",
      "MSE = 0.9204675545174575\n",
      "MSE = 0.9204668525783057\n",
      "MSE = 0.9204661537781976\n",
      "MSE = 0.9204651342658782\n",
      "MSE = 0.920464701943061\n",
      "MSE = 0.920467052602099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.9204773987964511\n",
      "MSE = 0.9205008896861069\n",
      "MSE = 0.9205641897659114\n",
      "MSE = 0.9205021149886699\n",
      "MSE = 0.920526938844379\n",
      "MSE = 0.9205359138749829\n",
      "MSE = 0.9205349594077047\n",
      "MSE = 0.9205337180239839\n",
      "MSE = 0.920533022976005\n",
      "MSE = 0.9205380861846606\n",
      "MSE = 0.9205590367903724\n",
      "MSE = 0.9206010820906585\n",
      "MSE = 0.9205978315337793\n",
      "MSE = 0.9205898525519265\n",
      "MSE = 0.920637792522961\n",
      "MSE = 0.9206302504042437\n",
      "MSE = 0.9206231300225696\n",
      "MSE = 0.9206194989551776\n",
      "MSE = 0.9211533454981179\n",
      "MSE = 0.9206147349746115\n",
      "MSE = 0.9206103173405396\n",
      "MSE = 0.9206328784657091\n",
      "MSE = 0.9205502182739058\n",
      "MSE = 0.9205881626679796\n",
      "MSE = 0.9205948835834266\n",
      "MSE = 0.9205849873875404\n",
      "MSE = 0.9208202587247981\n",
      "MSE = 0.9205768682622121\n",
      "MSE = 0.9205516158512381\n",
      "MSE = 0.9205227000333837\n",
      "MSE = 0.920482813803834\n",
      "MSE = 0.9204443061738207\n",
      "MSE = 0.9203738860768645\n",
      "MSE = 0.9204156913896236\n",
      "MSE = 0.9204257320344986\n",
      "MSE = 0.9204319066346137\n",
      "MSE = 0.9204264851928531\n",
      "MSE = 0.9204258368546867\n",
      "MSE = 0.9204257468784579\n",
      "MSE = 0.9204257341417151\n",
      "MSE = 0.9204257323336373\n",
      "MSE = 0.9204257320769976\n",
      "MSE = 0.9204257320405456\n",
      "MSE = 0.9204257320353273\n",
      "MSE = 0.920425732034596\n",
      "MSE = 0.9204257320344988\n",
      "MSE = 0.9204257320345\n",
      "MSE = 0.9204257320344988\n",
      "Save best theta...\n",
      "1.0734200113625347\n",
      "Lambda = 1.7e-05\n",
      "MSE = 1.4811804647362536\n",
      "MSE = 2.2989695434223663\n",
      "MSE = 1.473797040475686\n",
      "MSE = 1.473551934124409\n",
      "MSE = 1.4731735137933264\n",
      "MSE = 1.4719183939579128\n",
      "MSE = 1.468925482759017\n",
      "MSE = 1.461022477529896\n",
      "MSE = 1.4418710121827527\n",
      "MSE = 1.3991443156463492\n",
      "MSE = 1.324339555306592\n",
      "MSE = 1.2423613454725024\n",
      "MSE = 1.2025490684965188\n",
      "MSE = 1.1972757647888987\n",
      "MSE = 1.1975227334385172\n",
      "MSE = 1.197340930871478\n",
      "MSE = 1.194574717590222\n",
      "MSE = 1.1866608562480738\n",
      "MSE = 1.1659250161207997\n",
      "MSE = 1.1291328142459138\n",
      "MSE = 1.0889442934447844\n",
      "MSE = 1.0655215042937967\n",
      "MSE = 34.55645685737947\n",
      "MSE = 1.0619991599058591\n",
      "MSE = 1.060297479142427\n",
      "MSE = 1.0582828593909677\n",
      "MSE = 1.0468216364663432\n",
      "MSE = 1.0239749077053566\n",
      "MSE = 0.9971781399092462\n",
      "MSE = 1.0004239485748523\n",
      "MSE = 0.991267063106572\n",
      "MSE = 0.9860305125425023\n",
      "MSE = 0.9761245480630415\n",
      "MSE = 0.9704914884737553\n",
      "MSE = 0.9662771403463625\n",
      "MSE = 0.9652373790547245\n",
      "MSE = 0.9643642804111888\n",
      "MSE = 0.9632764084497568\n",
      "MSE = 0.9550092634679813\n",
      "MSE = 0.9530649532870422\n",
      "MSE = 0.9496596348581592\n",
      "MSE = 0.9448059962336467\n",
      "MSE = 5.302361774540402\n",
      "MSE = 0.9444647462760944\n",
      "MSE = 0.9431668050013324\n",
      "MSE = 0.9423389877035919\n",
      "MSE = 0.9422129556882272\n",
      "MSE = 0.9418995328052419\n",
      "MSE = 1.0160865635506315\n",
      "MSE = 0.942893835814424\n",
      "MSE = 0.9437210243173426\n",
      "MSE = 0.9449171271315246\n",
      "MSE = 0.9453361059026318\n",
      "MSE = 0.945526670093465\n",
      "MSE = 0.9456153452082058\n",
      "MSE = 0.9453465861713234\n",
      "MSE = 1.573844582328061\n",
      "MSE = 0.9453795057302556\n",
      "MSE = 0.9447181944641686\n",
      "MSE = 0.9441079012733181\n",
      "MSE = 0.9436792894738025\n",
      "MSE = 0.9432908431442245\n",
      "MSE = 0.9486720863319531\n",
      "MSE = 0.9435700997822\n",
      "MSE = 0.9427308838136906\n",
      "MSE = 0.9379519971073681\n",
      "MSE = 0.9408901668886568\n",
      "MSE = 0.9391397110671104\n",
      "MSE = 0.9347804150382113\n",
      "MSE = 0.9325651664268509\n",
      "MSE = 0.9322473904143218\n",
      "MSE = 0.9319209986636031\n",
      "MSE = 0.9320773772261249\n",
      "MSE = 0.9318029665803159\n",
      "MSE = 0.9304596983848946\n",
      "MSE = 0.9288061243225817\n",
      "MSE = 0.9248059752514489\n",
      "MSE = 0.9268197989484896\n",
      "MSE = 0.926152174680809\n",
      "MSE = 0.9260950248282082\n",
      "MSE = 0.9270424724971004\n",
      "MSE = 0.9262180420989823\n",
      "MSE = 0.926124496992463\n",
      "MSE = 0.9261860633603952\n",
      "MSE = 0.9264326675364373\n",
      "MSE = 0.9265509833623767\n",
      "MSE = 0.9266825415373071\n",
      "MSE = 0.9298363827562596\n",
      "MSE = 0.9267422885354816\n",
      "MSE = 0.9262491098142879\n",
      "MSE = 0.9258525139600037\n",
      "MSE = 0.9255058301495319\n",
      "MSE = 0.9253782132502638\n",
      "MSE = 0.9255855301130524\n",
      "MSE = 0.9260245882013673\n",
      "MSE = 0.9256956697509446\n",
      "MSE = 0.9261689758864621\n",
      "MSE = 0.9261743729742233\n",
      "MSE = 0.9261103074365314\n",
      "MSE = 0.9254950041013712\n",
      "MSE = 0.9251548432276901\n",
      "MSE = 0.9241469527608048\n",
      "MSE = 0.9249177152866386\n",
      "MSE = 0.9249335508366889\n",
      "MSE = 0.9255767676035962\n",
      "MSE = 0.9249808877646525\n",
      "MSE = 0.9250659367085617\n",
      "MSE = 0.9250199821493194\n",
      "MSE = 0.9249647860200493\n",
      "MSE = 0.9248420675423944\n",
      "MSE = 0.9247141181596855\n",
      "MSE = 0.9245168510269752\n",
      "MSE = 0.9244831296798329\n",
      "MSE = 0.9237535370344827\n",
      "MSE = 0.9243244790134725\n",
      "MSE = 0.924382829472918\n",
      "MSE = 0.9243424792554826\n",
      "MSE = 0.9243882593313227\n",
      "MSE = 0.9243992544356645\n",
      "MSE = 0.924348007822031\n",
      "MSE = 0.9242574667825301\n",
      "MSE = 0.9240286498089381\n",
      "MSE = 0.9238083492809194\n",
      "MSE = 0.9234308222958952\n",
      "MSE = 0.9230816348751085\n",
      "MSE = 0.9230975202451981\n",
      "MSE = 0.9232893187788048\n",
      "MSE = 0.9247731630207708\n",
      "MSE = 0.9232864151740316\n",
      "MSE = 0.9233098575085902\n",
      "MSE = 0.9233426083615702\n",
      "MSE = 0.9233527366333852\n",
      "MSE = 0.923362588004276\n",
      "MSE = 0.9233519952637078\n",
      "MSE = 0.9233571229234224\n",
      "MSE = 0.9233975803121606\n",
      "MSE = 0.9234767604422158\n",
      "MSE = 0.9234092515191324\n",
      "MSE = 0.9233995448843795\n",
      "MSE = 0.923397918101275\n",
      "MSE = 0.9233976386025234\n",
      "MSE = 0.9233975903772853\n",
      "MSE = 0.9233975820503805\n",
      "MSE = 0.923397580612117\n",
      "MSE = 0.9233975803639007\n",
      "MSE = 0.9233975803210747\n",
      "MSE = 0.9233975803136476\n",
      "MSE = 0.9233975803123312\n",
      "MSE = 0.9233975803131973\n",
      "MSE = 0.9233975803124401\n",
      "MSE = 0.9233975803123518\n",
      "MSE = 0.9233975803123312\n",
      "1.0737866337278643\n",
      "Lambda = 1.8e-05\n",
      "MSE = 1.4810926748339204\n",
      "MSE = 2.299778513793813\n",
      "MSE = 1.4737963096431803\n",
      "MSE = 1.4735519323163366\n",
      "MSE = 1.473171509698061\n",
      "MSE = 1.4719152641300866\n",
      "MSE = 1.4689154744196522\n",
      "MSE = 1.4609989337343883\n",
      "MSE = 1.4418110136673234\n",
      "MSE = 1.399009350262573\n",
      "MSE = 1.3240937679489506\n",
      "MSE = 1.2420637876852965\n",
      "MSE = 1.2023890450389791\n",
      "MSE = 1.1972625575805458\n",
      "MSE = 1.1975515274322284\n",
      "MSE = 1.1973879361765707\n",
      "MSE = 1.1946914801949966\n",
      "MSE = 1.1868552400440597\n",
      "MSE = 1.1661807459633977\n",
      "MSE = 1.129419983207906\n",
      "MSE = 1.0893353534925807\n",
      "MSE = 1.0656290189419202\n",
      "MSE = 30.114655659002842\n",
      "MSE = 1.0621520463152283\n",
      "MSE = 1.060441795459785\n",
      "MSE = 1.0582011587977598\n",
      "MSE = 1.0452632531822592\n",
      "MSE = 1.0213927051746792\n",
      "MSE = 0.9939191721544588\n",
      "MSE = 1.0005101203000664\n",
      "MSE = 0.9829411590238434\n",
      "MSE = 0.9820568723028936\n",
      "MSE = 0.9781698845588204\n",
      "MSE = 0.9709249528093935\n",
      "MSE = 0.9641068340003677\n",
      "MSE = 0.9601029438458277\n",
      "MSE = 2.801647029823204\n",
      "MSE = 0.9601208565832577\n",
      "MSE = 0.9573661105941683\n",
      "MSE = 0.950322201698353\n",
      "MSE = 0.9496438711500182\n",
      "MSE = 0.9407629847096601\n",
      "MSE = 0.940916702667364\n",
      "MSE = 0.9412451863612381\n",
      "MSE = 0.9402609422901944\n",
      "MSE = 0.9388896249144801\n",
      "MSE = 0.9367832085147344\n",
      "MSE = 0.9356555841706357\n",
      "MSE = 0.9327596189465854\n",
      "MSE = 0.9311479505871596\n",
      "MSE = 0.932096458871555\n",
      "MSE = 0.9310773921565166\n",
      "MSE = 0.931603837268352\n",
      "MSE = 0.9324329852938\n",
      "MSE = 0.9322940437686379\n",
      "MSE = 0.9316113224959617\n",
      "MSE = 0.9301568276305577\n",
      "MSE = 0.9871618757323677\n",
      "MSE = 0.929871832447946\n",
      "MSE = 0.9286346453371577\n",
      "MSE = 0.9286528500736888\n",
      "MSE = 0.9302317365734634\n",
      "MSE = 0.9286173834163213\n",
      "MSE = 0.9266149137197518\n",
      "MSE = 0.9277703664484198\n",
      "MSE = 0.9284744124378941\n",
      "MSE = 0.9284856784487163\n",
      "MSE = 0.9285570771391872\n",
      "MSE = 0.9284827540746445\n",
      "MSE = 0.9281893995465043\n",
      "MSE = 0.9338004724267218\n",
      "MSE = 0.9286612494040415\n",
      "MSE = 0.9283112036717531\n",
      "MSE = 0.9475419883650102\n",
      "MSE = 0.9283391680233177\n",
      "MSE = 0.9287148800430941\n",
      "MSE = 0.9288148108538804\n",
      "MSE = 0.9280510413980398\n",
      "MSE = 0.9263547181881038\n",
      "MSE = 0.9275004537524473\n",
      "MSE = 0.9270851665951046\n",
      "MSE = 0.9265783215706409\n",
      "MSE = 0.925923576837197\n",
      "MSE = 0.926249069320316\n",
      "MSE = 0.926194369156659\n",
      "MSE = 0.9262166648786484\n",
      "MSE = 0.9263518243873056\n",
      "MSE = 0.926666781384386\n",
      "MSE = 0.9270073421674829\n",
      "MSE = 0.9267513905231232\n",
      "MSE = 0.9272742678183684\n",
      "MSE = 0.9269006401056546\n",
      "MSE = 0.927332931365617\n",
      "MSE = 0.927021048787008\n",
      "MSE = 0.9273450448686816\n",
      "MSE = 0.9271118680739205\n",
      "MSE = 0.9273465904599312\n",
      "MSE = 0.927340641579299\n",
      "MSE = 0.9268742023493053\n",
      "MSE = 0.9266598687526849\n",
      "MSE = 0.9262638812231027\n",
      "MSE = 0.9259581118359311\n",
      "MSE = 0.9259609656972769\n",
      "MSE = 0.9258667638240429\n",
      "MSE = 0.9301102178423019\n",
      "MSE = 0.9262718705319507\n",
      "MSE = 0.9264225280568186\n",
      "MSE = 0.9265365859957533\n",
      "MSE = 0.9263795536919426\n",
      "MSE = 0.9262974833020662\n",
      "MSE = 0.9262881449875069\n",
      "MSE = 0.9262625762061858\n",
      "MSE = 0.9262023951223993\n",
      "MSE = 0.926064358332873\n",
      "MSE = 0.9255832519499334\n",
      "MSE = 0.9257257738905194\n",
      "MSE = 0.9256353357821117\n",
      "MSE = 0.9256712548824297\n",
      "MSE = 0.9256626952174186\n",
      "MSE = 0.9254612569414576\n",
      "MSE = 0.9254513482280575\n",
      "MSE = 0.9253954662363069\n",
      "MSE = 0.92520248675827\n",
      "MSE = 0.9251112270365744\n",
      "MSE = 0.9250919569798317\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.9267501579513642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.9249552569011027\n",
      "MSE = 0.9249101714877459\n",
      "MSE = 0.9249085066484912\n",
      "MSE = 0.9249084444275144\n",
      "MSE = 0.924908442101058\n",
      "MSE = 0.9249084420140499\n",
      "MSE = 0.9249084420108274\n",
      "MSE = 0.9249084420106874\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.924908442010685\n",
      "MSE = 0.9249084420106826\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.9249084420106828\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.9249084420106826\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.9249084420106825\n",
      "MSE = 0.924908542871265\n",
      "MSE = 0.9249086594490874\n",
      "MSE = 0.9249091298540754\n",
      "MSE = 0.9249110769735128\n",
      "MSE = 0.9249390826868168\n",
      "MSE = 0.9249622153199516\n",
      "MSE = 0.9249732867920089\n",
      "MSE = 0.9249745067977744\n",
      "MSE = 0.9249744290110493\n",
      "MSE = 0.9249749609816713\n",
      "MSE = 0.9249780101582573\n",
      "MSE = 0.9249891772413189\n",
      "MSE = 0.9250185087410511\n",
      "MSE = 0.9250786223689966\n",
      "MSE = 0.925161568799013\n",
      "MSE = 0.9250974876908933\n",
      "MSE = 0.9252042137902682\n",
      "MSE = 0.9251154976401953\n",
      "MSE = 0.9251955471478489\n",
      "MSE = 0.9251332075932325\n",
      "MSE = 0.9251928380255553\n",
      "MSE = 0.9251454347146999\n",
      "MSE = 0.9251909256450205\n",
      "MSE = 0.9251535726572686\n",
      "MSE = 0.9251907289237253\n",
      "MSE = 0.9251592962929313\n",
      "MSE = 0.9251546668700522\n",
      "MSE = 0.9251537900016109\n",
      "MSE = 0.9251536161537736\n",
      "MSE = 0.9251535813752259\n",
      "MSE = 0.9251535744050614\n",
      "MSE = 0.9251535730076516\n",
      "MSE = 0.9251535727275539\n",
      "MSE = 0.9251535726713357\n",
      "MSE = 0.9251535727084589\n",
      "MSE = 0.9251535726786206\n",
      "MSE = 0.9251535726727103\n",
      "MSE = 0.9251535726713357\n",
      "1.074256054597343\n",
      "Lambda = 2e-05\n",
      "MSE = 1.480782609599551\n",
      "MSE = 2.3026602297569463\n",
      "MSE = 1.4737936933489413\n",
      "MSE = 1.4735519208248637\n",
      "MSE = 1.4731640261214578\n",
      "MSE = 1.4719032367976044\n",
      "MSE = 1.4688772674607835\n",
      "MSE = 1.4609090930043307\n",
      "MSE = 1.4415861818820448\n",
      "MSE = 1.398525916515037\n",
      "MSE = 1.3232899420764597\n",
      "MSE = 1.241213258995564\n",
      "MSE = 1.2019610156824978\n",
      "MSE = 1.1971766801179082\n",
      "MSE = 1.1975451324582524\n",
      "MSE = 1.1974151834128588\n",
      "MSE = 1.1948462943401592\n",
      "MSE = 1.187143626203621\n",
      "MSE = 1.166548797131064\n",
      "MSE = 1.1298202243517987\n",
      "MSE = 1.0900754246567508\n",
      "MSE = 1.065851256776384\n",
      "MSE = 19.811691530507584\n",
      "MSE = 1.0624308420769686\n",
      "MSE = 1.0605924218506475\n",
      "MSE = 1.0575257807578773\n",
      "MSE = 1.0397348941770654\n",
      "MSE = 1.0135302560789397\n",
      "MSE = 0.9896253343853096\n",
      "MSE = 1.0743871990132967\n",
      "MSE = 0.9744780093438327\n",
      "MSE = 0.9747105044713731\n",
      "MSE = 0.9700364320071698\n",
      "MSE = 0.9635280752873712\n",
      "MSE = 0.9600954192537828\n",
      "MSE = 0.9531711906970576\n",
      "MSE = 0.9501754777236822\n",
      "MSE = 0.95088601061806\n",
      "MSE = 0.9407224870381344\n",
      "MSE = 0.9455233413665062\n",
      "MSE = 0.9499880996425656\n",
      "MSE = 0.9506692720313672\n",
      "MSE = 0.9531554597260574\n",
      "MSE = 0.9495291968747999\n",
      "MSE = 0.9478415884203568\n",
      "MSE = 0.9446627108689971\n",
      "MSE = 0.9415409408678643\n",
      "MSE = 0.9414249896946553\n",
      "MSE = 0.9410042451092625\n",
      "MSE = 0.9411027523517155\n",
      "MSE = 0.9412612594096592\n",
      "MSE = 0.9414749473850823\n",
      "MSE = 0.9415598072734906\n",
      "MSE = 0.9410185559086814\n",
      "MSE = 0.9400019005226885\n",
      "MSE = 0.950756850766013\n",
      "MSE = 0.937925292153498\n",
      "MSE = 0.9362392475476387\n",
      "MSE = 0.9351421448418142\n",
      "MSE = 0.9345938377693424\n",
      "MSE = 0.9435094357297791\n",
      "MSE = 0.9347523624763705\n",
      "MSE = 0.9347019547297027\n",
      "MSE = 0.9332952682069576\n",
      "MSE = 0.9324757135844453\n",
      "MSE = 0.9328310232346769\n",
      "MSE = 0.9319984452517807\n",
      "MSE = 0.9320135451383839\n",
      "MSE = 0.9313159031027947\n",
      "MSE = 1.012210380184477\n",
      "MSE = 0.9318198344520495\n",
      "MSE = 0.931408691201508\n",
      "MSE = 0.9315279267616221\n",
      "MSE = 0.9300940229366682\n",
      "MSE = 0.9308258318879098\n",
      "MSE = 0.9309548656206904\n",
      "MSE = 0.9314738851014824\n",
      "MSE = 0.93762585417876\n",
      "MSE = 0.9312872105432057\n",
      "MSE = 0.9314637522280633\n",
      "MSE = 0.9314829196288407\n",
      "MSE = 0.9315523592158403\n",
      "MSE = 0.9315257058039683\n",
      "MSE = 0.9322100143126073\n",
      "MSE = 0.9317568372318153\n",
      "MSE = 0.9325775339033847\n",
      "MSE = 0.9321809215008424\n",
      "MSE = 0.9310857923624575\n",
      "MSE = 0.9300362586560907\n",
      "MSE = 0.9306913382576198\n",
      "MSE = 0.9306439889242025\n",
      "MSE = 0.9306872959886173\n",
      "MSE = 0.930777646601682\n",
      "MSE = 1.2258714246658524\n",
      "MSE = 0.9307231490463891\n",
      "MSE = 0.9307887580149575\n",
      "MSE = 0.9307902764386434\n",
      "MSE = 0.930730126664204\n",
      "MSE = 0.9306655800130453\n",
      "MSE = 0.930549149298414\n",
      "MSE = 0.9302746512624558\n",
      "MSE = 0.9762493851940954\n",
      "MSE = 0.9304827985229125\n",
      "MSE = 0.9302997809044782\n",
      "MSE = 0.9302781058734769\n",
      "MSE = 0.9302751342742298\n",
      "MSE = 0.9302747189542694\n",
      "MSE = 0.9302746607522405\n",
      "MSE = 0.9302746525929255\n",
      "MSE = 0.9302746514490031\n",
      "MSE = 0.9302746512886017\n",
      "MSE = 0.9302746512661351\n",
      "MSE = 0.9302746512629271\n",
      "MSE = 0.9302746512650606\n",
      "MSE = 0.9302746512631481\n",
      "MSE = 0.9302746512643906\n",
      "MSE = 0.930274651263309\n",
      "MSE = 0.9302746512631767\n",
      "MSE = 0.9302746512631481\n",
      "1.076102050248465\n",
      "Lambda = 5e-05\n",
      "MSE = 1.480975436583329\n",
      "MSE = 2.3008636259568163\n",
      "MSE = 1.4737953273741136\n",
      "MSE = 1.4735519342763534\n",
      "MSE = 1.4731687980415085\n",
      "MSE = 1.4719109832606465\n",
      "MSE = 1.468900962781183\n",
      "MSE = 1.4609556622713\n",
      "MSE = 1.4416384144548213\n",
      "MSE = 1.3982914432049682\n",
      "MSE = 1.3219536881499483\n",
      "MSE = 1.2384398625314763\n",
      "MSE = 1.201053202562287\n",
      "MSE = 1.1990836877917377\n",
      "MSE = 1.2003085347436457\n",
      "MSE = 1.2007459199480397\n",
      "MSE = 1.2001627603022602\n",
      "MSE = 1.1947334621535712\n",
      "MSE = 1.1766797326425413\n",
      "MSE = 1.1416911902945535\n",
      "MSE = 1.1178875608812093\n",
      "MSE = 1.0884805549465206\n",
      "MSE = 1.0786977871208152\n",
      "MSE = 1.0742740683891674\n",
      "MSE = 1.0510748555863154\n",
      "MSE = 1.0346031580118895\n",
      "MSE = 1.015835582992465\n",
      "MSE = 1.0113609950245799\n",
      "MSE = 1.0109817438270101\n",
      "MSE = 1.0161169546432633\n",
      "MSE = 1.012618873626611\n",
      "MSE = 1.0101472064737123\n",
      "MSE = 1.0112333422047117\n",
      "MSE = 1.0102231496239726\n",
      "MSE = 1.008347018723588\n",
      "MSE = 1.0039860652542805\n",
      "MSE = 0.9999469121598262\n",
      "MSE = 0.9952273547581975\n",
      "MSE = 0.9921999320693642\n",
      "MSE = 0.9925531220197422\n",
      "MSE = 0.9982586651398829\n",
      "MSE = 0.9940847631986757\n",
      "MSE = 0.9923263871233331\n",
      "MSE = 0.9923163599740704\n",
      "MSE = 0.9947703312184507\n",
      "MSE = 0.9939565970689924\n",
      "MSE = 0.9938997305334042\n",
      "MSE = 0.9934125187556055\n",
      "MSE = 0.9910210575621071\n",
      "MSE = 0.9902132606670471\n",
      "MSE = 0.9884769950764605\n",
      "MSE = 0.9865292600720069\n",
      "MSE = 0.9861664758951941\n",
      "MSE = 0.9876114068574283\n",
      "MSE = 1.007402798257137\n",
      "MSE = 0.9877536814626554\n",
      "MSE = 0.9889159749596844\n",
      "MSE = 0.9923593423943071\n",
      "MSE = 0.9892370291676542\n",
      "MSE = 0.9889909318383029\n",
      "MSE = 0.9897078239031675\n",
      "MSE = 0.9886959670722332\n",
      "MSE = 0.989196076807751\n",
      "MSE = 0.9890064114607124\n",
      "MSE = 0.9881019604611899\n",
      "MSE = 0.9873170966187735\n",
      "MSE = 0.9870279218575266\n",
      "MSE = 0.9860574897945563\n",
      "MSE = 0.9866720988985086\n",
      "MSE = 0.9862086732033744\n",
      "MSE = 0.9874510313746889\n",
      "MSE = 0.986389574245995\n",
      "MSE = 0.9862403961327059\n",
      "MSE = 0.9862144080989202\n",
      "MSE = 0.9862097156221421\n",
      "MSE = 0.9862088628686151\n",
      "MSE = 0.9862087077185808\n",
      "MSE = 0.9862086794846412\n",
      "MSE = 0.9862086743463776\n",
      "MSE = 0.9862086734114877\n",
      "MSE = 0.9862086732411239\n",
      "MSE = 0.9862086732101618\n",
      "MSE = 0.9862086732044806\n",
      "MSE = 0.9862086732035765\n",
      "MSE = 0.9862086732041726\n",
      "MSE = 0.9862086732036389\n",
      "MSE = 0.9862086732035856\n",
      "MSE = 0.9862086732036193\n",
      "MSE = 0.9862086732035913\n",
      "MSE = 0.9862086732036006\n",
      "MSE = 0.9862086388059201\n",
      "MSE = 0.9862081364386155\n",
      "MSE = 0.9862081799542812\n",
      "MSE = 0.9862084577159919\n",
      "MSE = 0.9862097063374408\n",
      "MSE = 0.9862174129042051\n",
      "MSE = 0.9862346517562868\n",
      "MSE = 0.98627172484501\n",
      "MSE = 0.986314692784542\n",
      "MSE = 0.9863358098659601\n",
      "MSE = 0.9863334558561064\n",
      "MSE = 0.9863297205222723\n",
      "MSE = 0.9863265914045789\n",
      "MSE = 0.9863212378427167\n",
      "MSE = 0.9863197662038489\n",
      "MSE = 0.9863340821150922\n",
      "MSE = 0.9863870054449013\n",
      "MSE = 0.9864797968077259\n",
      "MSE = 0.9865360384153341\n",
      "MSE = 0.9865589051635214\n",
      "MSE = 0.9865751551984833\n",
      "MSE = 0.9871542075122601\n",
      "MSE = 0.9866058437220085\n",
      "MSE = 0.9865801870071526\n",
      "MSE = 0.9865760853614504\n",
      "MSE = 0.9865753308391263\n",
      "MSE = 0.9865751884966998\n",
      "MSE = 0.9865751615158629\n",
      "MSE = 0.9865751563970535\n",
      "MSE = 0.9865751554258191\n",
      "MSE = 0.9865751552414297\n",
      "MSE = 0.9865751552066401\n",
      "MSE = 0.9865751551997303\n",
      "MSE = 0.9865751551986697\n",
      "MSE = 0.9865751551985008\n",
      "MSE = 0.9865751551984846\n",
      "MSE = 0.9865751551984955\n",
      "MSE = 0.986575155198487\n",
      "MSE = 0.9865751551984846\n",
      "MSE = 0.9865751551984862\n",
      "MSE = 0.9865751551984846\n",
      "MSE = 0.9865752012769823\n",
      "MSE = 0.9865752114853221\n",
      "MSE = 0.9865755201057644\n",
      "MSE = 0.986576500910678\n",
      "MSE = 0.9865797143751865\n",
      "MSE = 0.986587235655349\n",
      "MSE = 0.9866007463807037\n",
      "MSE = 0.9866154896033811\n",
      "MSE = 0.9866224495308488\n",
      "MSE = 0.9866217018043837\n",
      "MSE = 0.9866208178554028\n",
      "MSE = 0.9866200292775751\n",
      "MSE = 0.9866209667297883\n",
      "MSE = 0.986628078391819\n",
      "MSE = 0.9866494759450831\n",
      "MSE = 0.9866879427205978\n",
      "MSE = 0.9870204255822513\n",
      "MSE = 0.9866869814911187\n",
      "MSE = 0.9867089284043093\n",
      "MSE = 0.986712075993279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.9867104509815025\n",
      "MSE = 0.9867094291989861\n",
      "MSE = 0.9867081093267677\n",
      "MSE = 0.9867104391852625\n",
      "MSE = 0.9867241918666048\n",
      "MSE = 0.9867605720141834\n",
      "MSE = 0.9868166188258535\n",
      "MSE = 0.9867749470878149\n",
      "MSE = 0.9868569702638745\n",
      "MSE = 0.9867953077462814\n",
      "MSE = 0.9868661281270799\n",
      "MSE = 0.9868102664157358\n",
      "MSE = 0.9868656551361018\n",
      "MSE = 0.9868202978994945\n",
      "MSE = 0.986864887275064\n",
      "MSE = 0.9868272802456884\n",
      "MSE = 0.9868216530546479\n",
      "MSE = 0.9868205712934234\n",
      "MSE = 0.9868203534822335\n",
      "MSE = 0.9868203092176432\n",
      "MSE = 0.9868203002051508\n",
      "MSE = 0.986820298369123\n",
      "MSE = 0.9868202979954149\n",
      "MSE = 0.9868202979190244\n",
      "MSE = 0.986820297969391\n",
      "MSE = 0.986820297929405\n",
      "MSE = 0.9868202979210295\n",
      "MSE = 0.9868202979190244\n",
      "1.105219727805016\n"
     ]
    }
   ],
   "source": [
    "lamb_values = [1e-5,1.3e-5,1.5e-5,1.7e-5,1.8e-5,2e-5,5e-5]\n",
    "MSE_valid =[]\n",
    "best_theta = []\n",
    "best_MSE = 0\n",
    "\n",
    "for lamb in lamb_values:\n",
    "    print(\"Lambda = {}\".format(lamb))\n",
    "  \n",
    "    theta_init = [alpha] + [0.0]*(nUsers+nItems)\n",
    "    unpack(theta_init)\n",
    "    \n",
    "    def cost(theta, labels, lamb):\n",
    "        unpack(theta)\n",
    "        predictions = [prediction(user, book) for user,book in Xtrain]\n",
    "        cost = MSE(predictions, labels)\n",
    "        print(\"MSE = \" + str(cost))\n",
    "        for u in userBiases:\n",
    "            cost += lamb*userBiases[u]**2\n",
    "        for i in itemBiases:\n",
    "            cost += lamb*itemBiases[i]**2\n",
    "        return cost\n",
    "\n",
    "    def derivative(theta, labels, lamb):\n",
    "        unpack(theta)\n",
    "        N = len(dataset)\n",
    "        dalpha = 0\n",
    "        dUserBiases = defaultdict(float)\n",
    "        dItemBiases = defaultdict(float)\n",
    "        for value in zip(Xtrain,ytrain):\n",
    "            x,rating = value\n",
    "            user = x[0]\n",
    "            book = x[1]\n",
    "            pred = prediction(user,book)\n",
    "            diff = pred - rating\n",
    "            dalpha += 2/N*diff\n",
    "            dUserBiases[user] += 2/N*diff\n",
    "            dItemBiases[book] += 2/N*diff\n",
    "        for u in userBiases:\n",
    "            dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "        for i in itemBiases:\n",
    "            dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "        dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "        return numpy.array(dtheta)\n",
    "    \n",
    "    theta,_,_ = scipy.optimize.fmin_l_bfgs_b(cost, theta_init, derivative, args = (ytrain, lamb))\n",
    "\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(user, book) for user,book in Xvalid]\n",
    "    cost = MSE(predictions, yvalid)\n",
    "    \n",
    "    if best_MSE is 0:\n",
    "        best_MSE = cost\n",
    "        best_theta = theta\n",
    "        print(\"Save best theta...\")\n",
    "    else:\n",
    "        if cost < best_MSE:\n",
    "            best_MSE = cost\n",
    "            best_theta = theta\n",
    "            print(\"Save best theta...\")\n",
    "    \n",
    "    MSE_valid.append(cost)\n",
    "    print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamb-1e-05 gives Validation MSE of 1.0741282593688577\n",
      "Lamb-1.3e-05 gives Validation MSE of 1.0735889944967134\n",
      "Lamb-1.5e-05 gives Validation MSE of 1.0734200113625347\n",
      "Lamb-1.7e-05 gives Validation MSE of 1.0737866337278643\n",
      "Lamb-1.8e-05 gives Validation MSE of 1.074256054597343\n",
      "Lamb-2e-05 gives Validation MSE of 1.076102050248465\n",
      "Lamb-5e-05 gives Validation MSE of 1.105219727805016\n"
     ]
    }
   ],
   "source": [
    "for lamb, MSE in zip(lamb_values ,MSE_valid):\n",
    "    print (\"Lamb-{} gives Validation MSE of {}\".format(lamb, MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18528\n",
      "11357\n",
      "3.8115271045716583\n",
      "[-0.55554225 -0.62195259 -3.27765648 ...  0.32498864 -0.19803966\n",
      " -0.00600419]\n",
      "[ 0.36158633  0.64962104 -0.6115337  ...  0.02200522  0.17283829\n",
      " -0.22501942]\n"
     ]
    }
   ],
   "source": [
    "print(len(best_theta))\n",
    "\n",
    "print(best_theta[0])\n",
    "print(best_theta[1:nUsers+1])\n",
    "print(best_theta[nUsers+1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 Kaggle Submission - Lambda=1.2e-5,  MSE=1.143, User_Name='Luke Liem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0aca62c198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.xlabel('Lambda')\n",
    "plt.title('Validation MSE')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.plot(lamb_values, MSE_valid, label='Validation MSE')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.81233938,  0.3113362 ,  0.46095858, ..., -0.3010043 ,\n",
       "       -0.01805242, -0.26187858])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rating baseline: compute averages for each user, or return \n",
    "### the global average if we've never seen the user before\n",
    "\n",
    "predictions = open(\"assignment1/predictions_Rating.txt\", 'w')\n",
    "unpack(best_theta)\n",
    "\n",
    "for l in open(\"assignment1/pairs_Rating.txt\"):\n",
    "  # write header  \n",
    "  if l.startswith(\"userID\"):\n",
    "    predictions.write(l)\n",
    "    continue\n",
    "    \n",
    "  # write user-item-rating\n",
    "  u,b = l.strip().split('-')\n",
    "  predictions.write(u + '-' + b + ',' + str(prediction(u,b)) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Model\n",
    "\n",
    "### Try to replicate Assignment Result for Bias-Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_index(list, id):\n",
    "    return list.index(id)\n",
    "\n",
    "def id_to_index_reformat(X):\n",
    "    x_new = []\n",
    "    \n",
    "    for x in X:\n",
    "        uid = x[0]\n",
    "        bid = x[1]\n",
    "        # print(uid,bid)\n",
    "        \n",
    "        uidx = lookup_index(user_ids,uid)\n",
    "        bidx = lookup_index(book_ids,bid)\n",
    "        # print(uidx,bidx)\n",
    "        \n",
    "        x_new.append([uidx,bidx])\n",
    "    return x_new\n",
    "    \n",
    "\n",
    "def l2_regularize(array):\n",
    "    loss = torch.sum(array ** 2.0)\n",
    "    return loss\n",
    "\n",
    "class MF(nn.Module):\n",
    "    # itr = 0\n",
    "    \n",
    "    def __init__(self, n_user, n_item, k=1, c_vector=1.0, c_bias=1.0, writer=None, mean=0):\n",
    "        super(MF, self).__init__()\n",
    "        self.writer = writer\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.c_bias = c_bias\n",
    "        self.c_vector = c_vector\n",
    "        \n",
    "        # gammas (users and items)\n",
    "        # self.user = nn.Embedding(n_user, k)\n",
    "        # self.item = nn.Embedding(n_item, k)\n",
    "        \n",
    "        # alpha and betas (users and items)\n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        # Initialize\n",
    "        self.bias.data.fill_(mean)\n",
    "        self.bias_user.weight.data.fill_(0)\n",
    "        self.bias_item.weight.data.fill_(0)\n",
    "    \n",
    "    def forward(self, train_x):\n",
    "        user_id = train_x[:, 0]\n",
    "        item_id = train_x[:, 1]\n",
    "        # vector_user = self.user(user_id)\n",
    "        # vector_item = self.item(item_id)\n",
    "        \n",
    "        # Pull out biases\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        biases = (self.bias + bias_user + bias_item)\n",
    "        \n",
    "        # ui_interaction = torch.sum(vector_user * vector_item, dim=1)\n",
    "        \n",
    "        # Add bias prediction to the interaction prediction\n",
    "        # prediction = ui_interaction + biases\n",
    "        \n",
    "        prediction = biases\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "        \n",
    "        # BUG - PyTorch optimizer already takes care of regularization!!!\n",
    "        \n",
    "        # prior_bias_user =  l2_regularize(self.bias_user.weight) * self.c_bias\n",
    "        # prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias\n",
    "        # prior_user =  l2_regularize(self.user.weight) * self.c_vector\n",
    "        # prior_item = l2_regularize(self.item.weight) * self.c_vector\n",
    "        # total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item\n",
    "        \n",
    "        total = loss_mse\n",
    "        \n",
    "        # for name, var in locals().items():\n",
    "        #    if type(var) is torch.Tensor and var.nelement() == 1 and self.writer is not None:\n",
    "        #        self.writer.add_scalar(name, var, self.itr)\n",
    "        \n",
    "        return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 3 columns):\n",
      "userID    200000 non-null object\n",
      "bookID    200000 non-null object\n",
      "rating    200000 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 4.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>bookID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u79354815</td>\n",
       "      <td>b14275065</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u56917948</td>\n",
       "      <td>b82152306</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u97915914</td>\n",
       "      <td>b44882292</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u49688858</td>\n",
       "      <td>b79927466</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u08384938</td>\n",
       "      <td>b05683889</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userID     bookID  rating\n",
       "0  u79354815  b14275065       4\n",
       "1  u56917948  b82152306       5\n",
       "2  u97915914  b44882292       5\n",
       "3  u49688858  b79927466       5\n",
       "4  u08384938  b05683889       2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../datasets/cse258/assignment1/train_Interactions.csv\")\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7170 ['b14275065', 'b82152306', 'b44882292']\n",
      "11357 ['u79354815', 'u56917948', 'u97915914']\n",
      "b52453648\n",
      "203\n",
      "u67309666\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "book_ids = list(data['bookID'].unique())\n",
    "user_ids = list(data['userID'].unique())\n",
    "\n",
    "print(len(book_ids), book_ids[:3])\n",
    "print(len(user_ids), user_ids[:3])\n",
    "\n",
    "print(book_ids[203])\n",
    "print(lookup_index(book_ids,'b52453648'))\n",
    "\n",
    "print(user_ids[108])\n",
    "print(lookup_index(user_ids,'u67309666'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['u76247191', 'b82374557'], ['u12510042', 'b47677202'], ['u87065149', 'b15322881'], ['u56882043', 'b21517939'], ['u50024754', 'b25719319']] [[237, 857], [477, 2686], [4246, 772], [3831, 1083], [7302, 7005]]\n",
      "[['u40726318', 'b25220721'], ['u00550479', 'b32614969'], ['u75580304', 'b88667874'], ['u29859759', 'b34184750'], ['u25261335', 'b98923466']] [[8095, 3119], [10009, 6560], [1776, 169], [10098, 3385], [5708, 1503]]\n"
     ]
    }
   ],
   "source": [
    "train_x = id_to_index_reformat(Xtrain)\n",
    "valid_x = id_to_index_reformat(Xvalid)\n",
    "\n",
    "train_y = ytrain\n",
    "valid_y = yvalid\n",
    "\n",
    "print (Xtrain[10:15], train_x[10:15])\n",
    "print (Xvalid[10:15], valid_x[10:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38604 2766 512 u27349804 b68268306\n",
      "['u27349804', 'b68268306']\n",
      "8084 7891 5971 u98796150 b65760924\n",
      "['u98796150', 'b65760924']\n",
      "60252 5250 2065 u88978112 b13620341\n",
      "['u88978112', 'b13620341']\n",
      "61807 3872 2206 u82123673 b94925721\n",
      "['u82123673', 'b94925721']\n",
      "89854 1471 1072 u59808881 b84299066\n",
      "['u59808881', 'b84299066']\n"
     ]
    }
   ],
   "source": [
    "# Basic QC\n",
    "\n",
    "for i in range(5):\n",
    "    index = random.randint(0, len(train_x))\n",
    "    uid = train_x[index][0]\n",
    "    bid = train_x[index][1]\n",
    "    print(index, uid, bid, user_ids[uid],book_ids[bid])\n",
    "    print(Xtrain[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Iteration[0] Training Loss: 1.457\n",
      "Epoch[0] Validation Loss: 1.477 \n",
      "Epoch[0] Iteration[100] Training Loss: 1.318\n",
      "Epoch[0] Validation Loss: 1.475 \n",
      "Epoch[10] Iteration[0] Training Loss: 1.230\n",
      "Epoch[10] Validation Loss: 1.435 \n",
      "Epoch[10] Iteration[100] Training Loss: 1.515\n",
      "Epoch[10] Validation Loss: 1.433 \n",
      "Epoch[20] Iteration[0] Training Loss: 1.458\n",
      "Epoch[20] Validation Loss: 1.401 \n",
      "Epoch[20] Iteration[100] Training Loss: 1.395\n",
      "Epoch[20] Validation Loss: 1.399 \n",
      "Epoch[30] Iteration[0] Training Loss: 1.346\n",
      "Epoch[30] Validation Loss: 1.372 \n",
      "Epoch[30] Iteration[100] Training Loss: 1.373\n",
      "Epoch[30] Validation Loss: 1.370 \n",
      "Epoch[40] Iteration[0] Training Loss: 1.308\n",
      "Epoch[40] Validation Loss: 1.347 \n",
      "Epoch[40] Iteration[100] Training Loss: 1.299\n",
      "Epoch[40] Validation Loss: 1.346 \n",
      "Epoch[50] Iteration[0] Training Loss: 1.284\n",
      "Epoch[50] Validation Loss: 1.325 \n",
      "Epoch[50] Iteration[100] Training Loss: 1.191\n",
      "Epoch[50] Validation Loss: 1.324 \n",
      "Epoch[60] Iteration[0] Training Loss: 1.252\n",
      "Epoch[60] Validation Loss: 1.306 \n",
      "Epoch[60] Iteration[100] Training Loss: 1.346\n",
      "Epoch[60] Validation Loss: 1.305 \n",
      "Epoch[70] Iteration[0] Training Loss: 1.257\n",
      "Epoch[70] Validation Loss: 1.289 \n",
      "Epoch[70] Iteration[100] Training Loss: 1.209\n",
      "Epoch[70] Validation Loss: 1.288 \n",
      "Epoch[80] Iteration[0] Training Loss: 1.208\n",
      "Epoch[80] Validation Loss: 1.274 \n",
      "Epoch[80] Iteration[100] Training Loss: 1.094\n",
      "Epoch[80] Validation Loss: 1.273 \n",
      "Epoch[90] Iteration[0] Training Loss: 1.329\n",
      "Epoch[90] Validation Loss: 1.260 \n",
      "Epoch[90] Iteration[100] Training Loss: 1.252\n",
      "Epoch[90] Validation Loss: 1.260 \n",
      "Epoch[100] Iteration[0] Training Loss: 1.074\n",
      "Epoch[100] Validation Loss: 1.248 \n",
      "Epoch[100] Iteration[100] Training Loss: 1.097\n",
      "Epoch[100] Validation Loss: 1.248 \n",
      "Epoch[110] Iteration[0] Training Loss: 1.173\n",
      "Epoch[110] Validation Loss: 1.237 \n",
      "Epoch[110] Iteration[100] Training Loss: 1.135\n",
      "Epoch[110] Validation Loss: 1.236 \n",
      "Epoch[120] Iteration[0] Training Loss: 1.081\n",
      "Epoch[120] Validation Loss: 1.227 \n",
      "Epoch[120] Iteration[100] Training Loss: 1.164\n",
      "Epoch[120] Validation Loss: 1.226 \n",
      "Epoch[130] Iteration[0] Training Loss: 1.115\n",
      "Epoch[130] Validation Loss: 1.218 \n",
      "Epoch[130] Iteration[100] Training Loss: 1.026\n",
      "Epoch[130] Validation Loss: 1.218 \n",
      "Epoch[140] Iteration[0] Training Loss: 0.931\n",
      "Epoch[140] Validation Loss: 1.209 \n",
      "Epoch[140] Iteration[100] Training Loss: 1.206\n",
      "Epoch[140] Validation Loss: 1.209 \n",
      "Epoch[150] Iteration[0] Training Loss: 1.018\n",
      "Epoch[150] Validation Loss: 1.202 \n",
      "Epoch[150] Iteration[100] Training Loss: 1.120\n",
      "Epoch[150] Validation Loss: 1.201 \n",
      "Epoch[160] Iteration[0] Training Loss: 1.211\n",
      "Epoch[160] Validation Loss: 1.195 \n",
      "Epoch[160] Iteration[100] Training Loss: 0.984\n",
      "Epoch[160] Validation Loss: 1.194 \n",
      "Epoch[170] Iteration[0] Training Loss: 1.179\n",
      "Epoch[170] Validation Loss: 1.188 \n",
      "Epoch[170] Iteration[100] Training Loss: 1.082\n",
      "Epoch[170] Validation Loss: 1.188 \n",
      "Epoch[180] Iteration[0] Training Loss: 1.037\n",
      "Epoch[180] Validation Loss: 1.182 \n",
      "Epoch[180] Iteration[100] Training Loss: 0.978\n",
      "Epoch[180] Validation Loss: 1.182 \n",
      "Epoch[190] Iteration[0] Training Loss: 1.016\n",
      "Epoch[190] Validation Loss: 1.177 \n",
      "Epoch[190] Iteration[100] Training Loss: 1.188\n",
      "Epoch[190] Validation Loss: 1.176 \n",
      "Epoch[200] Iteration[0] Training Loss: 0.975\n",
      "Epoch[200] Validation Loss: 1.171 \n",
      "Epoch[200] Iteration[100] Training Loss: 1.074\n",
      "Epoch[200] Validation Loss: 1.171 \n",
      "Epoch[210] Iteration[0] Training Loss: 1.015\n",
      "Epoch[210] Validation Loss: 1.167 \n",
      "Epoch[210] Iteration[100] Training Loss: 1.010\n",
      "Epoch[210] Validation Loss: 1.167 \n",
      "Epoch[220] Iteration[0] Training Loss: 0.991\n",
      "Epoch[220] Validation Loss: 1.162 \n",
      "Epoch[220] Iteration[100] Training Loss: 1.010\n",
      "Epoch[220] Validation Loss: 1.162 \n",
      "Epoch[230] Iteration[0] Training Loss: 1.114\n",
      "Epoch[230] Validation Loss: 1.159 \n",
      "Epoch[230] Iteration[100] Training Loss: 1.003\n",
      "Epoch[230] Validation Loss: 1.158 \n",
      "Epoch[240] Iteration[0] Training Loss: 0.979\n",
      "Epoch[240] Validation Loss: 1.155 \n",
      "Epoch[240] Iteration[100] Training Loss: 1.126\n",
      "Epoch[240] Validation Loss: 1.155 \n",
      "Epoch[250] Iteration[0] Training Loss: 1.035\n",
      "Epoch[250] Validation Loss: 1.152 \n",
      "Epoch[250] Iteration[100] Training Loss: 1.079\n",
      "Epoch[250] Validation Loss: 1.151 \n",
      "Epoch[260] Iteration[0] Training Loss: 1.048\n",
      "Epoch[260] Validation Loss: 1.148 \n",
      "Epoch[260] Iteration[100] Training Loss: 1.009\n",
      "Epoch[260] Validation Loss: 1.148 \n",
      "Epoch[270] Iteration[0] Training Loss: 0.935\n",
      "Epoch[270] Validation Loss: 1.146 \n",
      "Epoch[270] Iteration[100] Training Loss: 1.261\n",
      "Epoch[270] Validation Loss: 1.145 \n",
      "Epoch[280] Iteration[0] Training Loss: 0.961\n",
      "Epoch[280] Validation Loss: 1.143 \n",
      "Epoch[280] Iteration[100] Training Loss: 0.907\n",
      "Epoch[280] Validation Loss: 1.142 \n",
      "Epoch[290] Iteration[0] Training Loss: 1.031\n",
      "Epoch[290] Validation Loss: 1.140 \n",
      "Epoch[290] Iteration[100] Training Loss: 1.014\n",
      "Epoch[290] Validation Loss: 1.140 \n",
      "Epoch[300] Iteration[0] Training Loss: 1.005\n",
      "Epoch[300] Validation Loss: 1.138 \n",
      "Epoch[300] Iteration[100] Training Loss: 0.983\n",
      "Epoch[300] Validation Loss: 1.138 \n",
      "Epoch[310] Iteration[0] Training Loss: 0.896\n",
      "Epoch[310] Validation Loss: 1.137 \n",
      "Epoch[310] Iteration[100] Training Loss: 1.096\n",
      "Epoch[310] Validation Loss: 1.135 \n",
      "Epoch[320] Iteration[0] Training Loss: 0.879\n",
      "Epoch[320] Validation Loss: 1.134 \n",
      "Epoch[320] Iteration[100] Training Loss: 1.090\n",
      "Epoch[320] Validation Loss: 1.134 \n",
      "Epoch[330] Iteration[0] Training Loss: 0.860\n",
      "Epoch[330] Validation Loss: 1.132 \n",
      "Epoch[330] Iteration[100] Training Loss: 1.090\n",
      "Epoch[330] Validation Loss: 1.132 \n",
      "Epoch[340] Iteration[0] Training Loss: 1.047\n",
      "Epoch[340] Validation Loss: 1.130 \n",
      "Epoch[340] Iteration[100] Training Loss: 1.037\n",
      "Epoch[340] Validation Loss: 1.130 \n",
      "Epoch[350] Iteration[0] Training Loss: 0.968\n",
      "Epoch[350] Validation Loss: 1.128 \n",
      "Epoch[350] Iteration[100] Training Loss: 0.907\n",
      "Epoch[350] Validation Loss: 1.128 \n",
      "Epoch[360] Iteration[0] Training Loss: 1.039\n",
      "Epoch[360] Validation Loss: 1.127 \n",
      "Epoch[360] Iteration[100] Training Loss: 0.997\n",
      "Epoch[360] Validation Loss: 1.127 \n",
      "Epoch[370] Iteration[0] Training Loss: 0.923\n",
      "Epoch[370] Validation Loss: 1.125 \n",
      "Epoch[370] Iteration[100] Training Loss: 0.994\n",
      "Epoch[370] Validation Loss: 1.125 \n",
      "Epoch[380] Iteration[0] Training Loss: 0.984\n",
      "Epoch[380] Validation Loss: 1.124 \n",
      "Epoch[380] Iteration[100] Training Loss: 1.047\n",
      "Epoch[380] Validation Loss: 1.124 \n",
      "Epoch[390] Iteration[0] Training Loss: 0.909\n",
      "Epoch[390] Validation Loss: 1.123 \n",
      "Epoch[390] Iteration[100] Training Loss: 0.858\n",
      "Epoch[390] Validation Loss: 1.123 \n",
      "Epoch[400] Iteration[0] Training Loss: 0.877\n",
      "Epoch[400] Validation Loss: 1.122 \n",
      "Epoch[400] Iteration[100] Training Loss: 0.992\n",
      "Epoch[400] Validation Loss: 1.122 \n",
      "Epoch[410] Iteration[0] Training Loss: 0.982\n",
      "Epoch[410] Validation Loss: 1.121 \n",
      "Epoch[410] Iteration[100] Training Loss: 0.900\n",
      "Epoch[410] Validation Loss: 1.121 \n",
      "Epoch[420] Iteration[0] Training Loss: 0.908\n",
      "Epoch[420] Validation Loss: 1.120 \n",
      "Epoch[420] Iteration[100] Training Loss: 0.914\n",
      "Epoch[420] Validation Loss: 1.120 \n",
      "Epoch[430] Iteration[0] Training Loss: 0.973\n",
      "Epoch[430] Validation Loss: 1.119 \n",
      "Epoch[430] Iteration[100] Training Loss: 0.952\n",
      "Epoch[430] Validation Loss: 1.119 \n",
      "Epoch[440] Iteration[0] Training Loss: 0.875\n",
      "Epoch[440] Validation Loss: 1.118 \n",
      "Epoch[440] Iteration[100] Training Loss: 0.890\n",
      "Epoch[440] Validation Loss: 1.118 \n",
      "Epoch[450] Iteration[0] Training Loss: 0.958\n",
      "Epoch[450] Validation Loss: 1.117 \n",
      "Epoch[450] Iteration[100] Training Loss: 1.021\n",
      "Epoch[450] Validation Loss: 1.117 \n",
      "Epoch[460] Iteration[0] Training Loss: 0.991\n",
      "Epoch[460] Validation Loss: 1.116 \n",
      "Epoch[460] Iteration[100] Training Loss: 0.937\n",
      "Epoch[460] Validation Loss: 1.116 \n",
      "Epoch[470] Iteration[0] Training Loss: 0.977\n",
      "Epoch[470] Validation Loss: 1.116 \n",
      "Epoch[470] Iteration[100] Training Loss: 1.028\n",
      "Epoch[470] Validation Loss: 1.116 \n",
      "Epoch[480] Iteration[0] Training Loss: 0.881\n",
      "Epoch[480] Validation Loss: 1.115 \n",
      "Epoch[480] Iteration[100] Training Loss: 0.990\n",
      "Epoch[480] Validation Loss: 1.115 \n",
      "Epoch[490] Iteration[0] Training Loss: 0.950\n",
      "Epoch[490] Validation Loss: 1.114 \n",
      "Epoch[490] Iteration[100] Training Loss: 0.925\n",
      "Epoch[490] Validation Loss: 1.115 \n",
      "Epoch[500] Iteration[0] Training Loss: 0.921\n",
      "Epoch[500] Validation Loss: 1.114 \n",
      "Epoch[500] Iteration[100] Training Loss: 0.880\n",
      "Epoch[500] Validation Loss: 1.114 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[510] Iteration[0] Training Loss: 0.985\n",
      "Epoch[510] Validation Loss: 1.113 \n",
      "Epoch[510] Iteration[100] Training Loss: 0.947\n",
      "Epoch[510] Validation Loss: 1.113 \n",
      "Epoch[520] Iteration[0] Training Loss: 0.971\n",
      "Epoch[520] Validation Loss: 1.113 \n",
      "Epoch[520] Iteration[100] Training Loss: 0.988\n",
      "Epoch[520] Validation Loss: 1.113 \n",
      "Epoch[530] Iteration[0] Training Loss: 1.038\n",
      "Epoch[530] Validation Loss: 1.113 \n",
      "Epoch[530] Iteration[100] Training Loss: 0.996\n",
      "Epoch[530] Validation Loss: 1.112 \n",
      "Epoch[540] Iteration[0] Training Loss: 0.942\n",
      "Epoch[540] Validation Loss: 1.112 \n",
      "Epoch[540] Iteration[100] Training Loss: 0.967\n",
      "Epoch[540] Validation Loss: 1.112 \n",
      "Epoch[550] Iteration[0] Training Loss: 0.905\n",
      "Epoch[550] Validation Loss: 1.112 \n",
      "Epoch[550] Iteration[100] Training Loss: 0.952\n",
      "Epoch[550] Validation Loss: 1.112 \n",
      "Epoch[560] Iteration[0] Training Loss: 0.923\n",
      "Epoch[560] Validation Loss: 1.112 \n",
      "Epoch[560] Iteration[100] Training Loss: 0.841\n",
      "Epoch[560] Validation Loss: 1.111 \n",
      "Epoch[570] Iteration[0] Training Loss: 0.929\n",
      "Epoch[570] Validation Loss: 1.111 \n",
      "Epoch[570] Iteration[100] Training Loss: 0.922\n",
      "Epoch[570] Validation Loss: 1.111 \n",
      "Epoch[580] Iteration[0] Training Loss: 0.907\n",
      "Epoch[580] Validation Loss: 1.111 \n",
      "Epoch[580] Iteration[100] Training Loss: 0.903\n",
      "Epoch[580] Validation Loss: 1.111 \n",
      "Epoch[590] Iteration[0] Training Loss: 0.870\n",
      "Epoch[590] Validation Loss: 1.110 \n",
      "Epoch[590] Iteration[100] Training Loss: 0.882\n",
      "Epoch[590] Validation Loss: 1.111 \n",
      "Epoch[600] Iteration[0] Training Loss: 0.952\n",
      "Epoch[600] Validation Loss: 1.110 \n",
      "Epoch[600] Iteration[100] Training Loss: 1.004\n",
      "Epoch[600] Validation Loss: 1.110 \n",
      "Epoch[610] Iteration[0] Training Loss: 0.878\n",
      "Epoch[610] Validation Loss: 1.110 \n",
      "Epoch[610] Iteration[100] Training Loss: 0.962\n",
      "Epoch[610] Validation Loss: 1.111 \n",
      "Epoch[620] Iteration[0] Training Loss: 0.955\n",
      "Epoch[620] Validation Loss: 1.110 \n",
      "Epoch[620] Iteration[100] Training Loss: 0.875\n",
      "Epoch[620] Validation Loss: 1.110 \n",
      "Epoch[630] Iteration[0] Training Loss: 0.912\n",
      "Epoch[630] Validation Loss: 1.109 \n",
      "Epoch[630] Iteration[100] Training Loss: 0.861\n",
      "Epoch[630] Validation Loss: 1.109 \n",
      "Epoch[640] Iteration[0] Training Loss: 0.906\n",
      "Epoch[640] Validation Loss: 1.109 \n",
      "Epoch[640] Iteration[100] Training Loss: 0.875\n",
      "Epoch[640] Validation Loss: 1.110 \n",
      "Epoch[650] Iteration[0] Training Loss: 0.999\n",
      "Epoch[650] Validation Loss: 1.109 \n",
      "Epoch[650] Iteration[100] Training Loss: 0.970\n",
      "Epoch[650] Validation Loss: 1.109 \n",
      "Epoch[660] Iteration[0] Training Loss: 0.986\n",
      "Epoch[660] Validation Loss: 1.109 \n",
      "Epoch[660] Iteration[100] Training Loss: 1.019\n",
      "Epoch[660] Validation Loss: 1.109 \n",
      "Epoch[670] Iteration[0] Training Loss: 0.934\n",
      "Epoch[670] Validation Loss: 1.109 \n",
      "Epoch[670] Iteration[100] Training Loss: 0.891\n",
      "Epoch[670] Validation Loss: 1.109 \n",
      "Epoch[680] Iteration[0] Training Loss: 0.933\n",
      "Epoch[680] Validation Loss: 1.109 \n",
      "Epoch[680] Iteration[100] Training Loss: 0.928\n",
      "Epoch[680] Validation Loss: 1.109 \n",
      "Epoch[690] Iteration[0] Training Loss: 1.019\n",
      "Epoch[690] Validation Loss: 1.109 \n",
      "Epoch[690] Iteration[100] Training Loss: 0.908\n",
      "Epoch[690] Validation Loss: 1.109 \n",
      "Epoch[700] Iteration[0] Training Loss: 0.867\n",
      "Epoch[700] Validation Loss: 1.109 \n",
      "Epoch[700] Iteration[100] Training Loss: 0.966\n",
      "Epoch[700] Validation Loss: 1.108 \n",
      "Epoch[710] Iteration[0] Training Loss: 0.877\n",
      "Epoch[710] Validation Loss: 1.108 \n",
      "Epoch[710] Iteration[100] Training Loss: 0.920\n",
      "Epoch[710] Validation Loss: 1.108 \n",
      "Epoch[720] Iteration[0] Training Loss: 0.941\n",
      "Epoch[720] Validation Loss: 1.109 \n",
      "Epoch[720] Iteration[100] Training Loss: 0.968\n",
      "Epoch[720] Validation Loss: 1.108 \n",
      "Epoch[730] Iteration[0] Training Loss: 1.037\n",
      "Epoch[730] Validation Loss: 1.108 \n",
      "Epoch[730] Iteration[100] Training Loss: 0.924\n",
      "Epoch[730] Validation Loss: 1.108 \n",
      "Epoch[740] Iteration[0] Training Loss: 0.988\n",
      "Epoch[740] Validation Loss: 1.108 \n",
      "Epoch[740] Iteration[100] Training Loss: 1.026\n",
      "Epoch[740] Validation Loss: 1.108 \n",
      "Epoch[750] Iteration[0] Training Loss: 0.927\n",
      "Epoch[750] Validation Loss: 1.108 \n",
      "Epoch[750] Iteration[100] Training Loss: 0.941\n",
      "Epoch[750] Validation Loss: 1.108 \n",
      "Epoch[760] Iteration[0] Training Loss: 0.933\n",
      "Epoch[760] Validation Loss: 1.108 \n",
      "Epoch[760] Iteration[100] Training Loss: 0.966\n",
      "Epoch[760] Validation Loss: 1.108 \n",
      "Epoch[770] Iteration[0] Training Loss: 0.922\n",
      "Epoch[770] Validation Loss: 1.108 \n",
      "Epoch[770] Iteration[100] Training Loss: 0.824\n",
      "Epoch[770] Validation Loss: 1.108 \n",
      "Epoch[780] Iteration[0] Training Loss: 0.870\n",
      "Epoch[780] Validation Loss: 1.108 \n",
      "Epoch[780] Iteration[100] Training Loss: 0.857\n",
      "Epoch[780] Validation Loss: 1.108 \n",
      "Epoch[790] Iteration[0] Training Loss: 0.840\n",
      "Epoch[790] Validation Loss: 1.108 \n",
      "Epoch[790] Iteration[100] Training Loss: 0.857\n",
      "Epoch[790] Validation Loss: 1.108 \n",
      "Epoch[800] Iteration[0] Training Loss: 0.925\n",
      "Epoch[800] Validation Loss: 1.108 \n",
      "Epoch[800] Iteration[100] Training Loss: 0.931\n",
      "Epoch[800] Validation Loss: 1.108 \n",
      "Epoch[810] Iteration[0] Training Loss: 0.941\n",
      "Epoch[810] Validation Loss: 1.108 \n",
      "Epoch[810] Iteration[100] Training Loss: 0.897\n",
      "Epoch[810] Validation Loss: 1.108 \n",
      "Epoch[820] Iteration[0] Training Loss: 0.918\n",
      "Epoch[820] Validation Loss: 1.108 \n",
      "Epoch[820] Iteration[100] Training Loss: 0.886\n",
      "Epoch[820] Validation Loss: 1.108 \n",
      "Epoch[830] Iteration[0] Training Loss: 0.932\n",
      "Epoch[830] Validation Loss: 1.108 \n",
      "Epoch[830] Iteration[100] Training Loss: 0.930\n",
      "Epoch[830] Validation Loss: 1.108 \n",
      "Epoch[840] Iteration[0] Training Loss: 0.947\n",
      "Epoch[840] Validation Loss: 1.108 \n",
      "Epoch[840] Iteration[100] Training Loss: 0.887\n",
      "Epoch[840] Validation Loss: 1.108 \n",
      "Epoch[850] Iteration[0] Training Loss: 0.980\n",
      "Epoch[850] Validation Loss: 1.108 \n",
      "Epoch[850] Iteration[100] Training Loss: 0.933\n",
      "Epoch[850] Validation Loss: 1.108 \n",
      "Epoch[860] Iteration[0] Training Loss: 0.899\n",
      "Epoch[860] Validation Loss: 1.108 \n",
      "Epoch[860] Iteration[100] Training Loss: 0.966\n",
      "Epoch[860] Validation Loss: 1.108 \n",
      "Epoch[870] Iteration[0] Training Loss: 0.937\n",
      "Epoch[870] Validation Loss: 1.109 \n",
      "Epoch[870] Iteration[100] Training Loss: 0.867\n",
      "Epoch[870] Validation Loss: 1.108 \n",
      "Epoch[880] Iteration[0] Training Loss: 0.878\n",
      "Epoch[880] Validation Loss: 1.108 \n",
      "Epoch[880] Iteration[100] Training Loss: 0.875\n",
      "Epoch[880] Validation Loss: 1.108 \n",
      "Epoch[890] Iteration[0] Training Loss: 0.951\n",
      "Epoch[890] Validation Loss: 1.108 \n",
      "Epoch[890] Iteration[100] Training Loss: 0.938\n",
      "Epoch[890] Validation Loss: 1.108 \n",
      "Epoch[900] Iteration[0] Training Loss: 0.902\n",
      "Epoch[900] Validation Loss: 1.108 \n",
      "Epoch[900] Iteration[100] Training Loss: 0.927\n",
      "Epoch[900] Validation Loss: 1.108 \n",
      "Epoch[910] Iteration[0] Training Loss: 0.901\n",
      "Epoch[910] Validation Loss: 1.108 \n",
      "Epoch[910] Iteration[100] Training Loss: 1.002\n",
      "Epoch[910] Validation Loss: 1.108 \n",
      "Epoch[920] Iteration[0] Training Loss: 1.002\n",
      "Epoch[920] Validation Loss: 1.108 \n",
      "Epoch[920] Iteration[100] Training Loss: 0.961\n",
      "Epoch[920] Validation Loss: 1.108 \n",
      "Epoch[930] Iteration[0] Training Loss: 1.030\n",
      "Epoch[930] Validation Loss: 1.108 \n",
      "Epoch[930] Iteration[100] Training Loss: 0.955\n",
      "Epoch[930] Validation Loss: 1.108 \n",
      "Epoch[940] Iteration[0] Training Loss: 0.877\n",
      "Epoch[940] Validation Loss: 1.108 \n",
      "Epoch[940] Iteration[100] Training Loss: 0.989\n",
      "Epoch[940] Validation Loss: 1.108 \n",
      "Epoch[950] Iteration[0] Training Loss: 0.809\n",
      "Epoch[950] Validation Loss: 1.108 \n",
      "Epoch[950] Iteration[100] Training Loss: 0.860\n",
      "Epoch[950] Validation Loss: 1.108 \n",
      "Epoch[960] Iteration[0] Training Loss: 0.887\n",
      "Epoch[960] Validation Loss: 1.108 \n",
      "Epoch[960] Iteration[100] Training Loss: 0.928\n",
      "Epoch[960] Validation Loss: 1.109 \n",
      "Epoch[970] Iteration[0] Training Loss: 0.853\n",
      "Epoch[970] Validation Loss: 1.108 \n",
      "Epoch[970] Iteration[100] Training Loss: 1.060\n",
      "Epoch[970] Validation Loss: 1.108 \n",
      "Epoch[980] Iteration[0] Training Loss: 0.859\n",
      "Epoch[980] Validation Loss: 1.108 \n",
      "Epoch[980] Iteration[100] Training Loss: 0.900\n",
      "Epoch[980] Validation Loss: 1.109 \n",
      "Epoch[990] Iteration[0] Training Loss: 1.088\n",
      "Epoch[990] Validation Loss: 1.108 \n",
      "Epoch[990] Iteration[100] Training Loss: 0.919\n",
      "Epoch[990] Validation Loss: 1.108 \n",
      "Epoch[1000] Iteration[0] Training Loss: 0.745\n",
      "Epoch[1000] Validation Loss: 1.109 \n",
      "Epoch[1000] Iteration[100] Training Loss: 0.934\n",
      "Epoch[1000] Validation Loss: 1.109 \n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 1e-2\n",
    "\n",
    "# New parameter for regularizing bias\n",
    "lamb = 1e-5\n",
    "batch_size = 1024\n",
    "\n",
    "n_user = len(ratingsPerUser)\n",
    "n_item = len(ratingsPerItem)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "model = MF(n_user, n_item, mean=ratingMean)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=lamb, momentum=0.9)\n",
    "\n",
    "def chunks(X, Y, size):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    starts = list(range(0, len(X), size))\n",
    "    shuffle(starts)\n",
    "    for i in starts:\n",
    "        yield (X[i:i + size], Y[i:i + size])\n",
    "        \n",
    "losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(1000+1):\n",
    "    \n",
    "    i = 0\n",
    "    for feature, target in chunks(np.array(train_x), np.array(train_y), batch_size):\n",
    "        # This zeros the gradients on every parameter. \n",
    "        # This is easy to miss and hard to troubleshoot.\n",
    "        optimizer.zero_grad()\n",
    "        # Convert \n",
    "        feature = Variable(torch.from_numpy(feature))\n",
    "        target = Variable(torch.from_numpy(target).type(torch.FloatTensor))\n",
    "        \n",
    "        if cuda:\n",
    "            feature = feature.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "        # model in training mode    \n",
    "        model.train()\n",
    "            \n",
    "        # Compute a prediction for these features\n",
    "        prediction = model.forward(feature)\n",
    "        # Compute a loss given what the true target outcome was\n",
    "        loss = model.loss(prediction, target)\n",
    "        # break\n",
    "        # Backpropagate: compute the direction / gradient every model parameter\n",
    "        # defined in your __init__ should move in in order to minimize this loss\n",
    "        # However, we're not actually changing these parameters, we're just storing\n",
    "        # how they should change.\n",
    "\n",
    "        loss.backward()\n",
    "        # Now take a step & update the model parameters. The optimizer uses the gradient at \n",
    "        # defined on every parameter in our model and nudges it in that direction.\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%100 == 0 and epoch%10 == 0:\n",
    "            print(\"Epoch[{}] Iteration[{}] Training Loss: {:.3f}\".format(epoch, i, loss.data))\n",
    "\n",
    "        # Record the loss per example\n",
    "        losses.append(loss.cpu().data.numpy() / len(feature))\n",
    "        \n",
    "        if i%100 == 0 and epoch%10 == 0:\n",
    "            val_feature = torch.from_numpy(np.array(valid_x))\n",
    "            val_target = torch.from_numpy(np.array(valid_y)).type(torch.FloatTensor)\n",
    "            \n",
    "            if cuda:\n",
    "                val_feature = val_feature.cuda()\n",
    "                val_target = val_target.cuda()\n",
    "                \n",
    "            # model in test mode    \n",
    "            model.eval()\n",
    "\n",
    "            val_pred = model.forward(val_feature)\n",
    "            val_loss = model.loss(val_pred, val_target)\n",
    "            print(\"Epoch[{}] Validation Loss: {:.3f} \".format(epoch, val_loss.data))\n",
    "            \n",
    "            # Record the validation loss per example\n",
    "            valid_losses.append(val_loss.cpu().data.numpy()/len(val_feature))\n",
    "\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "\n",
    "print(ratingMean)\n",
    "print(best_theta[0])\n",
    "print(model.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
